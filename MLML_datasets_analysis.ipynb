{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "import random \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, cohen_kappa_score, hamming_loss\n",
    "random.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples is: 391\n",
      "Number of test samples is: 202\n"
     ]
    }
   ],
   "source": [
    "emotions_data_train = scipy.io.arff.loadarff(\"/home/karim/Documents/research/MLML datasets/emotions/emotions-train.arff\")\n",
    "emotions_data_test = scipy.io.arff.loadarff(\"/home/karim/Documents/research/MLML datasets/emotions/emotions-test.arff\")\n",
    "features_labels = emotions_data_train[0]\n",
    "features_labels_df = pd.DataFrame(features_labels)\n",
    "print(\"Number of training samples is: {}\".format(len(emotions_data[0])))\n",
    "\n",
    "features_labels_test = emotions_data_test[0]\n",
    "features_labels_test_df = pd.DataFrame(features_labels_test)\n",
    "print(\"Number of test samples is: {}\".format(len(features_labels__test_df)))\n",
    "features = features_labels_df.values[:,:-6]\n",
    "labels = features_labels_df.values[:,-6:]\n",
    "test_features = features_labels_test_df.values[:,:-6]\n",
    "test_labels = features_labels_test_df.values[:,-6:]\n",
    "LABEL_LIST = list(features_labels_df.columns[-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "code_folding": [
     5,
     10,
     16,
     22
    ]
   },
   "outputs": [],
   "source": [
    "input_shape = 72\n",
    "output_shape = 6\n",
    "hidden_layer_1_shape = 48\n",
    "hidden_layer_2_shape = 24\n",
    "\n",
    "def get_weights(shape):\n",
    "    w = tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "    #variable_summaries(w)\n",
    "    return w\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    b = tf.Variable(initial)\n",
    "    #variable_summaries(b)\n",
    "    return b\n",
    "\n",
    "def full_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = get_weights([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input, W) + b\n",
    "\n",
    "def weighted_loss(y_true, y_pred, positive_weights, negative_weights):\n",
    "    # clip to prevent NaN's and Inf's\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1-1e-7, name=None)\n",
    "    #y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # calc\n",
    "    loss = (-y_true * tf.log(y_pred) * positive_weights) - ((1.0 - y_true) * tf.log(1.0 - y_pred) * negative_weights)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 2 layers network to train \n",
    "y = tf.placeholder(tf.float32, [None, output_shape], name=\"true_labels\")\n",
    "x_input = tf.placeholder(tf.float32, [None,input_shape],name=\"input_layer\")\n",
    "h1 = tf.nn.relu(full_layer(x_input, hidden_layer_1_shape))\n",
    "h2 = tf.nn.relu(full_layer(h1, hidden_layer_2_shape))\n",
    "#h3 = tf.nn.relu(full_layer(h2, hidden_layer_2_shape))\n",
    "#h4 = tf.nn.relu(full_layer(h3, hidden_layer_2_shape))\n",
    "logits = full_layer(h2,output_shape)\n",
    "output = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with original labels [no missing] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "# Learning rate decay\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(learning_rate=0.1, global_step=global_step, decay_steps=1000,\n",
    "                                          decay_rate=0.95,staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "correct_prediction = tf.equal(tf.round(output), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #500 Loss: 0.5808 accuracy: 0.7097 Test loss: 0.6398 Test accuracy: 0.6724 Learning rate: 0.10000000149011612\n",
      "Epoch #1000 Loss: 0.5807 accuracy: 0.7025 Test loss: 0.5929 Test accuracy: 0.6972 Learning rate: 0.0949999988079071\n",
      "Epoch #1500 Loss: 0.6254 accuracy: 0.6803 Test loss: 0.6060 Test accuracy: 0.6700 Learning rate: 0.0949999988079071\n",
      "Epoch #2000 Loss: 0.5885 accuracy: 0.7110 Test loss: 0.6079 Test accuracy: 0.6790 Learning rate: 0.09025000035762787\n",
      "Epoch #2500 Loss: 0.5738 accuracy: 0.7229 Test loss: 0.6027 Test accuracy: 0.6947 Learning rate: 0.09025000035762787\n",
      "Epoch #3000 Loss: 0.5647 accuracy: 0.7234 Test loss: 0.6336 Test accuracy: 0.6691 Learning rate: 0.08573749661445618\n",
      "Epoch #3500 Loss: 0.5703 accuracy: 0.7229 Test loss: 0.6671 Test accuracy: 0.6650 Learning rate: 0.08573749661445618\n",
      "Epoch #4000 Loss: 0.5297 accuracy: 0.7408 Test loss: 0.5788 Test accuracy: 0.6988 Learning rate: 0.08145061880350113\n",
      "Epoch #4500 Loss: 0.5649 accuracy: 0.7225 Test loss: 0.5518 Test accuracy: 0.7104 Learning rate: 0.08145061880350113\n",
      "Epoch #5000 Loss: 0.5436 accuracy: 0.7229 Test loss: 0.5753 Test accuracy: 0.6898 Learning rate: 0.07737808674573898\n",
      "Epoch #5500 Loss: 0.5126 accuracy: 0.7455 Test loss: 0.5694 Test accuracy: 0.6997 Learning rate: 0.07737808674573898\n",
      "Epoch #6000 Loss: 0.6192 accuracy: 0.6952 Test loss: 0.6169 Test accuracy: 0.6889 Learning rate: 0.07350917905569077\n",
      "Epoch #6500 Loss: 0.5103 accuracy: 0.7383 Test loss: 0.5481 Test accuracy: 0.6980 Learning rate: 0.07350917905569077\n",
      "Epoch #7000 Loss: 0.5878 accuracy: 0.7076 Test loss: 0.5868 Test accuracy: 0.6865 Learning rate: 0.06983371824026108\n",
      "Epoch #7500 Loss: 0.5389 accuracy: 0.7221 Test loss: 0.5841 Test accuracy: 0.6955 Learning rate: 0.06983371824026108\n",
      "Epoch #8000 Loss: 0.4981 accuracy: 0.7587 Test loss: 0.5569 Test accuracy: 0.7145 Learning rate: 0.06634203344583511\n",
      "Epoch #8500 Loss: 0.5083 accuracy: 0.7498 Test loss: 0.5624 Test accuracy: 0.7096 Learning rate: 0.06634203344583511\n",
      "Epoch #9000 Loss: 0.5019 accuracy: 0.7438 Test loss: 0.5431 Test accuracy: 0.7096 Learning rate: 0.06302493065595627\n",
      "Epoch #9500 Loss: 0.5462 accuracy: 0.7306 Test loss: 0.6007 Test accuracy: 0.6914 Learning rate: 0.06302493065595627\n",
      "Epoch #10000 Loss: 0.5012 accuracy: 0.7545 Test loss: 0.5524 Test accuracy: 0.7310 Learning rate: 0.059873681515455246\n",
      "Epoch #10500 Loss: 0.5045 accuracy: 0.7434 Test loss: 0.5534 Test accuracy: 0.7178 Learning rate: 0.059873681515455246\n",
      "Epoch #11000 Loss: 0.5167 accuracy: 0.7366 Test loss: 0.5988 Test accuracy: 0.7013 Learning rate: 0.05687999725341797\n",
      "Epoch #11500 Loss: 0.5065 accuracy: 0.7515 Test loss: 0.5709 Test accuracy: 0.7327 Learning rate: 0.05687999725341797\n",
      "Epoch #12000 Loss: 0.4841 accuracy: 0.7647 Test loss: 0.5481 Test accuracy: 0.7426 Learning rate: 0.05403599888086319\n",
      "Epoch #12500 Loss: 0.4900 accuracy: 0.7575 Test loss: 0.5514 Test accuracy: 0.7120 Learning rate: 0.05403599888086319\n",
      "Epoch #13000 Loss: 0.4884 accuracy: 0.7647 Test loss: 0.5789 Test accuracy: 0.7129 Learning rate: 0.051334198564291\n",
      "Epoch #13500 Loss: 0.4810 accuracy: 0.7694 Test loss: 0.5900 Test accuracy: 0.7120 Learning rate: 0.051334198564291\n",
      "Epoch #14000 Loss: 0.4750 accuracy: 0.7626 Test loss: 0.5295 Test accuracy: 0.7492 Learning rate: 0.048767488449811935\n",
      "Epoch #14500 Loss: 0.4682 accuracy: 0.7707 Test loss: 0.5217 Test accuracy: 0.7384 Learning rate: 0.048767488449811935\n",
      "Epoch #15000 Loss: 0.4737 accuracy: 0.7660 Test loss: 0.5346 Test accuracy: 0.7417 Learning rate: 0.04632911458611488\n",
      "Epoch #15500 Loss: 0.5225 accuracy: 0.7468 Test loss: 0.5585 Test accuracy: 0.7219 Learning rate: 0.04632911458611488\n",
      "Epoch #16000 Loss: 0.4881 accuracy: 0.7656 Test loss: 0.6007 Test accuracy: 0.7104 Learning rate: 0.044012658298015594\n",
      "Epoch #16500 Loss: 0.4888 accuracy: 0.7545 Test loss: 0.5087 Test accuracy: 0.7541 Learning rate: 0.044012658298015594\n",
      "Epoch #17000 Loss: 0.4864 accuracy: 0.7587 Test loss: 0.5337 Test accuracy: 0.7525 Learning rate: 0.041812025010585785\n",
      "Epoch #17500 Loss: 0.4731 accuracy: 0.7754 Test loss: 0.5773 Test accuracy: 0.7236 Learning rate: 0.041812025010585785\n",
      "Epoch #18000 Loss: 0.4612 accuracy: 0.7796 Test loss: 0.5479 Test accuracy: 0.7492 Learning rate: 0.039721421897411346\n",
      "Epoch #18500 Loss: 0.4852 accuracy: 0.7528 Test loss: 0.5198 Test accuracy: 0.7574 Learning rate: 0.039721421897411346\n",
      "Epoch #19000 Loss: 0.4659 accuracy: 0.7690 Test loss: 0.5305 Test accuracy: 0.7426 Learning rate: 0.03773535043001175\n",
      "Epoch #19500 Loss: 0.4682 accuracy: 0.7720 Test loss: 0.5025 Test accuracy: 0.7574 Learning rate: 0.03773535043001175\n",
      "Epoch #20000 Loss: 0.4581 accuracy: 0.7779 Test loss: 0.5595 Test accuracy: 0.7318 Learning rate: 0.03584858402609825\n",
      "Epoch #20500 Loss: 0.4559 accuracy: 0.7788 Test loss: 0.5137 Test accuracy: 0.7459 Learning rate: 0.03584858402609825\n",
      "Epoch #21000 Loss: 0.4554 accuracy: 0.7826 Test loss: 0.5614 Test accuracy: 0.7393 Learning rate: 0.034056153148412704\n",
      "Epoch #21500 Loss: 0.4531 accuracy: 0.7801 Test loss: 0.5303 Test accuracy: 0.7459 Learning rate: 0.034056153148412704\n",
      "Epoch #22000 Loss: 0.4575 accuracy: 0.7839 Test loss: 0.5540 Test accuracy: 0.7475 Learning rate: 0.032353345304727554\n",
      "Epoch #22500 Loss: 0.4531 accuracy: 0.7779 Test loss: 0.5338 Test accuracy: 0.7450 Learning rate: 0.032353345304727554\n",
      "Epoch #23000 Loss: 0.4518 accuracy: 0.7783 Test loss: 0.5384 Test accuracy: 0.7401 Learning rate: 0.030735677108168602\n",
      "Epoch #23500 Loss: 0.4495 accuracy: 0.7783 Test loss: 0.5038 Test accuracy: 0.7657 Learning rate: 0.030735677108168602\n",
      "Epoch #24000 Loss: 0.4437 accuracy: 0.7869 Test loss: 0.5310 Test accuracy: 0.7649 Learning rate: 0.029198890551924706\n",
      "Epoch #24500 Loss: 0.4506 accuracy: 0.7873 Test loss: 0.5504 Test accuracy: 0.7566 Learning rate: 0.029198890551924706\n",
      "Epoch #25000 Loss: 0.4448 accuracy: 0.7886 Test loss: 0.5445 Test accuracy: 0.7550 Learning rate: 0.027738947421312332\n",
      "Epoch #25500 Loss: 0.4463 accuracy: 0.7835 Test loss: 0.5246 Test accuracy: 0.7533 Learning rate: 0.027738947421312332\n",
      "Epoch #26000 Loss: 0.4315 accuracy: 0.7899 Test loss: 0.5075 Test accuracy: 0.7632 Learning rate: 0.02635200135409832\n",
      "Epoch #26500 Loss: 0.4386 accuracy: 0.7882 Test loss: 0.5081 Test accuracy: 0.7624 Learning rate: 0.02635200135409832\n",
      "Epoch #27000 Loss: 0.4385 accuracy: 0.7950 Test loss: 0.5444 Test accuracy: 0.7574 Learning rate: 0.025034397840499878\n",
      "Epoch #27500 Loss: 0.4323 accuracy: 0.7907 Test loss: 0.5001 Test accuracy: 0.7607 Learning rate: 0.025034397840499878\n",
      "Epoch #28000 Loss: 0.4426 accuracy: 0.7830 Test loss: 0.5221 Test accuracy: 0.7566 Learning rate: 0.023782677948474884\n",
      "Epoch #28500 Loss: 0.4376 accuracy: 0.7899 Test loss: 0.4967 Test accuracy: 0.7756 Learning rate: 0.023782677948474884\n",
      "Epoch #29000 Loss: 0.4333 accuracy: 0.7899 Test loss: 0.5249 Test accuracy: 0.7533 Learning rate: 0.0225935447961092\n",
      "Epoch #29500 Loss: 0.4349 accuracy: 0.7890 Test loss: 0.5135 Test accuracy: 0.7632 Learning rate: 0.0225935447961092\n",
      "Epoch #30000 Loss: 0.4325 accuracy: 0.7980 Test loss: 0.5490 Test accuracy: 0.7599 Learning rate: 0.021463867276906967\n",
      "Epoch #30500 Loss: 0.4472 accuracy: 0.7796 Test loss: 0.5382 Test accuracy: 0.7459 Learning rate: 0.021463867276906967\n",
      "Epoch #31000 Loss: 0.4339 accuracy: 0.7903 Test loss: 0.5170 Test accuracy: 0.7566 Learning rate: 0.020390672609210014\n",
      "Epoch #31500 Loss: 0.4361 accuracy: 0.7877 Test loss: 0.5251 Test accuracy: 0.7574 Learning rate: 0.020390672609210014\n",
      "Epoch #32000 Loss: 0.4202 accuracy: 0.8018 Test loss: 0.5002 Test accuracy: 0.7665 Learning rate: 0.019371138885617256\n",
      "Epoch #32500 Loss: 0.4266 accuracy: 0.8005 Test loss: 0.5168 Test accuracy: 0.7640 Learning rate: 0.019371138885617256\n",
      "Epoch #33000 Loss: 0.4240 accuracy: 0.8018 Test loss: 0.5175 Test accuracy: 0.7632 Learning rate: 0.01840258203446865\n",
      "Epoch #33500 Loss: 0.4267 accuracy: 0.7954 Test loss: 0.5242 Test accuracy: 0.7607 Learning rate: 0.01840258203446865\n",
      "Epoch #34000 Loss: 0.4169 accuracy: 0.8031 Test loss: 0.4995 Test accuracy: 0.7673 Learning rate: 0.01748245395720005\n",
      "Epoch #34500 Loss: 0.4268 accuracy: 0.7933 Test loss: 0.5239 Test accuracy: 0.7616 Learning rate: 0.01748245395720005\n",
      "Epoch #35000 Loss: 0.4289 accuracy: 0.7882 Test loss: 0.5166 Test accuracy: 0.7665 Learning rate: 0.016608331352472305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #35500 Loss: 0.4142 accuracy: 0.8031 Test loss: 0.5043 Test accuracy: 0.7715 Learning rate: 0.016608331352472305\n",
      "Epoch #36000 Loss: 0.4180 accuracy: 0.8001 Test loss: 0.5288 Test accuracy: 0.7690 Learning rate: 0.015777913853526115\n",
      "Epoch #36500 Loss: 0.4130 accuracy: 0.8078 Test loss: 0.5271 Test accuracy: 0.7665 Learning rate: 0.015777913853526115\n",
      "Epoch #37000 Loss: 0.4151 accuracy: 0.8026 Test loss: 0.5057 Test accuracy: 0.7640 Learning rate: 0.014989017508924007\n",
      "Epoch #37500 Loss: 0.4104 accuracy: 0.8056 Test loss: 0.5054 Test accuracy: 0.7673 Learning rate: 0.014989017508924007\n",
      "Epoch #38000 Loss: 0.4126 accuracy: 0.8031 Test loss: 0.5040 Test accuracy: 0.7690 Learning rate: 0.014239566400647163\n",
      "Epoch #38500 Loss: 0.4153 accuracy: 0.8069 Test loss: 0.5235 Test accuracy: 0.7632 Learning rate: 0.014239566400647163\n",
      "Epoch #39000 Loss: 0.4108 accuracy: 0.8090 Test loss: 0.5307 Test accuracy: 0.7723 Learning rate: 0.013527587056159973\n",
      "Epoch #39500 Loss: 0.4151 accuracy: 0.8014 Test loss: 0.5287 Test accuracy: 0.7640 Learning rate: 0.013527587056159973\n",
      "Epoch #40000 Loss: 0.4073 accuracy: 0.8090 Test loss: 0.5156 Test accuracy: 0.7657 Learning rate: 0.012851208448410034\n",
      "Epoch #40500 Loss: 0.4071 accuracy: 0.8090 Test loss: 0.5056 Test accuracy: 0.7698 Learning rate: 0.012851208448410034\n",
      "Epoch #41000 Loss: 0.4065 accuracy: 0.8099 Test loss: 0.5138 Test accuracy: 0.7706 Learning rate: 0.012208647094666958\n",
      "Epoch #41500 Loss: 0.4088 accuracy: 0.8120 Test loss: 0.5240 Test accuracy: 0.7599 Learning rate: 0.012208647094666958\n",
      "Epoch #42000 Loss: 0.4065 accuracy: 0.8103 Test loss: 0.5085 Test accuracy: 0.7706 Learning rate: 0.011598214507102966\n",
      "Epoch #42500 Loss: 0.4034 accuracy: 0.8099 Test loss: 0.5119 Test accuracy: 0.7715 Learning rate: 0.011598214507102966\n",
      "Epoch #43000 Loss: 0.4119 accuracy: 0.8026 Test loss: 0.5279 Test accuracy: 0.7682 Learning rate: 0.011018304154276848\n",
      "Epoch #43500 Loss: 0.4026 accuracy: 0.8107 Test loss: 0.5257 Test accuracy: 0.7715 Learning rate: 0.011018304154276848\n",
      "Epoch #44000 Loss: 0.4027 accuracy: 0.8112 Test loss: 0.5289 Test accuracy: 0.7715 Learning rate: 0.010467388667166233\n",
      "Epoch #44500 Loss: 0.4023 accuracy: 0.8120 Test loss: 0.5299 Test accuracy: 0.7698 Learning rate: 0.010467388667166233\n",
      "Epoch #45000 Loss: 0.4074 accuracy: 0.8031 Test loss: 0.5284 Test accuracy: 0.7682 Learning rate: 0.00994401890784502\n",
      "Epoch #45500 Loss: 0.3998 accuracy: 0.8120 Test loss: 0.5247 Test accuracy: 0.7673 Learning rate: 0.00994401890784502\n",
      "Epoch #46000 Loss: 0.4079 accuracy: 0.8035 Test loss: 0.5329 Test accuracy: 0.7657 Learning rate: 0.009446817450225353\n",
      "Epoch #46500 Loss: 0.4004 accuracy: 0.8116 Test loss: 0.5279 Test accuracy: 0.7682 Learning rate: 0.009446817450225353\n",
      "Epoch #47000 Loss: 0.4054 accuracy: 0.8039 Test loss: 0.5294 Test accuracy: 0.7665 Learning rate: 0.008974476717412472\n",
      "Epoch #47500 Loss: 0.3998 accuracy: 0.8129 Test loss: 0.5203 Test accuracy: 0.7715 Learning rate: 0.008974476717412472\n",
      "Epoch #48000 Loss: 0.3995 accuracy: 0.8120 Test loss: 0.5195 Test accuracy: 0.7723 Learning rate: 0.008525753393769264\n",
      "Epoch #48500 Loss: 0.3970 accuracy: 0.8142 Test loss: 0.5230 Test accuracy: 0.7690 Learning rate: 0.008525753393769264\n",
      "Epoch #49000 Loss: 0.4007 accuracy: 0.8112 Test loss: 0.5254 Test accuracy: 0.7632 Learning rate: 0.008099465630948544\n",
      "Epoch #49500 Loss: 0.3991 accuracy: 0.8133 Test loss: 0.5244 Test accuracy: 0.7657 Learning rate: 0.008099465630948544\n",
      "Epoch #50000 Loss: 0.4043 accuracy: 0.8082 Test loss: 0.5290 Test accuracy: 0.7649 Learning rate: 0.007694491650909185\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_loss, epoch_accuracy,epoch_output, _ = sess.run([loss, accuracy,output, train_step],feed_dict={x_input: \n",
    "                                                                                         features,y: labels,})\n",
    "        if (epoch+1)% 500 == 0:\n",
    "            val_losses, val_accuracies, val_output,current_learning_rate = sess.run([loss, accuracy,output,learning_rate],feed_dict={\n",
    "                                                                                          x_input: test_features,\n",
    "                                                                                          y:test_labels})\n",
    "            print(\"Epoch #{}\".format(epoch+1), \"Loss: {:.4f}\".format(epoch_loss), \n",
    "                  \"accuracy: {:.4f}\".format(epoch_accuracy), \n",
    "                  \"Test loss: {:.4f}\".format(val_losses), \n",
    "                  \"Test accuracy: {:.4f}\".format(val_accuracies), \"Learning rate: {}\".format(current_learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate missing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_indices = np.nonzero(labels)\n",
    "ratio_of_hidden_samples = 0.1\n",
    "number_of_hidden_samples = int(len(ones_indices[0]) * ratio_of_hidden_samples)\n",
    "random_indices = random.sample(list(np.arange(len(ones_indices[0]))),number_of_hidden_samples)\n",
    "indices_to_hide = (ones_indices[0][random_indices] , ones_indices[1][random_indices])\n",
    "labels_with_missing_positives = np.copy(labels)\n",
    "for counter in range (number_of_hidden_samples):\n",
    "    labels_with_missing_positives[indices_to_hide[0][counter]][indices_to_hide[1][counter]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #500 Loss: 0.5590 accuracy: 0.7302 Test loss: 0.6049 Test accuracy: 0.6807 Learning rate: 0.10000000149011612\n",
      "Epoch #1000 Loss: 0.5596 accuracy: 0.7319 Test loss: 0.6122 Test accuracy: 0.6782 Learning rate: 0.0949999988079071\n",
      "Epoch #1500 Loss: 0.5347 accuracy: 0.7400 Test loss: 0.5758 Test accuracy: 0.6939 Learning rate: 0.0949999988079071\n",
      "Epoch #2000 Loss: 0.6166 accuracy: 0.6918 Test loss: 0.6890 Test accuracy: 0.6708 Learning rate: 0.09025000035762787\n",
      "Epoch #2500 Loss: 0.5518 accuracy: 0.7400 Test loss: 0.7069 Test accuracy: 0.6708 Learning rate: 0.09025000035762787\n",
      "Epoch #3000 Loss: 0.5565 accuracy: 0.7302 Test loss: 0.5926 Test accuracy: 0.7013 Learning rate: 0.08573749661445618\n",
      "Epoch #3500 Loss: 0.5547 accuracy: 0.7370 Test loss: 0.6173 Test accuracy: 0.6716 Learning rate: 0.08573749661445618\n",
      "Epoch #4000 Loss: 0.5652 accuracy: 0.7289 Test loss: 0.6149 Test accuracy: 0.6741 Learning rate: 0.08145061880350113\n",
      "Epoch #4500 Loss: 0.5755 accuracy: 0.7298 Test loss: 0.6273 Test accuracy: 0.6642 Learning rate: 0.08145061880350113\n",
      "Epoch #5000 Loss: 0.5885 accuracy: 0.7089 Test loss: 0.5998 Test accuracy: 0.6790 Learning rate: 0.07737808674573898\n",
      "Epoch #5500 Loss: 0.5212 accuracy: 0.7349 Test loss: 0.5836 Test accuracy: 0.6881 Learning rate: 0.07737808674573898\n",
      "Epoch #6000 Loss: 0.5623 accuracy: 0.7298 Test loss: 0.5939 Test accuracy: 0.6848 Learning rate: 0.07350917905569077\n",
      "Epoch #6500 Loss: 0.5354 accuracy: 0.7408 Test loss: 0.5797 Test accuracy: 0.6988 Learning rate: 0.07350917905569077\n",
      "Epoch #7000 Loss: 0.5375 accuracy: 0.7374 Test loss: 0.5703 Test accuracy: 0.7079 Learning rate: 0.06983371824026108\n",
      "Epoch #7500 Loss: 0.5212 accuracy: 0.7417 Test loss: 0.5586 Test accuracy: 0.7063 Learning rate: 0.06983371824026108\n",
      "Epoch #8000 Loss: 0.5060 accuracy: 0.7455 Test loss: 0.5519 Test accuracy: 0.7013 Learning rate: 0.06634203344583511\n",
      "Epoch #8500 Loss: 0.5193 accuracy: 0.7460 Test loss: 0.5560 Test accuracy: 0.7030 Learning rate: 0.06634203344583511\n",
      "Epoch #9000 Loss: 0.5286 accuracy: 0.7374 Test loss: 0.5669 Test accuracy: 0.7104 Learning rate: 0.06302493065595627\n",
      "Epoch #9500 Loss: 0.5046 accuracy: 0.7519 Test loss: 0.5578 Test accuracy: 0.7178 Learning rate: 0.06302493065595627\n",
      "Epoch #10000 Loss: 0.4923 accuracy: 0.7553 Test loss: 0.5404 Test accuracy: 0.7104 Learning rate: 0.059873681515455246\n",
      "Epoch #10500 Loss: 0.4951 accuracy: 0.7536 Test loss: 0.5423 Test accuracy: 0.7178 Learning rate: 0.059873681515455246\n",
      "Epoch #11000 Loss: 0.5210 accuracy: 0.7451 Test loss: 0.5505 Test accuracy: 0.7145 Learning rate: 0.05687999725341797\n",
      "Epoch #11500 Loss: 0.4846 accuracy: 0.7596 Test loss: 0.5336 Test accuracy: 0.7153 Learning rate: 0.05687999725341797\n",
      "Epoch #12000 Loss: 0.5111 accuracy: 0.7481 Test loss: 0.5461 Test accuracy: 0.7186 Learning rate: 0.05403599888086319\n",
      "Epoch #12500 Loss: 0.4836 accuracy: 0.7600 Test loss: 0.5310 Test accuracy: 0.7153 Learning rate: 0.05403599888086319\n",
      "Epoch #13000 Loss: 0.4785 accuracy: 0.7626 Test loss: 0.5290 Test accuracy: 0.7120 Learning rate: 0.051334198564291\n",
      "Epoch #13500 Loss: 0.4753 accuracy: 0.7630 Test loss: 0.5278 Test accuracy: 0.7170 Learning rate: 0.051334198564291\n",
      "Epoch #14000 Loss: 0.4778 accuracy: 0.7596 Test loss: 0.5264 Test accuracy: 0.7178 Learning rate: 0.048767488449811935\n",
      "Epoch #14500 Loss: 0.4872 accuracy: 0.7609 Test loss: 0.5328 Test accuracy: 0.7252 Learning rate: 0.048767488449811935\n",
      "Epoch #15000 Loss: 0.4859 accuracy: 0.7604 Test loss: 0.5347 Test accuracy: 0.7310 Learning rate: 0.04632911458611488\n",
      "Epoch #15500 Loss: 0.4699 accuracy: 0.7621 Test loss: 0.5237 Test accuracy: 0.7219 Learning rate: 0.04632911458611488\n",
      "Epoch #16000 Loss: 0.4793 accuracy: 0.7630 Test loss: 0.5249 Test accuracy: 0.7261 Learning rate: 0.044012658298015594\n",
      "Epoch #16500 Loss: 0.4694 accuracy: 0.7621 Test loss: 0.5209 Test accuracy: 0.7211 Learning rate: 0.044012658298015594\n",
      "Epoch #17000 Loss: 0.4975 accuracy: 0.7600 Test loss: 0.5487 Test accuracy: 0.7269 Learning rate: 0.041812025010585785\n",
      "Epoch #17500 Loss: 0.4691 accuracy: 0.7634 Test loss: 0.5191 Test accuracy: 0.7236 Learning rate: 0.041812025010585785\n",
      "Epoch #18000 Loss: 0.4796 accuracy: 0.7651 Test loss: 0.5213 Test accuracy: 0.7244 Learning rate: 0.039721421897411346\n",
      "Epoch #18500 Loss: 0.4707 accuracy: 0.7681 Test loss: 0.5411 Test accuracy: 0.7195 Learning rate: 0.039721421897411346\n",
      "Epoch #19000 Loss: 0.4677 accuracy: 0.7694 Test loss: 0.5349 Test accuracy: 0.7228 Learning rate: 0.03773535043001175\n",
      "Epoch #19500 Loss: 0.4695 accuracy: 0.7681 Test loss: 0.5390 Test accuracy: 0.7236 Learning rate: 0.03773535043001175\n",
      "Epoch #20000 Loss: 0.4743 accuracy: 0.7677 Test loss: 0.5474 Test accuracy: 0.7211 Learning rate: 0.03584858402609825\n",
      "Epoch #20500 Loss: 0.4652 accuracy: 0.7711 Test loss: 0.5311 Test accuracy: 0.7244 Learning rate: 0.03584858402609825\n",
      "Epoch #21000 Loss: 0.4610 accuracy: 0.7664 Test loss: 0.5266 Test accuracy: 0.7277 Learning rate: 0.034056153148412704\n",
      "Epoch #21500 Loss: 0.4728 accuracy: 0.7681 Test loss: 0.5467 Test accuracy: 0.7310 Learning rate: 0.034056153148412704\n",
      "Epoch #22000 Loss: 0.4742 accuracy: 0.7660 Test loss: 0.5484 Test accuracy: 0.7285 Learning rate: 0.032353345304727554\n",
      "Epoch #22500 Loss: 0.4596 accuracy: 0.7677 Test loss: 0.5270 Test accuracy: 0.7343 Learning rate: 0.032353345304727554\n",
      "Epoch #23000 Loss: 0.4607 accuracy: 0.7694 Test loss: 0.5297 Test accuracy: 0.7310 Learning rate: 0.030735677108168602\n",
      "Epoch #23500 Loss: 0.4591 accuracy: 0.7694 Test loss: 0.5272 Test accuracy: 0.7335 Learning rate: 0.030735677108168602\n",
      "Epoch #24000 Loss: 0.4606 accuracy: 0.7690 Test loss: 0.5311 Test accuracy: 0.7351 Learning rate: 0.029198890551924706\n",
      "Epoch #24500 Loss: 0.4573 accuracy: 0.7698 Test loss: 0.5257 Test accuracy: 0.7360 Learning rate: 0.029198890551924706\n",
      "Epoch #25000 Loss: 0.4653 accuracy: 0.7677 Test loss: 0.5385 Test accuracy: 0.7343 Learning rate: 0.027738947421312332\n",
      "Epoch #25500 Loss: 0.4581 accuracy: 0.7715 Test loss: 0.5281 Test accuracy: 0.7393 Learning rate: 0.027738947421312332\n",
      "Epoch #26000 Loss: 0.4559 accuracy: 0.7720 Test loss: 0.5260 Test accuracy: 0.7376 Learning rate: 0.02635200135409832\n",
      "Epoch #26500 Loss: 0.4565 accuracy: 0.7702 Test loss: 0.5260 Test accuracy: 0.7376 Learning rate: 0.02635200135409832\n",
      "Epoch #27000 Loss: 0.4544 accuracy: 0.7754 Test loss: 0.5243 Test accuracy: 0.7401 Learning rate: 0.025034397840499878\n",
      "Epoch #27500 Loss: 0.4516 accuracy: 0.7749 Test loss: 0.5182 Test accuracy: 0.7401 Learning rate: 0.025034397840499878\n",
      "Epoch #28000 Loss: 0.4503 accuracy: 0.7762 Test loss: 0.5177 Test accuracy: 0.7442 Learning rate: 0.023782677948474884\n",
      "Epoch #28500 Loss: 0.4518 accuracy: 0.7749 Test loss: 0.5205 Test accuracy: 0.7450 Learning rate: 0.023782677948474884\n",
      "Epoch #29000 Loss: 0.4567 accuracy: 0.7715 Test loss: 0.5314 Test accuracy: 0.7442 Learning rate: 0.0225935447961092\n",
      "Epoch #29500 Loss: 0.4480 accuracy: 0.7766 Test loss: 0.5176 Test accuracy: 0.7434 Learning rate: 0.0225935447961092\n",
      "Epoch #30000 Loss: 0.4507 accuracy: 0.7758 Test loss: 0.5232 Test accuracy: 0.7442 Learning rate: 0.021463867276906967\n",
      "Epoch #30500 Loss: 0.4507 accuracy: 0.7741 Test loss: 0.5250 Test accuracy: 0.7450 Learning rate: 0.021463867276906967\n",
      "Epoch #31000 Loss: 0.4485 accuracy: 0.7766 Test loss: 0.5222 Test accuracy: 0.7467 Learning rate: 0.020390672609210014\n",
      "Epoch #31500 Loss: 0.4492 accuracy: 0.7762 Test loss: 0.5230 Test accuracy: 0.7450 Learning rate: 0.020390672609210014\n",
      "Epoch #32000 Loss: 0.4473 accuracy: 0.7775 Test loss: 0.5225 Test accuracy: 0.7467 Learning rate: 0.019371138885617256\n",
      "Epoch #32500 Loss: 0.4492 accuracy: 0.7758 Test loss: 0.5252 Test accuracy: 0.7459 Learning rate: 0.019371138885617256\n",
      "Epoch #33000 Loss: 0.4462 accuracy: 0.7771 Test loss: 0.5233 Test accuracy: 0.7467 Learning rate: 0.01840258203446865\n",
      "Epoch #33500 Loss: 0.4475 accuracy: 0.7754 Test loss: 0.5243 Test accuracy: 0.7467 Learning rate: 0.01840258203446865\n",
      "Epoch #34000 Loss: 0.4473 accuracy: 0.7758 Test loss: 0.5240 Test accuracy: 0.7475 Learning rate: 0.01748245395720005\n",
      "Epoch #34500 Loss: 0.4451 accuracy: 0.7775 Test loss: 0.5220 Test accuracy: 0.7483 Learning rate: 0.01748245395720005\n",
      "Epoch #35000 Loss: 0.4448 accuracy: 0.7779 Test loss: 0.5224 Test accuracy: 0.7492 Learning rate: 0.016608331352472305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #35500 Loss: 0.4412 accuracy: 0.7835 Test loss: 0.5162 Test accuracy: 0.7450 Learning rate: 0.016608331352472305\n",
      "Epoch #36000 Loss: 0.4425 accuracy: 0.7792 Test loss: 0.5196 Test accuracy: 0.7450 Learning rate: 0.015777913853526115\n",
      "Epoch #36500 Loss: 0.4416 accuracy: 0.7813 Test loss: 0.5180 Test accuracy: 0.7459 Learning rate: 0.015777913853526115\n",
      "Epoch #37000 Loss: 0.4418 accuracy: 0.7792 Test loss: 0.5187 Test accuracy: 0.7459 Learning rate: 0.014989017508924007\n",
      "Epoch #37500 Loss: 0.4401 accuracy: 0.7813 Test loss: 0.5164 Test accuracy: 0.7483 Learning rate: 0.014989017508924007\n",
      "Epoch #38000 Loss: 0.4410 accuracy: 0.7792 Test loss: 0.5192 Test accuracy: 0.7450 Learning rate: 0.014239566400647163\n",
      "Epoch #38500 Loss: 0.4379 accuracy: 0.7852 Test loss: 0.5149 Test accuracy: 0.7483 Learning rate: 0.014239566400647163\n",
      "Epoch #39000 Loss: 0.4382 accuracy: 0.7835 Test loss: 0.5174 Test accuracy: 0.7459 Learning rate: 0.013527587056159973\n",
      "Epoch #39500 Loss: 0.4368 accuracy: 0.7843 Test loss: 0.5156 Test accuracy: 0.7483 Learning rate: 0.013527587056159973\n",
      "Epoch #40000 Loss: 0.4384 accuracy: 0.7830 Test loss: 0.5177 Test accuracy: 0.7500 Learning rate: 0.012851208448410034\n",
      "Epoch #40500 Loss: 0.4370 accuracy: 0.7847 Test loss: 0.5173 Test accuracy: 0.7500 Learning rate: 0.012851208448410034\n",
      "Epoch #41000 Loss: 0.4360 accuracy: 0.7847 Test loss: 0.5153 Test accuracy: 0.7483 Learning rate: 0.012208647094666958\n",
      "Epoch #41500 Loss: 0.4348 accuracy: 0.7882 Test loss: 0.5136 Test accuracy: 0.7483 Learning rate: 0.012208647094666958\n",
      "Epoch #42000 Loss: 0.4352 accuracy: 0.7873 Test loss: 0.5157 Test accuracy: 0.7483 Learning rate: 0.011598214507102966\n",
      "Epoch #42500 Loss: 0.4344 accuracy: 0.7873 Test loss: 0.5144 Test accuracy: 0.7492 Learning rate: 0.011598214507102966\n",
      "Epoch #43000 Loss: 0.4337 accuracy: 0.7907 Test loss: 0.5128 Test accuracy: 0.7517 Learning rate: 0.011018304154276848\n",
      "Epoch #43500 Loss: 0.4341 accuracy: 0.7873 Test loss: 0.5158 Test accuracy: 0.7508 Learning rate: 0.011018304154276848\n",
      "Epoch #44000 Loss: 0.4333 accuracy: 0.7890 Test loss: 0.5142 Test accuracy: 0.7508 Learning rate: 0.010467388667166233\n",
      "Epoch #44500 Loss: 0.4329 accuracy: 0.7894 Test loss: 0.5141 Test accuracy: 0.7525 Learning rate: 0.010467388667166233\n",
      "Epoch #45000 Loss: 0.4324 accuracy: 0.7911 Test loss: 0.5124 Test accuracy: 0.7517 Learning rate: 0.00994401890784502\n",
      "Epoch #45500 Loss: 0.4323 accuracy: 0.7886 Test loss: 0.5130 Test accuracy: 0.7517 Learning rate: 0.00994401890784502\n",
      "Epoch #46000 Loss: 0.4321 accuracy: 0.7899 Test loss: 0.5143 Test accuracy: 0.7541 Learning rate: 0.009446817450225353\n",
      "Epoch #46500 Loss: 0.4317 accuracy: 0.7899 Test loss: 0.5142 Test accuracy: 0.7541 Learning rate: 0.009446817450225353\n",
      "Epoch #47000 Loss: 0.4315 accuracy: 0.7899 Test loss: 0.5140 Test accuracy: 0.7541 Learning rate: 0.008974476717412472\n",
      "Epoch #47500 Loss: 0.4312 accuracy: 0.7911 Test loss: 0.5128 Test accuracy: 0.7550 Learning rate: 0.008974476717412472\n",
      "Epoch #48000 Loss: 0.4309 accuracy: 0.7899 Test loss: 0.5115 Test accuracy: 0.7541 Learning rate: 0.008525753393769264\n",
      "Epoch #48500 Loss: 0.4307 accuracy: 0.7903 Test loss: 0.5124 Test accuracy: 0.7533 Learning rate: 0.008525753393769264\n",
      "Epoch #49000 Loss: 0.4306 accuracy: 0.7899 Test loss: 0.5125 Test accuracy: 0.7533 Learning rate: 0.008099465630948544\n",
      "Epoch #49500 Loss: 0.4302 accuracy: 0.7903 Test loss: 0.5121 Test accuracy: 0.7550 Learning rate: 0.008099465630948544\n",
      "Epoch #50000 Loss: 0.4301 accuracy: 0.7903 Test loss: 0.5118 Test accuracy: 0.7541 Learning rate: 0.007694491650909185\n"
     ]
    }
   ],
   "source": [
    "# Training with missing labels with 10%\n",
    "NUM_EPOCHS = 50000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_loss, epoch_accuracy,epoch_output, _ = sess.run([loss, accuracy,output, train_step],feed_dict={x_input: \n",
    "                                                                                         features,y: labels_with_missing_positives})\n",
    "        if (epoch+1)% 500 == 0:\n",
    "            val_losses, val_accuracies, val_output,current_learning_rate = sess.run([loss, accuracy,output,learning_rate],feed_dict={\n",
    "                                                                                          x_input: test_features,\n",
    "                                                                                          y:test_labels})\n",
    "            print(\"Epoch #{}\".format(epoch+1), \"Loss: {:.4f}\".format(epoch_loss), \n",
    "                  \"accuracy: {:.4f}\".format(epoch_accuracy), \n",
    "                  \"Test loss: {:.4f}\".format(val_losses), \n",
    "                  \"Test accuracy: {:.4f}\".format(val_accuracies), \"Learning rate: {}\".format(current_learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with 40% hidden labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #500 Loss: 0.4654 accuracy: 0.8188 Test loss: 0.7191 Test accuracy: 0.6708 Learning rate: 0.10000000149011612\n",
      "Epoch #1000 Loss: 0.4577 accuracy: 0.8184 Test loss: 0.6441 Test accuracy: 0.6716 Learning rate: 0.0949999988079071\n",
      "Epoch #1500 Loss: 0.4597 accuracy: 0.8171 Test loss: 0.6477 Test accuracy: 0.6708 Learning rate: 0.0949999988079071\n",
      "Epoch #2000 Loss: 0.4793 accuracy: 0.8188 Test loss: 0.7385 Test accuracy: 0.6708 Learning rate: 0.09025000035762787\n",
      "Epoch #2500 Loss: 0.4786 accuracy: 0.8193 Test loss: 0.6396 Test accuracy: 0.6724 Learning rate: 0.09025000035762787\n",
      "Epoch #3000 Loss: 0.4565 accuracy: 0.8193 Test loss: 0.6697 Test accuracy: 0.6708 Learning rate: 0.08573749661445618\n",
      "Epoch #3500 Loss: 0.4670 accuracy: 0.8201 Test loss: 0.6294 Test accuracy: 0.6708 Learning rate: 0.08573749661445618\n",
      "Epoch #4000 Loss: 0.4241 accuracy: 0.8193 Test loss: 0.6530 Test accuracy: 0.6724 Learning rate: 0.08145061880350113\n",
      "Epoch #4500 Loss: 0.4779 accuracy: 0.8193 Test loss: 0.6281 Test accuracy: 0.6741 Learning rate: 0.08145061880350113\n",
      "Epoch #5000 Loss: 0.4242 accuracy: 0.8193 Test loss: 0.6158 Test accuracy: 0.6757 Learning rate: 0.07737808674573898\n",
      "Epoch #5500 Loss: 0.4105 accuracy: 0.8214 Test loss: 0.6369 Test accuracy: 0.6790 Learning rate: 0.07737808674573898\n",
      "Epoch #6000 Loss: 0.4297 accuracy: 0.8184 Test loss: 0.7440 Test accuracy: 0.6733 Learning rate: 0.07350917905569077\n",
      "Epoch #6500 Loss: 0.4227 accuracy: 0.8235 Test loss: 0.7105 Test accuracy: 0.6733 Learning rate: 0.07350917905569077\n",
      "Epoch #7000 Loss: 0.4112 accuracy: 0.8214 Test loss: 0.6812 Test accuracy: 0.6741 Learning rate: 0.06983371824026108\n",
      "Epoch #7500 Loss: 0.4063 accuracy: 0.8201 Test loss: 0.6825 Test accuracy: 0.6766 Learning rate: 0.06983371824026108\n",
      "Epoch #8000 Loss: 0.4108 accuracy: 0.8184 Test loss: 0.6331 Test accuracy: 0.6774 Learning rate: 0.06634203344583511\n",
      "Epoch #8500 Loss: 0.3992 accuracy: 0.8197 Test loss: 0.6337 Test accuracy: 0.6790 Learning rate: 0.06634203344583511\n",
      "Epoch #9000 Loss: 0.4287 accuracy: 0.8218 Test loss: 0.7374 Test accuracy: 0.6733 Learning rate: 0.06302493065595627\n",
      "Epoch #9500 Loss: 0.4189 accuracy: 0.8231 Test loss: 0.6953 Test accuracy: 0.6766 Learning rate: 0.06302493065595627\n",
      "Epoch #10000 Loss: 0.4118 accuracy: 0.8244 Test loss: 0.6893 Test accuracy: 0.6749 Learning rate: 0.059873681515455246\n",
      "Epoch #10500 Loss: 0.3975 accuracy: 0.8240 Test loss: 0.6536 Test accuracy: 0.6790 Learning rate: 0.059873681515455246\n",
      "Epoch #11000 Loss: 0.4031 accuracy: 0.8231 Test loss: 0.6490 Test accuracy: 0.6774 Learning rate: 0.05687999725341797\n",
      "Epoch #11500 Loss: 0.3891 accuracy: 0.8248 Test loss: 0.6668 Test accuracy: 0.6774 Learning rate: 0.05687999725341797\n",
      "Epoch #12000 Loss: 0.4072 accuracy: 0.8231 Test loss: 0.6631 Test accuracy: 0.6774 Learning rate: 0.05403599888086319\n",
      "Epoch #12500 Loss: 0.3885 accuracy: 0.8278 Test loss: 0.6435 Test accuracy: 0.6898 Learning rate: 0.05403599888086319\n",
      "Epoch #13000 Loss: 0.3955 accuracy: 0.8235 Test loss: 0.6829 Test accuracy: 0.6766 Learning rate: 0.051334198564291\n",
      "Epoch #13500 Loss: 0.3998 accuracy: 0.8240 Test loss: 0.6465 Test accuracy: 0.6881 Learning rate: 0.051334198564291\n",
      "Epoch #14000 Loss: 0.3915 accuracy: 0.8244 Test loss: 0.6723 Test accuracy: 0.6790 Learning rate: 0.048767488449811935\n",
      "Epoch #14500 Loss: 0.4130 accuracy: 0.8223 Test loss: 0.6942 Test accuracy: 0.6782 Learning rate: 0.048767488449811935\n",
      "Epoch #15000 Loss: 0.3888 accuracy: 0.8244 Test loss: 0.6843 Test accuracy: 0.6799 Learning rate: 0.04632911458611488\n",
      "Epoch #15500 Loss: 0.3997 accuracy: 0.8235 Test loss: 0.6787 Test accuracy: 0.6823 Learning rate: 0.04632911458611488\n",
      "Epoch #16000 Loss: 0.3883 accuracy: 0.8278 Test loss: 0.6448 Test accuracy: 0.6881 Learning rate: 0.044012658298015594\n",
      "Epoch #16500 Loss: 0.3883 accuracy: 0.8265 Test loss: 0.6831 Test accuracy: 0.6848 Learning rate: 0.044012658298015594\n",
      "Epoch #17000 Loss: 0.3819 accuracy: 0.8286 Test loss: 0.6774 Test accuracy: 0.6823 Learning rate: 0.041812025010585785\n",
      "Epoch #17500 Loss: 0.3907 accuracy: 0.8248 Test loss: 0.6956 Test accuracy: 0.6832 Learning rate: 0.041812025010585785\n",
      "Epoch #18000 Loss: 0.3740 accuracy: 0.8308 Test loss: 0.6484 Test accuracy: 0.6914 Learning rate: 0.039721421897411346\n",
      "Epoch #18500 Loss: 0.3810 accuracy: 0.8295 Test loss: 0.6527 Test accuracy: 0.6865 Learning rate: 0.039721421897411346\n",
      "Epoch #19000 Loss: 0.3832 accuracy: 0.8282 Test loss: 0.7069 Test accuracy: 0.6832 Learning rate: 0.03773535043001175\n",
      "Epoch #19500 Loss: 0.3815 accuracy: 0.8282 Test loss: 0.7089 Test accuracy: 0.6823 Learning rate: 0.03773535043001175\n",
      "Epoch #20000 Loss: 0.3826 accuracy: 0.8308 Test loss: 0.6663 Test accuracy: 0.6906 Learning rate: 0.03584858402609825\n",
      "Epoch #20500 Loss: 0.3866 accuracy: 0.8299 Test loss: 0.7116 Test accuracy: 0.6832 Learning rate: 0.03584858402609825\n",
      "Epoch #21000 Loss: 0.3817 accuracy: 0.8295 Test loss: 0.7192 Test accuracy: 0.6848 Learning rate: 0.034056153148412704\n",
      "Epoch #21500 Loss: 0.3780 accuracy: 0.8312 Test loss: 0.7215 Test accuracy: 0.6848 Learning rate: 0.034056153148412704\n",
      "Epoch #22000 Loss: 0.3705 accuracy: 0.8333 Test loss: 0.6678 Test accuracy: 0.6939 Learning rate: 0.032353345304727554\n",
      "Epoch #22500 Loss: 0.3702 accuracy: 0.8312 Test loss: 0.6721 Test accuracy: 0.6964 Learning rate: 0.032353345304727554\n",
      "Epoch #23000 Loss: 0.3668 accuracy: 0.8308 Test loss: 0.6789 Test accuracy: 0.6988 Learning rate: 0.030735677108168602\n",
      "Epoch #23500 Loss: 0.3697 accuracy: 0.8338 Test loss: 0.7100 Test accuracy: 0.6856 Learning rate: 0.030735677108168602\n",
      "Epoch #24000 Loss: 0.3710 accuracy: 0.8342 Test loss: 0.6712 Test accuracy: 0.6939 Learning rate: 0.029198890551924706\n",
      "Epoch #24500 Loss: 0.3702 accuracy: 0.8342 Test loss: 0.6714 Test accuracy: 0.6955 Learning rate: 0.029198890551924706\n",
      "Epoch #25000 Loss: 0.3725 accuracy: 0.8342 Test loss: 0.7065 Test accuracy: 0.6922 Learning rate: 0.027738947421312332\n",
      "Epoch #25500 Loss: 0.3642 accuracy: 0.8338 Test loss: 0.6935 Test accuracy: 0.6931 Learning rate: 0.027738947421312332\n",
      "Epoch #26000 Loss: 0.3669 accuracy: 0.8350 Test loss: 0.6855 Test accuracy: 0.6972 Learning rate: 0.02635200135409832\n",
      "Epoch #26500 Loss: 0.3675 accuracy: 0.8325 Test loss: 0.6727 Test accuracy: 0.7021 Learning rate: 0.02635200135409832\n",
      "Epoch #27000 Loss: 0.3635 accuracy: 0.8342 Test loss: 0.6907 Test accuracy: 0.7013 Learning rate: 0.025034397840499878\n",
      "Epoch #27500 Loss: 0.3619 accuracy: 0.8367 Test loss: 0.6927 Test accuracy: 0.7038 Learning rate: 0.025034397840499878\n",
      "Epoch #28000 Loss: 0.3645 accuracy: 0.8333 Test loss: 0.6961 Test accuracy: 0.6980 Learning rate: 0.023782677948474884\n",
      "Epoch #28500 Loss: 0.3643 accuracy: 0.8367 Test loss: 0.6980 Test accuracy: 0.7046 Learning rate: 0.023782677948474884\n",
      "Epoch #29000 Loss: 0.3645 accuracy: 0.8333 Test loss: 0.7123 Test accuracy: 0.6964 Learning rate: 0.0225935447961092\n",
      "Epoch #29500 Loss: 0.3627 accuracy: 0.8350 Test loss: 0.6966 Test accuracy: 0.7079 Learning rate: 0.0225935447961092\n",
      "Epoch #30000 Loss: 0.3637 accuracy: 0.8376 Test loss: 0.7031 Test accuracy: 0.7079 Learning rate: 0.021463867276906967\n",
      "Epoch #30500 Loss: 0.3582 accuracy: 0.8376 Test loss: 0.7057 Test accuracy: 0.6931 Learning rate: 0.021463867276906967\n",
      "Epoch #31000 Loss: 0.3599 accuracy: 0.8380 Test loss: 0.7034 Test accuracy: 0.7071 Learning rate: 0.020390672609210014\n",
      "Epoch #31500 Loss: 0.3568 accuracy: 0.8380 Test loss: 0.7099 Test accuracy: 0.6964 Learning rate: 0.020390672609210014\n",
      "Epoch #32000 Loss: 0.3607 accuracy: 0.8372 Test loss: 0.7066 Test accuracy: 0.7079 Learning rate: 0.019371138885617256\n",
      "Epoch #32500 Loss: 0.3594 accuracy: 0.8376 Test loss: 0.7064 Test accuracy: 0.7071 Learning rate: 0.019371138885617256\n",
      "Epoch #33000 Loss: 0.3563 accuracy: 0.8376 Test loss: 0.7128 Test accuracy: 0.6947 Learning rate: 0.01840258203446865\n",
      "Epoch #33500 Loss: 0.3598 accuracy: 0.8380 Test loss: 0.7113 Test accuracy: 0.7096 Learning rate: 0.01840258203446865\n",
      "Epoch #34000 Loss: 0.3600 accuracy: 0.8384 Test loss: 0.7133 Test accuracy: 0.7087 Learning rate: 0.01748245395720005\n",
      "Epoch #34500 Loss: 0.3555 accuracy: 0.8363 Test loss: 0.7150 Test accuracy: 0.7038 Learning rate: 0.01748245395720005\n",
      "Epoch #35000 Loss: 0.3562 accuracy: 0.8372 Test loss: 0.7201 Test accuracy: 0.7054 Learning rate: 0.016608331352472305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #35500 Loss: 0.3546 accuracy: 0.8372 Test loss: 0.7191 Test accuracy: 0.6997 Learning rate: 0.016608331352472305\n",
      "Epoch #36000 Loss: 0.3577 accuracy: 0.8393 Test loss: 0.7175 Test accuracy: 0.7120 Learning rate: 0.015777913853526115\n",
      "Epoch #36500 Loss: 0.3529 accuracy: 0.8376 Test loss: 0.7240 Test accuracy: 0.6988 Learning rate: 0.015777913853526115\n",
      "Epoch #37000 Loss: 0.3570 accuracy: 0.8363 Test loss: 0.7295 Test accuracy: 0.7038 Learning rate: 0.014989017508924007\n",
      "Epoch #37500 Loss: 0.3550 accuracy: 0.8359 Test loss: 0.7259 Test accuracy: 0.7087 Learning rate: 0.014989017508924007\n",
      "Epoch #38000 Loss: 0.3513 accuracy: 0.8384 Test loss: 0.7230 Test accuracy: 0.7021 Learning rate: 0.014239566400647163\n",
      "Epoch #38500 Loss: 0.3519 accuracy: 0.8389 Test loss: 0.7266 Test accuracy: 0.7038 Learning rate: 0.014239566400647163\n",
      "Epoch #39000 Loss: 0.3523 accuracy: 0.8376 Test loss: 0.7296 Test accuracy: 0.7096 Learning rate: 0.013527587056159973\n",
      "Epoch #39500 Loss: 0.3501 accuracy: 0.8410 Test loss: 0.7306 Test accuracy: 0.7038 Learning rate: 0.013527587056159973\n",
      "Epoch #40000 Loss: 0.3516 accuracy: 0.8384 Test loss: 0.7319 Test accuracy: 0.7104 Learning rate: 0.012851208448410034\n",
      "Epoch #40500 Loss: 0.3523 accuracy: 0.8363 Test loss: 0.7327 Test accuracy: 0.7096 Learning rate: 0.012851208448410034\n",
      "Epoch #41000 Loss: 0.3503 accuracy: 0.8393 Test loss: 0.7348 Test accuracy: 0.7096 Learning rate: 0.012208647094666958\n",
      "Epoch #41500 Loss: 0.3490 accuracy: 0.8397 Test loss: 0.7354 Test accuracy: 0.7063 Learning rate: 0.012208647094666958\n",
      "Epoch #42000 Loss: 0.3487 accuracy: 0.8393 Test loss: 0.7362 Test accuracy: 0.7071 Learning rate: 0.011598214507102966\n",
      "Epoch #42500 Loss: 0.3500 accuracy: 0.8367 Test loss: 0.7370 Test accuracy: 0.7112 Learning rate: 0.011598214507102966\n",
      "Epoch #43000 Loss: 0.3483 accuracy: 0.8414 Test loss: 0.7409 Test accuracy: 0.6964 Learning rate: 0.011018304154276848\n",
      "Epoch #43500 Loss: 0.3484 accuracy: 0.8423 Test loss: 0.7393 Test accuracy: 0.7038 Learning rate: 0.011018304154276848\n",
      "Epoch #44000 Loss: 0.3484 accuracy: 0.8427 Test loss: 0.7426 Test accuracy: 0.6988 Learning rate: 0.010467388667166233\n",
      "Epoch #44500 Loss: 0.3481 accuracy: 0.8427 Test loss: 0.7433 Test accuracy: 0.7030 Learning rate: 0.010467388667166233\n",
      "Epoch #45000 Loss: 0.3475 accuracy: 0.8419 Test loss: 0.7457 Test accuracy: 0.6972 Learning rate: 0.00994401890784502\n",
      "Epoch #45500 Loss: 0.3468 accuracy: 0.8406 Test loss: 0.7475 Test accuracy: 0.7046 Learning rate: 0.00994401890784502\n",
      "Epoch #46000 Loss: 0.3474 accuracy: 0.8431 Test loss: 0.7472 Test accuracy: 0.7038 Learning rate: 0.009446817450225353\n",
      "Epoch #46500 Loss: 0.3466 accuracy: 0.8410 Test loss: 0.7481 Test accuracy: 0.7038 Learning rate: 0.009446817450225353\n",
      "Epoch #47000 Loss: 0.3465 accuracy: 0.8393 Test loss: 0.7503 Test accuracy: 0.7079 Learning rate: 0.008974476717412472\n",
      "Epoch #47500 Loss: 0.3463 accuracy: 0.8410 Test loss: 0.7518 Test accuracy: 0.7005 Learning rate: 0.008974476717412472\n",
      "Epoch #48000 Loss: 0.3469 accuracy: 0.8393 Test loss: 0.7521 Test accuracy: 0.7079 Learning rate: 0.008525753393769264\n",
      "Epoch #48500 Loss: 0.3462 accuracy: 0.8427 Test loss: 0.7496 Test accuracy: 0.7005 Learning rate: 0.008525753393769264\n",
      "Epoch #49000 Loss: 0.3465 accuracy: 0.8423 Test loss: 0.7530 Test accuracy: 0.7030 Learning rate: 0.008099465630948544\n",
      "Epoch #49500 Loss: 0.3458 accuracy: 0.8389 Test loss: 0.7553 Test accuracy: 0.7087 Learning rate: 0.008099465630948544\n",
      "Epoch #50000 Loss: 0.3458 accuracy: 0.8393 Test loss: 0.7553 Test accuracy: 0.7071 Learning rate: 0.007694491650909185\n"
     ]
    }
   ],
   "source": [
    "ones_indices = np.nonzero(labels)\n",
    "ratio_of_hidden_samples = 0.4\n",
    "number_of_hidden_samples = int(len(ones_indices[0]) * ratio_of_hidden_samples)\n",
    "random_indices = random.sample(list(np.arange(len(ones_indices[0]))),number_of_hidden_samples)\n",
    "indices_to_hide = (ones_indices[0][random_indices] , ones_indices[1][random_indices])\n",
    "labels_with_missing_positives = np.copy(labels)\n",
    "for counter in range (number_of_hidden_samples):\n",
    "    labels_with_missing_positives[indices_to_hide[0][counter]][indices_to_hide[1][counter]] = 0\n",
    "    \n",
    "    \n",
    "# Training with missing labels with 40%\n",
    "NUM_EPOCHS = 50000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_loss, epoch_accuracy,epoch_output, _ = sess.run([loss, accuracy,output, train_step],feed_dict={x_input: \n",
    "                                                                                         features,y: labels_with_missing_positives})\n",
    "        if (epoch+1)% 500 == 0:\n",
    "            val_losses, val_accuracies, val_output,current_learning_rate = sess.run([loss, accuracy,output,learning_rate],feed_dict={\n",
    "                                                                                          x_input: test_features,\n",
    "                                                                                          y:test_labels})\n",
    "            print(\"Epoch #{}\".format(epoch+1), \"Loss: {:.4f}\".format(epoch_loss), \n",
    "                  \"accuracy: {:.4f}\".format(epoch_accuracy), \n",
    "                  \"Test loss: {:.4f}\".format(val_losses), \n",
    "                  \"Test accuracy: {:.4f}\".format(val_accuracies), \"Learning rate: {}\".format(current_learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With negative weights (fixed weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_negative_weights = np.zeros_like(labels) + 1 \n",
    "train_positive_weights = np.zeros_like(labels) + 1\n",
    "for counter in range (number_of_hidden_samples):\n",
    "    train_negative_weights[indices_to_hide[0][counter]][indices_to_hide[1][counter]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_weights = tf.placeholder(tf.float32, [None,output_shape], name = \"Positive_weights\")\n",
    "negative_weights = tf.placeholder(tf.float32, [None, output_shape], name=\"negative_weights\")\n",
    "my_weights_loss = weighted_loss(y_true= y, y_pred= output,\n",
    "                              positive_weights= positive_weights, negative_weights= negative_weights)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(my_weights_loss,global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #500 Loss: 0.4728 Weighted Loss: 0.4352 accuracy: 0.8184 Test loss: 0.7250 Test accuracy: 0.6716\n",
      "Epoch #1000 Loss: 0.4488 Weighted Loss: 0.4135 accuracy: 0.8193 Test loss: 0.6207 Test accuracy: 0.6741\n",
      "Epoch #1500 Loss: 0.5234 Weighted Loss: 0.4958 accuracy: 0.8167 Test loss: 0.6832 Test accuracy: 0.6708\n",
      "Epoch #2000 Loss: 0.4592 Weighted Loss: 0.4290 accuracy: 0.8176 Test loss: 0.6610 Test accuracy: 0.6700\n",
      "Epoch #2500 Loss: 0.4515 Weighted Loss: 0.4215 accuracy: 0.8176 Test loss: 0.6453 Test accuracy: 0.6741\n",
      "Epoch #3000 Loss: 0.4981 Weighted Loss: 0.4696 accuracy: 0.8171 Test loss: 0.7003 Test accuracy: 0.6617\n",
      "Epoch #3500 Loss: 0.4537 Weighted Loss: 0.4155 accuracy: 0.8184 Test loss: 0.6471 Test accuracy: 0.6889\n",
      "Epoch #4000 Loss: 0.4405 Weighted Loss: 0.4013 accuracy: 0.8163 Test loss: 0.5821 Test accuracy: 0.6939\n",
      "Epoch #4500 Loss: 0.4452 Weighted Loss: 0.4089 accuracy: 0.8176 Test loss: 0.5732 Test accuracy: 0.7071\n",
      "Epoch #5000 Loss: 0.4316 Weighted Loss: 0.3732 accuracy: 0.8014 Test loss: 0.6719 Test accuracy: 0.6807\n",
      "Epoch #5500 Loss: 0.4299 Weighted Loss: 0.3715 accuracy: 0.8137 Test loss: 0.6191 Test accuracy: 0.6873\n",
      "Epoch #6000 Loss: 0.4814 Weighted Loss: 0.4140 accuracy: 0.7728 Test loss: 0.7090 Test accuracy: 0.6700\n",
      "Epoch #6500 Loss: 0.4472 Weighted Loss: 0.4037 accuracy: 0.8026 Test loss: 0.6021 Test accuracy: 0.6997\n",
      "Epoch #7000 Loss: 0.4236 Weighted Loss: 0.3748 accuracy: 0.8129 Test loss: 0.5827 Test accuracy: 0.7162\n",
      "Epoch #7500 Loss: 0.4543 Weighted Loss: 0.3850 accuracy: 0.7954 Test loss: 0.7221 Test accuracy: 0.6922\n",
      "Epoch #8000 Loss: 0.4065 Weighted Loss: 0.3594 accuracy: 0.8167 Test loss: 0.5736 Test accuracy: 0.6939\n",
      "Epoch #8500 Loss: 0.4762 Weighted Loss: 0.4153 accuracy: 0.7907 Test loss: 0.6010 Test accuracy: 0.6931\n",
      "Epoch #9000 Loss: 0.4338 Weighted Loss: 0.3848 accuracy: 0.8095 Test loss: 0.6150 Test accuracy: 0.6964\n",
      "Epoch #9500 Loss: 0.4252 Weighted Loss: 0.3568 accuracy: 0.8052 Test loss: 0.6547 Test accuracy: 0.7013\n",
      "Epoch #10000 Loss: 0.4157 Weighted Loss: 0.3621 accuracy: 0.8086 Test loss: 0.5822 Test accuracy: 0.7104\n",
      "Epoch #10500 Loss: 0.4168 Weighted Loss: 0.3666 accuracy: 0.8043 Test loss: 0.5890 Test accuracy: 0.6964\n",
      "Epoch #11000 Loss: 0.4574 Weighted Loss: 0.3848 accuracy: 0.7907 Test loss: 0.6468 Test accuracy: 0.6997\n",
      "Epoch #11500 Loss: 0.4120 Weighted Loss: 0.3601 accuracy: 0.8159 Test loss: 0.5799 Test accuracy: 0.7087\n",
      "Epoch #12000 Loss: 0.4257 Weighted Loss: 0.3705 accuracy: 0.8120 Test loss: 0.5939 Test accuracy: 0.6972\n",
      "Epoch #12500 Loss: 0.4043 Weighted Loss: 0.3330 accuracy: 0.8069 Test loss: 0.5757 Test accuracy: 0.7170\n",
      "Epoch #13000 Loss: 0.4257 Weighted Loss: 0.3689 accuracy: 0.8124 Test loss: 0.5841 Test accuracy: 0.7046\n",
      "Epoch #13500 Loss: 0.4149 Weighted Loss: 0.3544 accuracy: 0.8171 Test loss: 0.5729 Test accuracy: 0.7335\n",
      "Epoch #14000 Loss: 0.4314 Weighted Loss: 0.3464 accuracy: 0.7980 Test loss: 0.7109 Test accuracy: 0.7063\n",
      "Epoch #14500 Loss: 0.3967 Weighted Loss: 0.3366 accuracy: 0.8171 Test loss: 0.5577 Test accuracy: 0.7285\n",
      "Epoch #15000 Loss: 0.4209 Weighted Loss: 0.3647 accuracy: 0.8137 Test loss: 0.6004 Test accuracy: 0.7104\n",
      "Epoch #15500 Loss: 0.3811 Weighted Loss: 0.3119 accuracy: 0.8231 Test loss: 0.5792 Test accuracy: 0.7384\n",
      "Epoch #16000 Loss: 0.3945 Weighted Loss: 0.3201 accuracy: 0.8154 Test loss: 0.6012 Test accuracy: 0.7252\n",
      "Epoch #16500 Loss: 0.4274 Weighted Loss: 0.3535 accuracy: 0.7958 Test loss: 0.5980 Test accuracy: 0.7261\n",
      "Epoch #17000 Loss: 0.3895 Weighted Loss: 0.3205 accuracy: 0.8214 Test loss: 0.5753 Test accuracy: 0.7302\n",
      "Epoch #17500 Loss: 0.3856 Weighted Loss: 0.3097 accuracy: 0.8265 Test loss: 0.5638 Test accuracy: 0.7318\n",
      "Epoch #18000 Loss: 0.4251 Weighted Loss: 0.3331 accuracy: 0.7975 Test loss: 0.6361 Test accuracy: 0.7038\n",
      "Epoch #18500 Loss: 0.3786 Weighted Loss: 0.3076 accuracy: 0.8218 Test loss: 0.5982 Test accuracy: 0.7343\n",
      "Epoch #19000 Loss: 0.4340 Weighted Loss: 0.3669 accuracy: 0.8176 Test loss: 0.6058 Test accuracy: 0.7186\n",
      "Epoch #19500 Loss: 0.3861 Weighted Loss: 0.3002 accuracy: 0.8223 Test loss: 0.5661 Test accuracy: 0.7376\n",
      "Epoch #20000 Loss: 0.3876 Weighted Loss: 0.3030 accuracy: 0.8218 Test loss: 0.5587 Test accuracy: 0.7401\n",
      "Epoch #20500 Loss: 0.3882 Weighted Loss: 0.3236 accuracy: 0.8274 Test loss: 0.5891 Test accuracy: 0.7343\n",
      "Epoch #21000 Loss: 0.4009 Weighted Loss: 0.3182 accuracy: 0.8171 Test loss: 0.6466 Test accuracy: 0.7162\n",
      "Epoch #21500 Loss: 0.3965 Weighted Loss: 0.3325 accuracy: 0.8240 Test loss: 0.5767 Test accuracy: 0.7327\n",
      "Epoch #22000 Loss: 0.3799 Weighted Loss: 0.2985 accuracy: 0.8244 Test loss: 0.5689 Test accuracy: 0.7401\n",
      "Epoch #22500 Loss: 0.3892 Weighted Loss: 0.3004 accuracy: 0.8227 Test loss: 0.6037 Test accuracy: 0.7269\n",
      "Epoch #23000 Loss: 0.3799 Weighted Loss: 0.3114 accuracy: 0.8261 Test loss: 0.6093 Test accuracy: 0.7368\n",
      "Epoch #23500 Loss: 0.3825 Weighted Loss: 0.2940 accuracy: 0.8261 Test loss: 0.5568 Test accuracy: 0.7517\n",
      "Epoch #24000 Loss: 0.3759 Weighted Loss: 0.2991 accuracy: 0.8282 Test loss: 0.5793 Test accuracy: 0.7417\n",
      "Epoch #24500 Loss: 0.3763 Weighted Loss: 0.3113 accuracy: 0.8295 Test loss: 0.5809 Test accuracy: 0.7327\n",
      "Epoch #25000 Loss: 0.4218 Weighted Loss: 0.3346 accuracy: 0.8061 Test loss: 0.5988 Test accuracy: 0.7384\n",
      "Epoch #25500 Loss: 0.3782 Weighted Loss: 0.2841 accuracy: 0.8308 Test loss: 0.6165 Test accuracy: 0.7360\n",
      "Epoch #26000 Loss: 0.3715 Weighted Loss: 0.2926 accuracy: 0.8286 Test loss: 0.5617 Test accuracy: 0.7384\n",
      "Epoch #26500 Loss: 0.3865 Weighted Loss: 0.2923 accuracy: 0.8214 Test loss: 0.6296 Test accuracy: 0.7368\n",
      "Epoch #27000 Loss: 0.3725 Weighted Loss: 0.2886 accuracy: 0.8303 Test loss: 0.5897 Test accuracy: 0.7450\n",
      "Epoch #27500 Loss: 0.3793 Weighted Loss: 0.2834 accuracy: 0.8261 Test loss: 0.5788 Test accuracy: 0.7459\n",
      "Epoch #28000 Loss: 0.3739 Weighted Loss: 0.2907 accuracy: 0.8235 Test loss: 0.6124 Test accuracy: 0.7417\n",
      "Epoch #28500 Loss: 0.3828 Weighted Loss: 0.2822 accuracy: 0.8240 Test loss: 0.5813 Test accuracy: 0.7533\n",
      "Epoch #29000 Loss: 0.3748 Weighted Loss: 0.2791 accuracy: 0.8316 Test loss: 0.5884 Test accuracy: 0.7467\n",
      "Epoch #29500 Loss: 0.3733 Weighted Loss: 0.2782 accuracy: 0.8346 Test loss: 0.6188 Test accuracy: 0.7450\n",
      "Epoch #30000 Loss: 0.3722 Weighted Loss: 0.2785 accuracy: 0.8338 Test loss: 0.6259 Test accuracy: 0.7434\n",
      "Epoch #30500 Loss: 0.3855 Weighted Loss: 0.2876 accuracy: 0.8235 Test loss: 0.6338 Test accuracy: 0.7351\n",
      "Epoch #31000 Loss: 0.3671 Weighted Loss: 0.2776 accuracy: 0.8397 Test loss: 0.5862 Test accuracy: 0.7450\n",
      "Epoch #31500 Loss: 0.3618 Weighted Loss: 0.2772 accuracy: 0.8308 Test loss: 0.6132 Test accuracy: 0.7401\n",
      "Epoch #32000 Loss: 0.3636 Weighted Loss: 0.2812 accuracy: 0.8329 Test loss: 0.5929 Test accuracy: 0.7492\n",
      "Epoch #32500 Loss: 0.3770 Weighted Loss: 0.2711 accuracy: 0.8303 Test loss: 0.5907 Test accuracy: 0.7401\n",
      "Epoch #33000 Loss: 0.3703 Weighted Loss: 0.2724 accuracy: 0.8333 Test loss: 0.5901 Test accuracy: 0.7401\n",
      "Epoch #33500 Loss: 0.3500 Weighted Loss: 0.2661 accuracy: 0.8440 Test loss: 0.5906 Test accuracy: 0.7450\n",
      "Epoch #34000 Loss: 0.3551 Weighted Loss: 0.2687 accuracy: 0.8308 Test loss: 0.6108 Test accuracy: 0.7426\n",
      "Epoch #34500 Loss: 0.3755 Weighted Loss: 0.2885 accuracy: 0.8308 Test loss: 0.6582 Test accuracy: 0.7310\n",
      "Epoch #35000 Loss: 0.3538 Weighted Loss: 0.2621 accuracy: 0.8453 Test loss: 0.6089 Test accuracy: 0.7426\n",
      "Epoch #35500 Loss: 0.3742 Weighted Loss: 0.2662 accuracy: 0.8329 Test loss: 0.6019 Test accuracy: 0.7442\n",
      "Epoch #36000 Loss: 0.3690 Weighted Loss: 0.2656 accuracy: 0.8355 Test loss: 0.6139 Test accuracy: 0.7360\n",
      "Epoch #36500 Loss: 0.3722 Weighted Loss: 0.2671 accuracy: 0.8346 Test loss: 0.6104 Test accuracy: 0.7327\n",
      "Epoch #37000 Loss: 0.3607 Weighted Loss: 0.2606 accuracy: 0.8436 Test loss: 0.6135 Test accuracy: 0.7459\n",
      "Epoch #37500 Loss: 0.3641 Weighted Loss: 0.2644 accuracy: 0.8410 Test loss: 0.6321 Test accuracy: 0.7409\n",
      "Epoch #38000 Loss: 0.3613 Weighted Loss: 0.2599 accuracy: 0.8461 Test loss: 0.6251 Test accuracy: 0.7360\n",
      "Epoch #38500 Loss: 0.3605 Weighted Loss: 0.2572 accuracy: 0.8419 Test loss: 0.6215 Test accuracy: 0.7450\n",
      "Epoch #39000 Loss: 0.3584 Weighted Loss: 0.2575 accuracy: 0.8410 Test loss: 0.6114 Test accuracy: 0.7483\n",
      "Epoch #39500 Loss: 0.3733 Weighted Loss: 0.2584 accuracy: 0.8350 Test loss: 0.6267 Test accuracy: 0.7376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #40000 Loss: 0.3679 Weighted Loss: 0.2600 accuracy: 0.8380 Test loss: 0.6288 Test accuracy: 0.7327\n",
      "Epoch #40500 Loss: 0.3557 Weighted Loss: 0.2525 accuracy: 0.8470 Test loss: 0.6319 Test accuracy: 0.7393\n",
      "Epoch #41000 Loss: 0.3603 Weighted Loss: 0.2531 accuracy: 0.8423 Test loss: 0.6385 Test accuracy: 0.7500\n",
      "Epoch #41500 Loss: 0.3575 Weighted Loss: 0.2516 accuracy: 0.8402 Test loss: 0.6580 Test accuracy: 0.7401\n",
      "Epoch #42000 Loss: 0.3588 Weighted Loss: 0.2542 accuracy: 0.8453 Test loss: 0.6394 Test accuracy: 0.7368\n",
      "Epoch #42500 Loss: 0.3542 Weighted Loss: 0.2487 accuracy: 0.8474 Test loss: 0.6380 Test accuracy: 0.7393\n",
      "Epoch #43000 Loss: 0.3632 Weighted Loss: 0.2540 accuracy: 0.8406 Test loss: 0.6487 Test accuracy: 0.7327\n",
      "Epoch #43500 Loss: 0.3573 Weighted Loss: 0.2487 accuracy: 0.8440 Test loss: 0.6436 Test accuracy: 0.7343\n",
      "Epoch #44000 Loss: 0.3584 Weighted Loss: 0.2485 accuracy: 0.8419 Test loss: 0.6480 Test accuracy: 0.7327\n",
      "Epoch #44500 Loss: 0.3560 Weighted Loss: 0.2475 accuracy: 0.8461 Test loss: 0.6345 Test accuracy: 0.7434\n",
      "Epoch #45000 Loss: 0.3613 Weighted Loss: 0.2478 accuracy: 0.8427 Test loss: 0.6411 Test accuracy: 0.7393\n",
      "Epoch #45500 Loss: 0.3638 Weighted Loss: 0.2484 accuracy: 0.8414 Test loss: 0.6437 Test accuracy: 0.7393\n",
      "Epoch #46000 Loss: 0.3666 Weighted Loss: 0.2504 accuracy: 0.8410 Test loss: 0.6452 Test accuracy: 0.7393\n",
      "Epoch #46500 Loss: 0.3560 Weighted Loss: 0.2444 accuracy: 0.8440 Test loss: 0.6614 Test accuracy: 0.7483\n",
      "Epoch #47000 Loss: 0.3579 Weighted Loss: 0.2451 accuracy: 0.8457 Test loss: 0.6559 Test accuracy: 0.7409\n",
      "Epoch #47500 Loss: 0.3551 Weighted Loss: 0.2451 accuracy: 0.8478 Test loss: 0.6655 Test accuracy: 0.7475\n",
      "Epoch #48000 Loss: 0.3558 Weighted Loss: 0.2434 accuracy: 0.8440 Test loss: 0.6495 Test accuracy: 0.7434\n",
      "Epoch #48500 Loss: 0.3542 Weighted Loss: 0.2436 accuracy: 0.8495 Test loss: 0.6616 Test accuracy: 0.7401\n",
      "Epoch #49000 Loss: 0.3555 Weighted Loss: 0.2419 accuracy: 0.8423 Test loss: 0.6591 Test accuracy: 0.7442\n",
      "Epoch #49500 Loss: 0.3562 Weighted Loss: 0.2410 accuracy: 0.8431 Test loss: 0.6604 Test accuracy: 0.7483\n",
      "Epoch #50000 Loss: 0.3560 Weighted Loss: 0.2414 accuracy: 0.8440 Test loss: 0.6645 Test accuracy: 0.7426\n"
     ]
    }
   ],
   "source": [
    "# Training with negative weights!\n",
    "NUM_EPOCHS = 50000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_my_weights_loss, epoch_loss, epoch_accuracy,epoch_output, _ = sess.run([my_weights_loss, loss, accuracy,output, train_step],feed_dict={x_input: \n",
    "                                                                                         features,y: labels_with_missing_positives,positive_weights: train_positive_weights,\n",
    "                                                                                                  negative_weights: train_negative_weights})\n",
    "        if (epoch+1)% 500 == 0:\n",
    "            val_losses, val_accuracies, val_output,current_learning_rate = sess.run([loss, accuracy,output,learning_rate],feed_dict={\n",
    "                                                                                          x_input: test_features,\n",
    "                                                                                          y:test_labels})\n",
    "            print(\"Epoch #{}\".format(epoch+1), \"Loss: {:.4f}\".format(epoch_loss), \n",
    "                  \"Weighted Loss: {:.4f}\".format(epoch_my_weights_loss),\"accuracy: {:.4f}\".format(epoch_accuracy), \n",
    "                  \"Test loss: {:.4f}\".format(val_losses), \"Test accuracy: {:.4f}\".format(val_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing correlations for negative weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_labeles_probabilities(hot_encoded,LABELS_LIST):\n",
    "    # count the number of times a combination has appeared with the negative label as 0 / the total number of\n",
    "    # occurances of that combination without the negative label\n",
    "    negative_weights = np.zeros([len(hot_encoded), len(LABELS_LIST)])\n",
    "    for sample_idx in range(len(hot_encoded)):\n",
    "        for label_idx in range(len(LABELS_LIST)):\n",
    "            if hot_encoded.iloc[sample_idx, label_idx] == 1:\n",
    "                negative_weights[sample_idx, label_idx] = 0\n",
    "            else:\n",
    "                temp_combination = hot_encoded.iloc[sample_idx,:].copy()\n",
    "                temp_combination[label_idx] = 1\n",
    "                positive_samples = len(hot_encoded[(hot_encoded.iloc[:, :].values == temp_combination.values).all(axis = 1)])\n",
    "                negative_samples = len(hot_encoded[(hot_encoded.iloc[:, :].values == hot_encoded.iloc[sample_idx, :].values).all(axis=1)])\n",
    "                negative_weights[sample_idx, label_idx] = negative_samples / (positive_samples + negative_samples)\n",
    "    negative_weights_df = pd.DataFrame(negative_weights, columns=LABELS_LIST)\n",
    "    #negative_weights_df[\"song_id\"] = hot_encoded.song_id\n",
    "    #negative_weights_df = negative_weights_df[[\"song_id\"] + LABELS_LIST]\n",
    "    #negative_weights_df.to_csv(\"/home/karim/Documents/BalancedDatasetDeezer/GroundTruth/negative_weights.csv\",index=False)\n",
    "    return negative_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "negative_weights_df = negative_labeles_probabilities(pd.DataFrame(labels_with_missing_positives), LABELS_LIST = LABEL_LIST)\n",
    "hidden_weights = []\n",
    "for x in range(len(indices_to_hide[0])):\n",
    "        hidden_weights.append(negative_weights_df.values[indices_to_hide[0][x]][indices_to_hide[1][x]])\n",
    "        \n",
    "correlations_based_weights = negative_weights_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def negative_labels_probabilities_pairwise(hot_encoded,LABELS_LIST):\n",
    "    # count the number of times a combination has appeared with the negative label as 1 / the total number of\n",
    "    # occurances of that combination without the negative label\n",
    "    negative_weights = np.ones([len(hot_encoded), len(LABELS_LIST)])\n",
    "    for sample_idx in range(len(hot_encoded)):\n",
    "        for label_idx in range(len(LABELS_LIST)):\n",
    "            if hot_encoded.iloc[sample_idx, label_idx+1] == 1:\n",
    "                negative_weights[sample_idx, label_idx] = 0\n",
    "            else:\n",
    "                for other_labels_index in range(len(LABELS_LIST)):\n",
    "                    # Iterate through all other labels for each label_index\n",
    "                    # check patterns if the other label == 1 (weight is #times occured as 0 / number of times occured as 0 or 1 for the target label)\n",
    "                    if (other_labels_index != label_idx) and (hot_encoded.iloc[sample_idx, other_labels_index] == 1):\n",
    "                        positive_occurances = len(hot_encoded[(hot_encoded.iloc[:, [label_idx,other_labels_index]].values == [1,1]).all(axis = 1)])\n",
    "                        negative_occurances = len(hot_encoded[(hot_encoded.iloc[:, [label_idx,other_labels_index]].values == [0,1]).all(axis = 1)])\n",
    "                        weight = negative_occurances / (negative_occurances + positive_occurances)\n",
    "                    # Check the patterns if the other labels == 0\n",
    "                    if (other_labels_index != label_idx) and (hot_encoded.iloc[sample_idx, other_labels_index] == 0):\n",
    "                        positive_occurances = len(hot_encoded[(hot_encoded.iloc[:, [label_idx,other_labels_index]].values == [1,0]).all(axis = 1)])\n",
    "                        negative_occurances = len(hot_encoded[(hot_encoded.iloc[:, [label_idx,other_labels_index]].values == [0,0]).all(axis = 1)])\n",
    "                        weight = negative_occurances / (negative_occurances + positive_occurances)\n",
    "                    negative_weights[sample_idx, label_idx] += weight\n",
    "                negative_weights[sample_idx, label_idx] /= (len(LABELS_LIST))\n",
    "    negative_weights_df = pd.DataFrame(negative_weights, columns=LABELS_LIST)\n",
    "    negative_weights_df[\"song_id\"] = hot_encoded.song_id\n",
    "    negative_weights_df = negative_weights_df[[\"song_id\"] + LABELS_LIST]\n",
    "    #negative_weights_df.to_csv(\"/home/karim/Documents/BalancedDatasetDeezer/GroundTruth/negative_weights_paris.csv\",index=False)\n",
    "    return negative_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #500 Loss: 0.4679 Weighted Loss: 0.4345 accuracy: 0.8188 Test loss: 0.6502 Test accuracy: 0.6708\n",
      "Epoch #1000 Loss: 0.4559 Weighted Loss: 0.4248 accuracy: 0.8180 Test loss: 0.6336 Test accuracy: 0.6716\n",
      "Epoch #1500 Loss: 0.4920 Weighted Loss: 0.4622 accuracy: 0.8163 Test loss: 0.6275 Test accuracy: 0.6757\n",
      "Epoch #2000 Loss: 0.4430 Weighted Loss: 0.4061 accuracy: 0.8214 Test loss: 0.6879 Test accuracy: 0.6733\n",
      "Epoch #2500 Loss: 0.4445 Weighted Loss: 0.4092 accuracy: 0.8099 Test loss: 0.6258 Test accuracy: 0.6955\n",
      "Epoch #3000 Loss: 0.4175 Weighted Loss: 0.3821 accuracy: 0.8210 Test loss: 0.6289 Test accuracy: 0.6823\n",
      "Epoch #3500 Loss: 0.4351 Weighted Loss: 0.3979 accuracy: 0.8159 Test loss: 0.6851 Test accuracy: 0.6741\n",
      "Epoch #4000 Loss: 0.4168 Weighted Loss: 0.3860 accuracy: 0.8257 Test loss: 0.5865 Test accuracy: 0.6931\n",
      "Epoch #4500 Loss: 0.4123 Weighted Loss: 0.3815 accuracy: 0.8248 Test loss: 0.5864 Test accuracy: 0.6898\n",
      "Epoch #5000 Loss: 0.4067 Weighted Loss: 0.3720 accuracy: 0.8227 Test loss: 0.5982 Test accuracy: 0.6931\n",
      "Epoch #5500 Loss: 0.4031 Weighted Loss: 0.3680 accuracy: 0.8214 Test loss: 0.5918 Test accuracy: 0.7046\n",
      "Epoch #6000 Loss: 0.4061 Weighted Loss: 0.3735 accuracy: 0.8210 Test loss: 0.6119 Test accuracy: 0.7063\n",
      "Epoch #6500 Loss: 0.4451 Weighted Loss: 0.4058 accuracy: 0.8099 Test loss: 0.6804 Test accuracy: 0.6774\n",
      "Epoch #7000 Loss: 0.4031 Weighted Loss: 0.3674 accuracy: 0.8240 Test loss: 0.6002 Test accuracy: 0.6972\n",
      "Epoch #7500 Loss: 0.3943 Weighted Loss: 0.3588 accuracy: 0.8227 Test loss: 0.5929 Test accuracy: 0.6988\n",
      "Epoch #8000 Loss: 0.3894 Weighted Loss: 0.3522 accuracy: 0.8286 Test loss: 0.6383 Test accuracy: 0.6898\n",
      "Epoch #8500 Loss: 0.3924 Weighted Loss: 0.3543 accuracy: 0.8316 Test loss: 0.6641 Test accuracy: 0.6906\n",
      "Epoch #9000 Loss: 0.3811 Weighted Loss: 0.3461 accuracy: 0.8231 Test loss: 0.6033 Test accuracy: 0.7211\n",
      "Epoch #9500 Loss: 0.3943 Weighted Loss: 0.3562 accuracy: 0.8180 Test loss: 0.6515 Test accuracy: 0.6947\n",
      "Epoch #10000 Loss: 0.3777 Weighted Loss: 0.3396 accuracy: 0.8265 Test loss: 0.6212 Test accuracy: 0.6980\n",
      "Epoch #10500 Loss: 0.3840 Weighted Loss: 0.3507 accuracy: 0.8295 Test loss: 0.5938 Test accuracy: 0.7079\n",
      "Epoch #11000 Loss: 0.4048 Weighted Loss: 0.3667 accuracy: 0.8176 Test loss: 0.7173 Test accuracy: 0.6848\n",
      "Epoch #11500 Loss: 0.3848 Weighted Loss: 0.3511 accuracy: 0.8214 Test loss: 0.6127 Test accuracy: 0.7046\n",
      "Epoch #12000 Loss: 0.3658 Weighted Loss: 0.3302 accuracy: 0.8333 Test loss: 0.6144 Test accuracy: 0.7285\n",
      "Epoch #12500 Loss: 0.3860 Weighted Loss: 0.3481 accuracy: 0.8295 Test loss: 0.6691 Test accuracy: 0.7013\n",
      "Epoch #13000 Loss: 0.3732 Weighted Loss: 0.3365 accuracy: 0.8329 Test loss: 0.6064 Test accuracy: 0.7186\n",
      "Epoch #13500 Loss: 0.3631 Weighted Loss: 0.3280 accuracy: 0.8363 Test loss: 0.5997 Test accuracy: 0.7178\n",
      "Epoch #14000 Loss: 0.3681 Weighted Loss: 0.3343 accuracy: 0.8346 Test loss: 0.6103 Test accuracy: 0.7285\n",
      "Epoch #14500 Loss: 0.3750 Weighted Loss: 0.3378 accuracy: 0.8376 Test loss: 0.6564 Test accuracy: 0.7137\n",
      "Epoch #15000 Loss: 0.3591 Weighted Loss: 0.3241 accuracy: 0.8406 Test loss: 0.6359 Test accuracy: 0.7054\n",
      "Epoch #15500 Loss: 0.3681 Weighted Loss: 0.3301 accuracy: 0.8286 Test loss: 0.6988 Test accuracy: 0.6906\n",
      "Epoch #16000 Loss: 0.3672 Weighted Loss: 0.3289 accuracy: 0.8342 Test loss: 0.6861 Test accuracy: 0.7063\n",
      "Epoch #16500 Loss: 0.3811 Weighted Loss: 0.3433 accuracy: 0.8193 Test loss: 0.6006 Test accuracy: 0.7384\n",
      "Epoch #17000 Loss: 0.3657 Weighted Loss: 0.3270 accuracy: 0.8295 Test loss: 0.6570 Test accuracy: 0.7129\n",
      "Epoch #17500 Loss: 0.3724 Weighted Loss: 0.3381 accuracy: 0.8342 Test loss: 0.6274 Test accuracy: 0.7269\n",
      "Epoch #18000 Loss: 0.3658 Weighted Loss: 0.3273 accuracy: 0.8231 Test loss: 0.6518 Test accuracy: 0.7063\n",
      "Epoch #18500 Loss: 0.3615 Weighted Loss: 0.3229 accuracy: 0.8257 Test loss: 0.6532 Test accuracy: 0.7186\n",
      "Epoch #19000 Loss: 0.3600 Weighted Loss: 0.3248 accuracy: 0.8380 Test loss: 0.6578 Test accuracy: 0.7269\n",
      "Epoch #19500 Loss: 0.3637 Weighted Loss: 0.3256 accuracy: 0.8325 Test loss: 0.6906 Test accuracy: 0.6997\n",
      "Epoch #20000 Loss: 0.3478 Weighted Loss: 0.3105 accuracy: 0.8359 Test loss: 0.6919 Test accuracy: 0.7030\n",
      "Epoch #20500 Loss: 0.3543 Weighted Loss: 0.3144 accuracy: 0.8389 Test loss: 0.7340 Test accuracy: 0.6832\n",
      "Epoch #21000 Loss: 0.3425 Weighted Loss: 0.3062 accuracy: 0.8419 Test loss: 0.6473 Test accuracy: 0.7219\n",
      "Epoch #21500 Loss: 0.3498 Weighted Loss: 0.3125 accuracy: 0.8312 Test loss: 0.6816 Test accuracy: 0.7120\n",
      "Epoch #22000 Loss: 0.3419 Weighted Loss: 0.3085 accuracy: 0.8402 Test loss: 0.6935 Test accuracy: 0.7269\n",
      "Epoch #22500 Loss: 0.3492 Weighted Loss: 0.3120 accuracy: 0.8342 Test loss: 0.6401 Test accuracy: 0.7203\n",
      "Epoch #23000 Loss: 0.3298 Weighted Loss: 0.2947 accuracy: 0.8478 Test loss: 0.6806 Test accuracy: 0.7269\n",
      "Epoch #23500 Loss: 0.3282 Weighted Loss: 0.2901 accuracy: 0.8457 Test loss: 0.7226 Test accuracy: 0.7071\n",
      "Epoch #24000 Loss: 0.3313 Weighted Loss: 0.2939 accuracy: 0.8444 Test loss: 0.6829 Test accuracy: 0.7186\n",
      "Epoch #24500 Loss: 0.3426 Weighted Loss: 0.3075 accuracy: 0.8397 Test loss: 0.6156 Test accuracy: 0.7360\n",
      "Epoch #25000 Loss: 0.3377 Weighted Loss: 0.3012 accuracy: 0.8427 Test loss: 0.7118 Test accuracy: 0.7137\n",
      "Epoch #25500 Loss: 0.3424 Weighted Loss: 0.3057 accuracy: 0.8423 Test loss: 0.7084 Test accuracy: 0.7153\n",
      "Epoch #26000 Loss: 0.3489 Weighted Loss: 0.3111 accuracy: 0.8333 Test loss: 0.6310 Test accuracy: 0.7285\n",
      "Epoch #26500 Loss: 0.3280 Weighted Loss: 0.2923 accuracy: 0.8431 Test loss: 0.6678 Test accuracy: 0.7244\n",
      "Epoch #27000 Loss: 0.3366 Weighted Loss: 0.2986 accuracy: 0.8470 Test loss: 0.7618 Test accuracy: 0.7096\n",
      "Epoch #27500 Loss: 0.3286 Weighted Loss: 0.2940 accuracy: 0.8440 Test loss: 0.7075 Test accuracy: 0.7236\n",
      "Epoch #28000 Loss: 0.3448 Weighted Loss: 0.3071 accuracy: 0.8329 Test loss: 0.7677 Test accuracy: 0.7063\n",
      "Epoch #28500 Loss: 0.3264 Weighted Loss: 0.2887 accuracy: 0.8474 Test loss: 0.7518 Test accuracy: 0.6988\n",
      "Epoch #29000 Loss: 0.3305 Weighted Loss: 0.2911 accuracy: 0.8448 Test loss: 0.7339 Test accuracy: 0.6931\n",
      "Epoch #29500 Loss: 0.3185 Weighted Loss: 0.2845 accuracy: 0.8546 Test loss: 0.6531 Test accuracy: 0.7310\n",
      "Epoch #30000 Loss: 0.3246 Weighted Loss: 0.2877 accuracy: 0.8427 Test loss: 0.7029 Test accuracy: 0.7063\n",
      "Epoch #30500 Loss: 0.3168 Weighted Loss: 0.2821 accuracy: 0.8542 Test loss: 0.7053 Test accuracy: 0.7195\n",
      "Epoch #31000 Loss: 0.3271 Weighted Loss: 0.2907 accuracy: 0.8500 Test loss: 0.6607 Test accuracy: 0.7294\n",
      "Epoch #31500 Loss: 0.3108 Weighted Loss: 0.2750 accuracy: 0.8593 Test loss: 0.6993 Test accuracy: 0.7186\n",
      "Epoch #32000 Loss: 0.3178 Weighted Loss: 0.2820 accuracy: 0.8512 Test loss: 0.6812 Test accuracy: 0.7186\n",
      "Epoch #32500 Loss: 0.3121 Weighted Loss: 0.2765 accuracy: 0.8546 Test loss: 0.7807 Test accuracy: 0.7096\n",
      "Epoch #33000 Loss: 0.3297 Weighted Loss: 0.2935 accuracy: 0.8453 Test loss: 0.6766 Test accuracy: 0.7327\n",
      "Epoch #33500 Loss: 0.3066 Weighted Loss: 0.2706 accuracy: 0.8581 Test loss: 0.7243 Test accuracy: 0.7129\n",
      "Epoch #34000 Loss: 0.3098 Weighted Loss: 0.2736 accuracy: 0.8576 Test loss: 0.7053 Test accuracy: 0.7195\n",
      "Epoch #34500 Loss: 0.3080 Weighted Loss: 0.2723 accuracy: 0.8585 Test loss: 0.6853 Test accuracy: 0.7310\n",
      "Epoch #35000 Loss: 0.3035 Weighted Loss: 0.2679 accuracy: 0.8606 Test loss: 0.7174 Test accuracy: 0.7104\n",
      "Epoch #35500 Loss: 0.3039 Weighted Loss: 0.2676 accuracy: 0.8529 Test loss: 0.7918 Test accuracy: 0.7071\n",
      "Epoch #36000 Loss: 0.3185 Weighted Loss: 0.2816 accuracy: 0.8559 Test loss: 0.6958 Test accuracy: 0.7294\n",
      "Epoch #36500 Loss: 0.3010 Weighted Loss: 0.2660 accuracy: 0.8640 Test loss: 0.7685 Test accuracy: 0.7285\n",
      "Epoch #37000 Loss: 0.3005 Weighted Loss: 0.2660 accuracy: 0.8645 Test loss: 0.7219 Test accuracy: 0.7236\n",
      "Epoch #37500 Loss: 0.2989 Weighted Loss: 0.2623 accuracy: 0.8640 Test loss: 0.7730 Test accuracy: 0.7112\n",
      "Epoch #38000 Loss: 0.2926 Weighted Loss: 0.2584 accuracy: 0.8657 Test loss: 0.7766 Test accuracy: 0.7261\n",
      "Epoch #38500 Loss: 0.2952 Weighted Loss: 0.2607 accuracy: 0.8606 Test loss: 0.7855 Test accuracy: 0.7203\n",
      "Epoch #39000 Loss: 0.2997 Weighted Loss: 0.2630 accuracy: 0.8593 Test loss: 0.7630 Test accuracy: 0.7112\n",
      "Epoch #39500 Loss: 0.2959 Weighted Loss: 0.2602 accuracy: 0.8649 Test loss: 0.8109 Test accuracy: 0.7120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #40000 Loss: 0.2875 Weighted Loss: 0.2535 accuracy: 0.8679 Test loss: 0.7986 Test accuracy: 0.7186\n",
      "Epoch #40500 Loss: 0.2952 Weighted Loss: 0.2605 accuracy: 0.8589 Test loss: 0.8164 Test accuracy: 0.7178\n",
      "Epoch #41000 Loss: 0.2872 Weighted Loss: 0.2519 accuracy: 0.8717 Test loss: 0.7530 Test accuracy: 0.7294\n",
      "Epoch #41500 Loss: 0.2850 Weighted Loss: 0.2518 accuracy: 0.8674 Test loss: 0.7288 Test accuracy: 0.7285\n",
      "Epoch #42000 Loss: 0.2843 Weighted Loss: 0.2510 accuracy: 0.8725 Test loss: 0.7972 Test accuracy: 0.7219\n",
      "Epoch #42500 Loss: 0.2823 Weighted Loss: 0.2487 accuracy: 0.8632 Test loss: 0.7743 Test accuracy: 0.7236\n",
      "Epoch #43000 Loss: 0.2950 Weighted Loss: 0.2599 accuracy: 0.8581 Test loss: 0.8437 Test accuracy: 0.7170\n",
      "Epoch #43500 Loss: 0.2828 Weighted Loss: 0.2483 accuracy: 0.8725 Test loss: 0.7963 Test accuracy: 0.7203\n",
      "Epoch #44000 Loss: 0.2806 Weighted Loss: 0.2470 accuracy: 0.8717 Test loss: 0.7428 Test accuracy: 0.7244\n",
      "Epoch #44500 Loss: 0.2794 Weighted Loss: 0.2452 accuracy: 0.8743 Test loss: 0.7992 Test accuracy: 0.7195\n",
      "Epoch #45000 Loss: 0.2818 Weighted Loss: 0.2484 accuracy: 0.8696 Test loss: 0.8339 Test accuracy: 0.7145\n",
      "Epoch #45500 Loss: 0.2788 Weighted Loss: 0.2439 accuracy: 0.8700 Test loss: 0.7959 Test accuracy: 0.7129\n",
      "Epoch #46000 Loss: 0.2815 Weighted Loss: 0.2475 accuracy: 0.8713 Test loss: 0.8476 Test accuracy: 0.7219\n",
      "Epoch #46500 Loss: 0.2794 Weighted Loss: 0.2460 accuracy: 0.8713 Test loss: 0.8491 Test accuracy: 0.7211\n",
      "Epoch #47000 Loss: 0.2763 Weighted Loss: 0.2424 accuracy: 0.8662 Test loss: 0.8365 Test accuracy: 0.7195\n",
      "Epoch #47500 Loss: 0.2693 Weighted Loss: 0.2363 accuracy: 0.8789 Test loss: 0.8210 Test accuracy: 0.7236\n",
      "Epoch #48000 Loss: 0.2708 Weighted Loss: 0.2374 accuracy: 0.8764 Test loss: 0.8053 Test accuracy: 0.7186\n",
      "Epoch #48500 Loss: 0.2798 Weighted Loss: 0.2447 accuracy: 0.8760 Test loss: 0.7621 Test accuracy: 0.7252\n",
      "Epoch #49000 Loss: 0.2818 Weighted Loss: 0.2458 accuracy: 0.8721 Test loss: 0.7888 Test accuracy: 0.7219\n",
      "Epoch #49500 Loss: 0.2749 Weighted Loss: 0.2409 accuracy: 0.8645 Test loss: 0.8591 Test accuracy: 0.7120\n",
      "Epoch #50000 Loss: 0.2689 Weighted Loss: 0.2347 accuracy: 0.8777 Test loss: 0.8008 Test accuracy: 0.7244\n"
     ]
    }
   ],
   "source": [
    "# Training with negative weights!\n",
    "NUM_EPOCHS = 50000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_my_weights_loss, epoch_loss, epoch_accuracy,epoch_output, _ = sess.run([my_weights_loss, loss, accuracy,output, train_step],feed_dict={x_input: features,\n",
    "                                                                                                                                                     y: labels_with_missing_positives,positive_weights: train_positive_weights,\n",
    "                                                                                                                                                     negative_weights: correlations_based_weights})\n",
    "        if (epoch+1)% 500 == 0:\n",
    "            val_losses, val_accuracies, val_output,current_learning_rate = sess.run([loss, accuracy,output,learning_rate],feed_dict={\n",
    "                                                                                          x_input: test_features,\n",
    "                                                                                          y:test_labels})\n",
    "            print(\"Epoch #{}\".format(epoch+1), \"Loss: {:.4f}\".format(epoch_loss), \n",
    "                  \"Weighted Loss: {:.4f}\".format(epoch_my_weights_loss),\"accuracy: {:.4f}\".format(epoch_accuracy), \n",
    "                  \"Test loss: {:.4f}\".format(val_losses), \"Test accuracy: {:.4f}\".format(val_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC is:0.737\n",
      "F1 is:0.450\n",
      "Recall is:0.350\n",
      "Precision is:0.653\n",
      "Hamming loss is:0.276\n"
     ]
    }
   ],
   "source": [
    "# On test\n",
    "print(\"AUC is:{:.3f}\".format(roc_auc_score(test_labels, val_output)))\n",
    "print(\"F1 is:{:.3f}\".format(f1_score(test_labels, np.round(val_output),average=\"macro\")))\n",
    "print(\"Recall is:{:.3f}\".format(recall_score(test_labels, np.round(val_output),average=\"macro\")))\n",
    "print(\"Precision is:{:.3f}\".format(precision_score(test_labels, np.round(val_output),average=\"macro\")))\n",
    "print(\"Hamming loss is:{:.3f}\".format(hamming_loss(test_labels, np.round(val_output))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC is:0.868\n",
      "F1 is:0.572\n",
      "Recall is:0.450\n",
      "Precision is:0.872\n",
      "Hamming loss is:0.185\n"
     ]
    }
   ],
   "source": [
    "# On training \n",
    "print(\"AUC is:{:.3f}\".format(roc_auc_score(labels, epoch_output)))\n",
    "print(\"F1 is:{:.3f}\".format(f1_score(labels, np.round(epoch_output),average=\"macro\")))\n",
    "print(\"Recall is:{:.3f}\".format(recall_score(labels, np.round(epoch_output),average=\"macro\")))\n",
    "print(\"Precision is:{:.3f}\".format(precision_score(labels, np.round(epoch_output),average=\"macro\")))\n",
    "print(\"Hamming loss is:{:.3f}\".format(hamming_loss(labels, np.round(epoch_output))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
