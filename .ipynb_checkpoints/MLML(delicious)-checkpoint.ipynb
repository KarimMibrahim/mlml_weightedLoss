{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np \n",
    "import os\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, cohen_kappa_score, hamming_loss, zero_one_loss, coverage_error, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "from time import strftime, localtime\n",
    "from sklearn.model_selection import KFold\n",
    "random.seed = 11\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples is: 5000\n",
      "#features is: 550, #labels is: 33\n",
      "Ratio of positive labels is: 4.43%\n"
     ]
    }
   ],
   "source": [
    "# Reading dataset\n",
    "educationDataset = loadmat(\"/home/karim/Documents/research/sourceCode/MLML/mlml_weightedLoss/Datasets/MulanDatasets/education.mat\")\n",
    "\n",
    "features = educationDataset['train_data']\n",
    "test_features = educationDataset['test_data']\n",
    "\n",
    "labels = educationDataset['train_target'].T\n",
    "test_labels = educationDataset['test_target'].T\n",
    "\n",
    "\"\"\"\n",
    "Split ratio is strange: 40% training and 60% testing, we merge and data and resplit with 70/30 split\n",
    "\"\"\"\n",
    "features = np.append(features,test_features,axis = 0)\n",
    "labels = np.append(labels,test_labels,axis = 0)\n",
    "\n",
    "print(\"Number of samples is: {}\".format(len(features)))\n",
    "print(\"#features is: {}, #labels is: {}\".format(features.shape[1],labels.shape[1]))\n",
    "print(\"Ratio of positive labels is: {:.2f}%\".format(100 * labels.sum()/(labels.shape[0]*labels.shape[1])))\n",
    "\n",
    "splitter = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "#features, test_features, labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     1,
     6,
     12,
     18,
     27,
     52,
     68,
     78
    ]
   },
   "outputs": [],
   "source": [
    "# define helper functions\n",
    "def get_weights(shape):\n",
    "    w = tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "    #variable_summaries(w)\n",
    "    return w\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    b = tf.Variable(initial)\n",
    "    #variable_summaries(b)\n",
    "    return b\n",
    "\n",
    "def full_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = get_weights([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input, W) + b\n",
    "\n",
    "def weighted_loss(y_true, y_pred, positive_weights, negative_weights):\n",
    "    # clip to prevent NaN's and Inf's\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1-1e-7, name=None)\n",
    "    #y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # calc\n",
    "    loss = (-y_true * tf.log(y_pred) * positive_weights) - ((1.0 - y_true) * tf.log(1.0 - y_pred) * negative_weights)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "def evaluation_report(folds_metrics,evaluation_file_path,header_note):\n",
    "    metrics_mean = np.mean(folds_metrics, axis = 0)\n",
    "    metrics_std = np.std(folds_metrics, axis = 0) * 1.96 # for 95% confidence interval\n",
    "    print(header_note)\n",
    "    print(\"===================\")\n",
    "    print(\"Test set evaluation\")\n",
    "    print(\"Hamming loss is:{:.3f} (+/-{:.3f})\".format(metrics_mean[0],metrics_std[0]))\n",
    "    print(\"Zero-one loss is:{:.3f} (+/-{:.3f})\".format(metrics_mean[1],metrics_std[1]))\n",
    "    print(\"Coverage error is:{:.3f} (+/-{:.3f})\".format(metrics_mean[2],metrics_std[2]))\n",
    "    print(\"F1 is:{:.3f} (+/-{:.3f})\".format(metrics_mean[3],metrics_std[3]))\n",
    "    print(\"Recall is:{:.3f} (+/-{:.3f})\".format(metrics_mean[4],metrics_std[4]))\n",
    "    print(\"Precision is:{:.3f} (+/-{:.3f})\".format(metrics_mean[5],metrics_std[5]))\n",
    "    print(\"Average Precision is:{:.3f} (+/-{:.3f})\".format(metrics_mean[6],metrics_std[6]))\n",
    "    \n",
    "    with open(evaluation_file_path, \"a+\") as f:\n",
    "        f.write(\"===================\\n\" + header_note + \"\\n\")\n",
    "        f.write(\"Hamming loss is:{:.3f} (+/-{:.3f})\".format(metrics_mean[0],metrics_std[0]) + \"\\n\"+\n",
    "                \"Zero-one loss is:{:.3f} (+/-{:.3f})\".format(metrics_mean[1],metrics_std[1]) + \"\\n\"+\n",
    "                \"Coverage error is:{:.3f} (+/-{:.3f})\".format(metrics_mean[2],metrics_std[2]) + \"\\n\"+\n",
    "                \"F1 is:{:.3f} (+/-{:.3f})\".format(metrics_mean[3],metrics_std[3]) + \"\\n\"+\n",
    "                \"Recall is:{:.3f} (+/-{:.3f})\".format(metrics_mean[4],metrics_std[4]) + \"\\n\"+\n",
    "                \"Precision is:{:.3f} (+/-{:.3f})\".format(metrics_mean[5],metrics_std[5]) + \"\\n\"+\n",
    "                \"Average Precision is:{:.3f} (+/-{:.3f})\".format(metrics_mean[6],metrics_std[6])\n",
    "               + \"\\n\\n\")\n",
    "        \n",
    "def evaluation_metrics_per_fold(test_labels, test_probs):\n",
    "    # ignoring auc for now because of error of undefined case of a class with no ones\n",
    "    try:\n",
    "        auc = roc_auc_score(test_labels, val_output)\n",
    "    except:\n",
    "        pass\n",
    "    HL = hamming_loss(test_labels, np.round(test_probs))\n",
    "    one_loss = zero_one_loss(test_labels, np.round(test_probs))\n",
    "    cover = coverage_error(test_labels, test_probs)\n",
    "    f1 = f1_score(test_labels, np.round(test_probs),average=\"micro\")\n",
    "    recall = recall_score(test_labels, np.round(test_probs),average=\"micro\")\n",
    "    precision = precision_score(test_labels, np.round(test_probs),average=\"micro\")\n",
    "    average_precision = average_precision_score(test_labels, test_probs,average=\"micro\")\n",
    "    metrics = [HL, one_loss, cover, f1, recall, precision, average_precision]\n",
    "    return metrics\n",
    "         \n",
    "def hide_labels(train_labels,ratio_of_hidden_samples = 0.4):\n",
    "    ones_indices = np.nonzero(train_labels)\n",
    "    number_of_hidden_samples = int(len(ones_indices[0]) * ratio_of_hidden_samples)\n",
    "    random_indices = random.sample(list(np.arange(len(ones_indices[0]))),number_of_hidden_samples)\n",
    "    indices_to_hide = (ones_indices[0][random_indices] , ones_indices[1][random_indices])\n",
    "    labels_with_missing_positives = np.copy(train_labels)\n",
    "    for counter in range(number_of_hidden_samples):\n",
    "        labels_with_missing_positives[indices_to_hide[0][counter]][indices_to_hide[1][counter]] = 0\n",
    "    return labels_with_missing_positives, indices_to_hide\n",
    "\n",
    "def get_weights_for_hidden_labels(train_labels,indices_to_hide, pos_weights = 5):\n",
    "    train_negative_weights = np.zeros_like(train_labels) + 1 \n",
    "    train_positive_weights = np.zeros_like(train_labels) + pos_weights # We make positive weight 5 becuase of data imbalance\n",
    "    for counter in range (len(indices_to_hide[0])):\n",
    "        train_negative_weights[indices_to_hide[0][counter]][indices_to_hide[1][counter]] = 0\n",
    "    return train_negative_weights, train_positive_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1119 16:33:09.674114 140619841525504 deprecation.py:506] From <ipython-input-5-51a4f1d79c6b>:14: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Define a 3 layers network to train \n",
    "input_shape = features.shape[1]\n",
    "output_shape = labels.shape[1]\n",
    "hidden_layer_1_shape = 300\n",
    "hidden_layer_2_shape = 200\n",
    "hidden_layer_3_shape = 100\n",
    "\n",
    "y = tf.placeholder(tf.float32, [None, output_shape], name=\"true_labels\")\n",
    "x_input = tf.placeholder(tf.float32, [None,input_shape],name=\"input_layer\")\n",
    "current_keep_prob = tf.placeholder(tf.float32, name=\"dropout_rate\")\n",
    "h1 = tf.nn.tanh(full_layer(x_input, hidden_layer_1_shape))\n",
    "h2 = tf.nn.tanh(full_layer(h1, hidden_layer_2_shape))\n",
    "h3 = tf.nn.tanh(full_layer(h2, hidden_layer_3_shape))\n",
    "dropped = tf.nn.dropout(h3, keep_prob=current_keep_prob)\n",
    "logits = full_layer(dropped,output_shape)\n",
    "output = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Prepare results report \n",
    "Experiment_name = \"Education_weightedloss\"\n",
    "experiment_time = strftime(\"%d-%m_%H:%M\", localtime())\n",
    "saving_dir = \"/home/karim/Documents/research/sourceCode/MLML/mlml_weightedLoss/Experiment_results/\"\n",
    "exp_dir = os.path.join(saving_dir,Experiment_name,experiment_time)\n",
    "os.makedirs(exp_dir)\n",
    "model_output_dir = os.path.join(exp_dir,\"model_output\")\n",
    "os.mkdir(model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "# using weighted cross entropy because dataset is sparse and we need to weight positives more\n",
    "#loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "POSITIVE_WEIGHT = 5\n",
    "loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=logits, labels=y,pos_weight = POSITIVE_WEIGHT))\n",
    "\n",
    "# Learning rate decay\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(learning_rate=0.1, global_step=global_step, decay_steps=1000,\n",
    "                                          decay_rate=0.95,staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "correct_prediction = tf.equal(tf.round(output), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Evaluate on fold 1\n",
      "Epoch #3000 LR: 0.0857 Loss: 0.3386 accuracy: 0.9315 Test loss: 0.3298 Test accuracy: 0.9350\n",
      "Epoch #6000 LR: 0.0735 Loss: 0.3136 accuracy: 0.9344 Test loss: 0.3121 Test accuracy: 0.9370\n",
      "Epoch #9000 LR: 0.0630 Loss: 0.2994 accuracy: 0.9363 Test loss: 0.3077 Test accuracy: 0.9378\n",
      "Epoch #12000 LR: 0.0540 Loss: 0.2896 accuracy: 0.9364 Test loss: 0.3075 Test accuracy: 0.9376\n",
      "Epoch #15000 LR: 0.0463 Loss: 0.2802 accuracy: 0.9391 Test loss: 0.3086 Test accuracy: 0.9374\n",
      "Train/Evaluate on fold 2\n",
      "Epoch #3000 LR: 0.0857 Loss: 0.3420 accuracy: 0.9309 Test loss: 0.3311 Test accuracy: 0.9369\n",
      "Epoch #6000 LR: 0.0735 Loss: 0.3172 accuracy: 0.9344 Test loss: 0.3156 Test accuracy: 0.9367\n",
      "Epoch #9000 LR: 0.0630 Loss: 0.3020 accuracy: 0.9366 Test loss: 0.3101 Test accuracy: 0.9376\n",
      "Epoch #12000 LR: 0.0540 Loss: 0.2915 accuracy: 0.9374 Test loss: 0.3085 Test accuracy: 0.9379\n",
      "Epoch #15000 LR: 0.0463 Loss: 0.2815 accuracy: 0.9386 Test loss: 0.3090 Test accuracy: 0.9382\n",
      "Train/Evaluate on fold 3\n",
      "Epoch #3000 LR: 0.0857 Loss: 0.3396 accuracy: 0.9326 Test loss: 0.3227 Test accuracy: 0.9388\n",
      "Epoch #6000 LR: 0.0735 Loss: 0.3170 accuracy: 0.9350 Test loss: 0.3045 Test accuracy: 0.9394\n",
      "Epoch #9000 LR: 0.0630 Loss: 0.3033 accuracy: 0.9361 Test loss: 0.2979 Test accuracy: 0.9398\n",
      "Epoch #12000 LR: 0.0540 Loss: 0.2926 accuracy: 0.9363 Test loss: 0.2952 Test accuracy: 0.9400\n",
      "Epoch #15000 LR: 0.0463 Loss: 0.2831 accuracy: 0.9379 Test loss: 0.2946 Test accuracy: 0.9402\n",
      "Train/Evaluate on fold 4\n",
      "Epoch #3000 LR: 0.0857 Loss: 0.3346 accuracy: 0.9316 Test loss: 0.3392 Test accuracy: 0.9340\n",
      "Epoch #6000 LR: 0.0735 Loss: 0.3126 accuracy: 0.9353 Test loss: 0.3259 Test accuracy: 0.9364\n",
      "Epoch #9000 LR: 0.0630 Loss: 0.2965 accuracy: 0.9372 Test loss: 0.3209 Test accuracy: 0.9368\n",
      "Epoch #12000 LR: 0.0540 Loss: 0.2868 accuracy: 0.9378 Test loss: 0.3203 Test accuracy: 0.9366\n",
      "Epoch #15000 LR: 0.0463 Loss: 0.2789 accuracy: 0.9387 Test loss: 0.3217 Test accuracy: 0.9370\n",
      "Train/Evaluate on fold 5\n",
      "Epoch #3000 LR: 0.0857 Loss: 0.3352 accuracy: 0.9318 Test loss: 0.3424 Test accuracy: 0.9334\n",
      "Epoch #6000 LR: 0.0735 Loss: 0.3126 accuracy: 0.9341 Test loss: 0.3285 Test accuracy: 0.9341\n",
      "Epoch #9000 LR: 0.0630 Loss: 0.2980 accuracy: 0.9371 Test loss: 0.3242 Test accuracy: 0.9345\n",
      "Epoch #12000 LR: 0.0540 Loss: 0.2883 accuracy: 0.9385 Test loss: 0.3238 Test accuracy: 0.9344\n",
      "Epoch #15000 LR: 0.0463 Loss: 0.2825 accuracy: 0.9394 Test loss: 0.3251 Test accuracy: 0.9343\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "NUM_EPOCHS = 15000\n",
    "\n",
    "fold = 0\n",
    "folds_metrics = []\n",
    "for train_index, test_index in splitter.split(features, labels):\n",
    "    fold += 1\n",
    "    print(\"Train/Evaluate on fold %d\" % fold)\n",
    "    train_features, test_features = features[train_index], features[test_index]\n",
    "    train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            epoch_loss, epoch_accuracy,epoch_output, _ = sess.run([loss, accuracy,output, train_step],feed_dict={x_input: \n",
    "                                                                                             train_features,y: train_labels,\n",
    "                                                                                             current_keep_prob: 0.3,})\n",
    "            if (epoch+1)% 3000 == 0:\n",
    "                val_losses, val_accuracies, val_output,current_learning_rate = sess.run([loss, accuracy,output,learning_rate],feed_dict={\n",
    "                                                                                              x_input: test_features,\n",
    "                                                                                              y:test_labels,\n",
    "                                                                                              current_keep_prob: 1.0})\n",
    "                print(\"Epoch #{}\".format(epoch+1), \"LR: {:.4f}\".format(current_learning_rate),\n",
    "                      \"Loss: {:.4f}\".format(epoch_loss), \n",
    "                      \"accuracy: {:.4f}\".format(epoch_accuracy),\n",
    "                      \"Test loss: {:.4f}\".format(val_losses), \n",
    "                      \"Test accuracy: {:.4f}\".format(val_accuracies))\n",
    "    np.savetxt(os.path.join(model_output_dir,'[complete]groundtruth_' + str(fold) + '.out'), test_labels, delimiter=',')\n",
    "    np.savetxt(os.path.join(model_output_dir, '[complete]output' + str(fold) + '.out'), val_output, delimiter=',')\n",
    "    folds_metrics.append(evaluation_metrics_per_fold(test_labels,val_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nthresholds = np.arange(0, 1, 0.01)\\nf1_array = np.zeros((output_shape, len(thresholds)))\\nfor idx in range(output_shape):\\n    f1_array[idx, :] = [\\n        f1_score(labels[:, idx], np.clip(np.round(epoch_output[:, idx] - threshold + 0.5), 0, 1))\\n        for threshold in thresholds]\\nthreshold_arg = np.argmax(f1_array, axis=1)\\nthreshold_per_class = thresholds[threshold_arg]\\n\\n# Applying thresholds optimized per class\\nmodel_output_rounded = np.zeros_like(epoch_output)\\nfor idx in range(output_shape):\\n    model_output_rounded[:, idx] = np.clip(np.round(epoch_output[:, idx] - threshold_per_class[idx] + 0.5), 0, 1)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjusting threshold \n",
    "\"\"\"\n",
    "thresholds = np.arange(0, 1, 0.01)\n",
    "f1_array = np.zeros((output_shape, len(thresholds)))\n",
    "for idx in range(output_shape):\n",
    "    f1_array[idx, :] = [\n",
    "        f1_score(train_labels[:, idx], np.clip(np.round(epoch_output[:, idx] - threshold + 0.5), 0, 1))\n",
    "        for threshold in thresholds]\n",
    "threshold_arg = np.argmax(f1_array, axis=1)\n",
    "threshold_per_class = thresholds[threshold_arg]\n",
    "\n",
    "# Applying thresholds optimized per class\n",
    "model_output_rounded = np.zeros_like(epoch_output)\n",
    "for idx in range(output_shape):\n",
    "    model_output_rounded[:, idx] = np.clip(np.round(epoch_output[:, idx] - threshold_per_class[idx] + 0.5), 0, 1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing labels\n",
      "===================\n",
      "Test set evaluation\n",
      "Hamming loss is:0.063 (+/-0.002)\n",
      "Zero-one loss is:0.868 (+/-0.012)\n",
      "Coverage error is:4.442 (+/-0.168)\n",
      "F1 is:0.449 (+/-0.016)\n",
      "Recall is:0.575 (+/-0.024)\n",
      "Precision is:0.368 (+/-0.013)\n",
      "Average Precision is:0.479 (+/-0.023)\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "evaluation_report(folds_metrics,os.path.join(exp_dir,\"evaluation_report.txt\"),\"No missing labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With missing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Evaluate on fold 1\n",
      "Epoch #2000 LR: 0.0903 Loss: 0.2797 accuracy: 0.9568 Test loss: 0.3765 Test accuracy: 0.9448\n",
      "Epoch #4000 LR: 0.0815 Loss: 0.2657 accuracy: 0.9564 Test loss: 0.3577 Test accuracy: 0.9507\n",
      "Epoch #6000 LR: 0.0735 Loss: 0.2555 accuracy: 0.9583 Test loss: 0.3467 Test accuracy: 0.9548\n",
      "Epoch #8000 LR: 0.0663 Loss: 0.2482 accuracy: 0.9593 Test loss: 0.3408 Test accuracy: 0.9567\n",
      "Epoch #10000 LR: 0.0599 Loss: 0.2426 accuracy: 0.9596 Test loss: 0.3378 Test accuracy: 0.9574\n",
      "Epoch #12000 LR: 0.0540 Loss: 0.2386 accuracy: 0.9593 Test loss: 0.3368 Test accuracy: 0.9575\n",
      "Epoch #14000 LR: 0.0488 Loss: 0.2339 accuracy: 0.9592 Test loss: 0.3373 Test accuracy: 0.9570\n",
      "Train/Evaluate on fold 2\n",
      "Epoch #2000 LR: 0.0903 Loss: 0.2793 accuracy: 0.9612 Test loss: 0.3743 Test accuracy: 0.9512\n",
      "Epoch #4000 LR: 0.0815 Loss: 0.2647 accuracy: 0.9611 Test loss: 0.3559 Test accuracy: 0.9569\n",
      "Epoch #6000 LR: 0.0735 Loss: 0.2546 accuracy: 0.9611 Test loss: 0.3468 Test accuracy: 0.9573\n",
      "Epoch #8000 LR: 0.0663 Loss: 0.2481 accuracy: 0.9601 Test loss: 0.3434 Test accuracy: 0.9574\n",
      "Epoch #10000 LR: 0.0599 Loss: 0.2417 accuracy: 0.9614 Test loss: 0.3421 Test accuracy: 0.9577\n",
      "Epoch #12000 LR: 0.0540 Loss: 0.2366 accuracy: 0.9608 Test loss: 0.3421 Test accuracy: 0.9584\n",
      "Epoch #14000 LR: 0.0488 Loss: 0.2320 accuracy: 0.9611 Test loss: 0.3427 Test accuracy: 0.9585\n",
      "Train/Evaluate on fold 3\n",
      "Epoch #2000 LR: 0.0903 Loss: 0.2812 accuracy: 0.9586 Test loss: 0.3722 Test accuracy: 0.9462\n",
      "Epoch #4000 LR: 0.0815 Loss: 0.2674 accuracy: 0.9592 Test loss: 0.3539 Test accuracy: 0.9525\n",
      "Epoch #6000 LR: 0.0735 Loss: 0.2564 accuracy: 0.9595 Test loss: 0.3427 Test accuracy: 0.9553\n",
      "Epoch #8000 LR: 0.0663 Loss: 0.2506 accuracy: 0.9603 Test loss: 0.3366 Test accuracy: 0.9571\n",
      "Epoch #10000 LR: 0.0599 Loss: 0.2448 accuracy: 0.9609 Test loss: 0.3331 Test accuracy: 0.9578\n",
      "Epoch #12000 LR: 0.0540 Loss: 0.2397 accuracy: 0.9611 Test loss: 0.3311 Test accuracy: 0.9581\n",
      "Epoch #14000 LR: 0.0488 Loss: 0.2359 accuracy: 0.9608 Test loss: 0.3304 Test accuracy: 0.9582\n",
      "Train/Evaluate on fold 4\n",
      "Epoch #2000 LR: 0.0903 Loss: 0.2769 accuracy: 0.9596 Test loss: 0.3859 Test accuracy: 0.9487\n",
      "Epoch #4000 LR: 0.0815 Loss: 0.2617 accuracy: 0.9601 Test loss: 0.3716 Test accuracy: 0.9522\n",
      "Epoch #6000 LR: 0.0735 Loss: 0.2522 accuracy: 0.9592 Test loss: 0.3636 Test accuracy: 0.9533\n",
      "Epoch #8000 LR: 0.0663 Loss: 0.2459 accuracy: 0.9593 Test loss: 0.3590 Test accuracy: 0.9545\n",
      "Epoch #10000 LR: 0.0599 Loss: 0.2406 accuracy: 0.9591 Test loss: 0.3564 Test accuracy: 0.9552\n",
      "Epoch #12000 LR: 0.0540 Loss: 0.2346 accuracy: 0.9599 Test loss: 0.3551 Test accuracy: 0.9557\n",
      "Epoch #14000 LR: 0.0488 Loss: 0.2311 accuracy: 0.9603 Test loss: 0.3544 Test accuracy: 0.9557\n",
      "Train/Evaluate on fold 5\n",
      "Epoch #2000 LR: 0.0903 Loss: 0.2804 accuracy: 0.9584 Test loss: 0.3880 Test accuracy: 0.9445\n",
      "Epoch #4000 LR: 0.0815 Loss: 0.2660 accuracy: 0.9612 Test loss: 0.3714 Test accuracy: 0.9518\n",
      "Epoch #6000 LR: 0.0735 Loss: 0.2547 accuracy: 0.9606 Test loss: 0.3589 Test accuracy: 0.9554\n",
      "Epoch #8000 LR: 0.0663 Loss: 0.2485 accuracy: 0.9608 Test loss: 0.3535 Test accuracy: 0.9564\n",
      "Epoch #10000 LR: 0.0599 Loss: 0.2415 accuracy: 0.9603 Test loss: 0.3513 Test accuracy: 0.9563\n",
      "Epoch #12000 LR: 0.0540 Loss: 0.2350 accuracy: 0.9605 Test loss: 0.3507 Test accuracy: 0.9560\n",
      "Epoch #14000 LR: 0.0488 Loss: 0.2319 accuracy: 0.9598 Test loss: 0.3510 Test accuracy: 0.9566\n"
     ]
    }
   ],
   "source": [
    "# Training with missing labels with 40%\n",
    "NUM_EPOCHS = 15000\n",
    "\n",
    "fold = 0\n",
    "folds_metrics = []\n",
    "for train_index, test_index in splitter.split(features, labels):\n",
    "    fold += 1\n",
    "    print(\"Train/Evaluate on fold %d\" % fold)\n",
    "    train_features, test_features = features[train_index], features[test_index]\n",
    "    train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "    labels_with_missing_positives, indices_to_hide = hide_labels(train_labels,0.4)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            epoch_loss, epoch_accuracy,epoch_output, _ = sess.run([loss, accuracy,output, train_step],feed_dict={x_input: \n",
    "                                                                                             train_features,y: labels_with_missing_positives,\n",
    "                                                                                             current_keep_prob: 0.3})\n",
    "            if (epoch+1)% 2000 == 0:\n",
    "                val_losses, val_accuracies, val_output,current_learning_rate = sess.run([loss, accuracy,output,learning_rate],feed_dict={\n",
    "                                                                                              x_input: test_features,\n",
    "                                                                                              y:test_labels,\n",
    "                                                                                              current_keep_prob: 1.0})\n",
    "                print(\"Epoch #{}\".format(epoch+1),  \"LR: {:.4f}\".format(current_learning_rate),\n",
    "                      \"Loss: {:.4f}\".format(epoch_loss), \n",
    "                      \"accuracy: {:.4f}\".format(epoch_accuracy), \n",
    "                      \"Test loss: {:.4f}\".format(val_losses), \n",
    "                      \"Test accuracy: {:.4f}\".format(val_accuracies))\n",
    "    np.savetxt(os.path.join(model_output_dir,'[missing]groundtruth_' + str(fold) + '.out'), test_labels, delimiter=',')\n",
    "    np.savetxt(os.path.join(model_output_dir, '[missing]output' + str(fold) + '.out'), val_output, delimiter=',')\n",
    "    folds_metrics.append(evaluation_metrics_per_fold(test_labels,val_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40% missing labels, no weighted loss\n",
      "===================\n",
      "Test set evaluation\n",
      "Hamming loss is:0.043 (+/-0.001)\n",
      "Zero-one loss is:0.744 (+/-0.009)\n",
      "Coverage error is:4.609 (+/-0.115)\n",
      "F1 is:0.437 (+/-0.011)\n",
      "Recall is:0.376 (+/-0.014)\n",
      "Precision is:0.523 (+/-0.016)\n",
      "Average Precision is:0.451 (+/-0.020)\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "evaluation_report(folds_metrics,os.path.join(exp_dir,\"evaluation_report.txt\"),\n",
    "                  \"40% missing labels, no weighted loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With fixed negative weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1119 16:33:43.104295 140619841525504 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Adding the weighted loss to the model\n",
    "positive_weights = tf.placeholder(tf.float32, [None,output_shape], name = \"Positive_weights\")\n",
    "negative_weights = tf.placeholder(tf.float32, [None, output_shape], name=\"negative_weights\")\n",
    "my_weights_loss = weighted_loss(y_true= y, y_pred= output,\n",
    "                              positive_weights= positive_weights, negative_weights= negative_weights)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(my_weights_loss,global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Evaluate on fold 1\n",
      "Epoch #5000 LR: 0.0774 Loss: 0.2586 Weighted Loss: 0.2471 accuracy: 0.9558 Test loss: 0.3414 Test accuracy: 0.9534\n",
      "Epoch #10000 LR: 0.0599 Loss: 0.2433 Weighted Loss: 0.2280 accuracy: 0.9550 Test loss: 0.3285 Test accuracy: 0.9537\n",
      "Epoch #15000 LR: 0.0463 Loss: 0.2333 Weighted Loss: 0.2167 accuracy: 0.9549 Test loss: 0.3293 Test accuracy: 0.9535\n",
      "Train/Evaluate on fold 2\n",
      "Epoch #5000 LR: 0.0774 Loss: 0.2571 Weighted Loss: 0.2455 accuracy: 0.9543 Test loss: 0.3410 Test accuracy: 0.9536\n",
      "Epoch #10000 LR: 0.0599 Loss: 0.2402 Weighted Loss: 0.2250 accuracy: 0.9554 Test loss: 0.3302 Test accuracy: 0.9550\n",
      "Epoch #15000 LR: 0.0463 Loss: 0.2304 Weighted Loss: 0.2140 accuracy: 0.9557 Test loss: 0.3304 Test accuracy: 0.9543\n",
      "Train/Evaluate on fold 3\n",
      "Epoch #5000 LR: 0.0774 Loss: 0.2640 Weighted Loss: 0.2528 accuracy: 0.9530 Test loss: 0.3382 Test accuracy: 0.9503\n",
      "Epoch #10000 LR: 0.0599 Loss: 0.2457 Weighted Loss: 0.2308 accuracy: 0.9548 Test loss: 0.3204 Test accuracy: 0.9554\n",
      "Epoch #15000 LR: 0.0463 Loss: 0.2341 Weighted Loss: 0.2178 accuracy: 0.9553 Test loss: 0.3181 Test accuracy: 0.9549\n",
      "Train/Evaluate on fold 4\n",
      "Epoch #5000 LR: 0.0774 Loss: 0.2549 Weighted Loss: 0.2431 accuracy: 0.9541 Test loss: 0.3561 Test accuracy: 0.9505\n",
      "Epoch #10000 LR: 0.0599 Loss: 0.2370 Weighted Loss: 0.2217 accuracy: 0.9553 Test loss: 0.3444 Test accuracy: 0.9538\n",
      "Epoch #15000 LR: 0.0463 Loss: 0.2286 Weighted Loss: 0.2118 accuracy: 0.9551 Test loss: 0.3445 Test accuracy: 0.9530\n",
      "Train/Evaluate on fold 5\n",
      "Epoch #5000 LR: 0.0774 Loss: 0.2574 Weighted Loss: 0.2457 accuracy: 0.9545 Test loss: 0.3556 Test accuracy: 0.9502\n",
      "Epoch #10000 LR: 0.0599 Loss: 0.2387 Weighted Loss: 0.2233 accuracy: 0.9552 Test loss: 0.3456 Test accuracy: 0.9531\n",
      "Epoch #15000 LR: 0.0463 Loss: 0.2301 Weighted Loss: 0.2130 accuracy: 0.9552 Test loss: 0.3461 Test accuracy: 0.9525\n"
     ]
    }
   ],
   "source": [
    "# Training with negative weights!\n",
    "NUM_EPOCHS = 15000\n",
    "\n",
    "fold = 0\n",
    "folds_metrics = []\n",
    "for train_index, test_index in splitter.split(features, labels):\n",
    "    fold += 1\n",
    "    print(\"Train/Evaluate on fold %d\" % fold)\n",
    "    train_features, test_features = features[train_index], features[test_index]\n",
    "    train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "    labels_with_missing_positives, indices_to_hide = hide_labels(train_labels,0.4)\n",
    "    train_negative_weights, train_positive_weights = get_weights_for_hidden_labels(train_labels,indices_to_hide)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            epoch_my_weights_loss, epoch_loss, epoch_accuracy,epoch_output, _ = sess.run([my_weights_loss, loss, accuracy,output, train_step],feed_dict={x_input: \n",
    "                                                                                             train_features,y: labels_with_missing_positives,positive_weights: train_positive_weights,\n",
    "                                                                                                      negative_weights: train_negative_weights,\n",
    "                                                                                                      current_keep_prob: 0.3})\n",
    "            if (epoch+1)% 5000 == 0:\n",
    "                val_losses, val_accuracies, val_output,current_learning_rate = sess.run([loss, accuracy,output,learning_rate],feed_dict={\n",
    "                                                                                              x_input: test_features,\n",
    "                                                                                              y:test_labels,\n",
    "                                                                                              current_keep_prob: 1.0})\n",
    "                print(\"Epoch #{}\".format(epoch+1),  \"LR: {:.4f}\".format(current_learning_rate),\n",
    "                      \"Loss: {:.4f}\".format(epoch_loss), \n",
    "                      \"Weighted Loss: {:.4f}\".format(epoch_my_weights_loss),\"accuracy: {:.4f}\".format(epoch_accuracy), \n",
    "                      \"Test loss: {:.4f}\".format(val_losses), \"Test accuracy: {:.4f}\".format(val_accuracies))\n",
    "    np.savetxt(os.path.join(model_output_dir,'[weighted]groundtruth_' + str(fold) + '.out'), test_labels, delimiter=',')\n",
    "    np.savetxt(os.path.join(model_output_dir, '[weighted]output' + str(fold) + '.out'), val_output, delimiter=',')\n",
    "    folds_metrics.append(evaluation_metrics_per_fold(test_labels,val_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted loss with 40% missing\n",
      "===================\n",
      "Test set evaluation\n",
      "Hamming loss is:0.046 (+/-0.002)\n",
      "Zero-one loss is:0.768 (+/-0.016)\n",
      "Coverage error is:4.587 (+/-0.291)\n",
      "F1 is:0.454 (+/-0.023)\n",
      "Recall is:0.436 (+/-0.030)\n",
      "Precision is:0.474 (+/-0.021)\n",
      "Average Precision is:0.469 (+/-0.035)\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "evaluation_report(folds_metrics,os.path.join(exp_dir,\"evaluation_report.txt\"),\n",
    "                  \"Weighted loss with 40% missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
