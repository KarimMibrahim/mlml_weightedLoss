{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np \n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, cohen_kappa_score, hamming_loss, zero_one_loss, coverage_error, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "random.seed = 0\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples is: 5000\n",
      "#features is: 550, #labels is: 33\n",
      "Ratio of positive labels is: 4.43%\n"
     ]
    }
   ],
   "source": [
    "# Reading dataset\n",
    "educationDataset = loadmat(\"/home/karim/Documents/research/sourceCode/MLML/mlml_weightedLoss/Datasets/MulanDatasets/education.mat\")\n",
    "\n",
    "features = educationDataset['train_data']\n",
    "test_features = educationDataset['test_data']\n",
    "\n",
    "labels = educationDataset['train_target'].T\n",
    "test_labels = educationDataset['test_target'].T\n",
    "\n",
    "\"\"\"\n",
    "Split ratio is strange: 40% training and 60% testing, we merge and data and resplit with 70/30 split\n",
    "\"\"\"\n",
    "features = np.append(features,test_features,axis = 0)\n",
    "labels = np.append(labels,test_labels,axis = 0)\n",
    "\n",
    "print(\"Number of samples is: {}\".format(len(features)))\n",
    "print(\"#features is: {}, #labels is: {}\".format(features.shape[1],labels.shape[1]))\n",
    "print(\"Ratio of positive labels is: {:.2f}%\".format(100 * labels.sum()/(labels.shape[0]*labels.shape[1])))\n",
    "\n",
    "features, test_features, labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0,
     1,
     6,
     12,
     18
    ]
   },
   "outputs": [],
   "source": [
    "# define network helper functions\n",
    "def get_weights(shape):\n",
    "    w = tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "    #variable_summaries(w)\n",
    "    return w\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    b = tf.Variable(initial)\n",
    "    #variable_summaries(b)\n",
    "    return b\n",
    "\n",
    "def full_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = get_weights([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input, W) + b\n",
    "\n",
    "def weighted_loss(y_true, y_pred, positive_weights, negative_weights):\n",
    "    # clip to prevent NaN's and Inf's\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1-1e-7, name=None)\n",
    "    #y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # calc\n",
    "    loss = (-y_true * tf.log(y_pred) * positive_weights) - ((1.0 - y_true) * tf.log(1.0 - y_pred) * negative_weights)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define a 2 layers network to train \n",
    "input_shape = features.shape[1]\n",
    "output_shape = labels.shape[1]\n",
    "hidden_layer_1_shape = 240\n",
    "hidden_layer_2_shape = 120\n",
    "\n",
    "y = tf.placeholder(tf.float32, [None, output_shape], name=\"true_labels\")\n",
    "x_input = tf.placeholder(tf.float32, [None,input_shape],name=\"input_layer\")\n",
    "current_keep_prob = tf.placeholder(tf.float32, name=\"dropout_rate\")\n",
    "h1 = tf.nn.tanh(full_layer(x_input, hidden_layer_1_shape))\n",
    "h2 = tf.nn.tanh(full_layer(h1, hidden_layer_2_shape))\n",
    "dropped = tf.nn.dropout(h2, keep_prob=current_keep_prob)\n",
    "logits = full_layer(dropped,output_shape)\n",
    "output = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "# using weighted cross entropy because dataset is sparse and we need to weight positives more\n",
    "#loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "POSITIVE_WEIGHT = 5\n",
    "loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=logits, labels=y,pos_weight = POSITIVE_WEIGHT))\n",
    "\n",
    "\n",
    "# Learning rate decay\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(learning_rate=0.1, global_step=global_step, decay_steps=1000,\n",
    "                                          decay_rate=0.95,staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "correct_prediction = tf.equal(tf.round(output), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #500 LR: 0.1000 Loss: 0.3963 accuracy: 0.8999 Test loss: 0.3646 Test accuracy: 0.9224\n",
      "Epoch #1000 LR: 0.0950 Loss: 0.3823 accuracy: 0.9079 Test loss: 0.3589 Test accuracy: 0.9259\n",
      "Epoch #1500 LR: 0.0950 Loss: 0.3733 accuracy: 0.9164 Test loss: 0.3554 Test accuracy: 0.9282\n",
      "Epoch #2000 LR: 0.0903 Loss: 0.3679 accuracy: 0.9216 Test loss: 0.3515 Test accuracy: 0.9319\n",
      "Epoch #2500 LR: 0.0903 Loss: 0.3625 accuracy: 0.9252 Test loss: 0.3474 Test accuracy: 0.9345\n",
      "Epoch #3000 LR: 0.0857 Loss: 0.3583 accuracy: 0.9275 Test loss: 0.3429 Test accuracy: 0.9364\n",
      "Epoch #3500 LR: 0.0857 Loss: 0.3524 accuracy: 0.9283 Test loss: 0.3383 Test accuracy: 0.9370\n",
      "Epoch #4000 LR: 0.0815 Loss: 0.3492 accuracy: 0.9301 Test loss: 0.3339 Test accuracy: 0.9374\n",
      "Epoch #4500 LR: 0.0815 Loss: 0.3448 accuracy: 0.9297 Test loss: 0.3300 Test accuracy: 0.9369\n",
      "Epoch #5000 LR: 0.0774 Loss: 0.3406 accuracy: 0.9308 Test loss: 0.3264 Test accuracy: 0.9366\n",
      "Epoch #5500 LR: 0.0774 Loss: 0.3373 accuracy: 0.9307 Test loss: 0.3234 Test accuracy: 0.9361\n",
      "Epoch #6000 LR: 0.0735 Loss: 0.3349 accuracy: 0.9309 Test loss: 0.3207 Test accuracy: 0.9359\n",
      "Epoch #6500 LR: 0.0735 Loss: 0.3317 accuracy: 0.9313 Test loss: 0.3185 Test accuracy: 0.9359\n",
      "Epoch #7000 LR: 0.0698 Loss: 0.3282 accuracy: 0.9321 Test loss: 0.3165 Test accuracy: 0.9363\n",
      "Epoch #7500 LR: 0.0698 Loss: 0.3249 accuracy: 0.9330 Test loss: 0.3148 Test accuracy: 0.9365\n",
      "Epoch #8000 LR: 0.0663 Loss: 0.3229 accuracy: 0.9335 Test loss: 0.3133 Test accuracy: 0.9368\n",
      "Epoch #8500 LR: 0.0663 Loss: 0.3228 accuracy: 0.9332 Test loss: 0.3120 Test accuracy: 0.9372\n",
      "Epoch #9000 LR: 0.0630 Loss: 0.3176 accuracy: 0.9341 Test loss: 0.3108 Test accuracy: 0.9372\n",
      "Epoch #9500 LR: 0.0630 Loss: 0.3185 accuracy: 0.9343 Test loss: 0.3098 Test accuracy: 0.9372\n",
      "Epoch #10000 LR: 0.0599 Loss: 0.3151 accuracy: 0.9352 Test loss: 0.3089 Test accuracy: 0.9374\n",
      "Epoch #10500 LR: 0.0599 Loss: 0.3131 accuracy: 0.9355 Test loss: 0.3081 Test accuracy: 0.9376\n",
      "Epoch #11000 LR: 0.0569 Loss: 0.3140 accuracy: 0.9351 Test loss: 0.3074 Test accuracy: 0.9378\n",
      "Epoch #11500 LR: 0.0569 Loss: 0.3111 accuracy: 0.9348 Test loss: 0.3067 Test accuracy: 0.9379\n",
      "Epoch #12000 LR: 0.0540 Loss: 0.3076 accuracy: 0.9360 Test loss: 0.3061 Test accuracy: 0.9379\n",
      "Epoch #12500 LR: 0.0540 Loss: 0.3081 accuracy: 0.9358 Test loss: 0.3056 Test accuracy: 0.9380\n",
      "Epoch #13000 LR: 0.0513 Loss: 0.3072 accuracy: 0.9360 Test loss: 0.3051 Test accuracy: 0.9381\n",
      "Epoch #13500 LR: 0.0513 Loss: 0.3047 accuracy: 0.9358 Test loss: 0.3047 Test accuracy: 0.9382\n",
      "Epoch #14000 LR: 0.0488 Loss: 0.3034 accuracy: 0.9356 Test loss: 0.3042 Test accuracy: 0.9383\n",
      "Epoch #14500 LR: 0.0488 Loss: 0.3010 accuracy: 0.9369 Test loss: 0.3039 Test accuracy: 0.9383\n",
      "Epoch #15000 LR: 0.0463 Loss: 0.3016 accuracy: 0.9369 Test loss: 0.3036 Test accuracy: 0.9386\n",
      "Epoch #15500 LR: 0.0463 Loss: 0.3004 accuracy: 0.9367 Test loss: 0.3033 Test accuracy: 0.9387\n",
      "Epoch #16000 LR: 0.0440 Loss: 0.2998 accuracy: 0.9376 Test loss: 0.3030 Test accuracy: 0.9391\n",
      "Epoch #16500 LR: 0.0440 Loss: 0.2974 accuracy: 0.9372 Test loss: 0.3028 Test accuracy: 0.9389\n",
      "Epoch #17000 LR: 0.0418 Loss: 0.2972 accuracy: 0.9377 Test loss: 0.3026 Test accuracy: 0.9389\n",
      "Epoch #17500 LR: 0.0418 Loss: 0.2977 accuracy: 0.9380 Test loss: 0.3024 Test accuracy: 0.9389\n",
      "Epoch #18000 LR: 0.0397 Loss: 0.2945 accuracy: 0.9370 Test loss: 0.3023 Test accuracy: 0.9390\n",
      "Epoch #18500 LR: 0.0397 Loss: 0.2946 accuracy: 0.9382 Test loss: 0.3021 Test accuracy: 0.9391\n",
      "Epoch #19000 LR: 0.0377 Loss: 0.2945 accuracy: 0.9375 Test loss: 0.3020 Test accuracy: 0.9391\n",
      "Epoch #19500 LR: 0.0377 Loss: 0.2933 accuracy: 0.9383 Test loss: 0.3019 Test accuracy: 0.9392\n",
      "Epoch #20000 LR: 0.0358 Loss: 0.2917 accuracy: 0.9381 Test loss: 0.3018 Test accuracy: 0.9392\n",
      "Epoch #20500 LR: 0.0358 Loss: 0.2928 accuracy: 0.9377 Test loss: 0.3017 Test accuracy: 0.9393\n",
      "Epoch #21000 LR: 0.0341 Loss: 0.2896 accuracy: 0.9386 Test loss: 0.3017 Test accuracy: 0.9393\n",
      "Epoch #21500 LR: 0.0341 Loss: 0.2890 accuracy: 0.9392 Test loss: 0.3016 Test accuracy: 0.9392\n",
      "Epoch #22000 LR: 0.0324 Loss: 0.2890 accuracy: 0.9389 Test loss: 0.3015 Test accuracy: 0.9390\n",
      "Epoch #22500 LR: 0.0324 Loss: 0.2888 accuracy: 0.9383 Test loss: 0.3015 Test accuracy: 0.9389\n",
      "Epoch #23000 LR: 0.0307 Loss: 0.2888 accuracy: 0.9391 Test loss: 0.3015 Test accuracy: 0.9390\n",
      "Epoch #23500 LR: 0.0307 Loss: 0.2876 accuracy: 0.9382 Test loss: 0.3015 Test accuracy: 0.9390\n",
      "Epoch #24000 LR: 0.0292 Loss: 0.2867 accuracy: 0.9388 Test loss: 0.3015 Test accuracy: 0.9392\n",
      "Epoch #24500 LR: 0.0292 Loss: 0.2866 accuracy: 0.9394 Test loss: 0.3014 Test accuracy: 0.9393\n",
      "Epoch #25000 LR: 0.0277 Loss: 0.2866 accuracy: 0.9386 Test loss: 0.3015 Test accuracy: 0.9394\n",
      "Epoch #25500 LR: 0.0277 Loss: 0.2853 accuracy: 0.9401 Test loss: 0.3015 Test accuracy: 0.9394\n",
      "Epoch #26000 LR: 0.0264 Loss: 0.2828 accuracy: 0.9392 Test loss: 0.3015 Test accuracy: 0.9393\n",
      "Epoch #26500 LR: 0.0264 Loss: 0.2836 accuracy: 0.9395 Test loss: 0.3015 Test accuracy: 0.9393\n",
      "Epoch #27000 LR: 0.0250 Loss: 0.2840 accuracy: 0.9399 Test loss: 0.3015 Test accuracy: 0.9394\n",
      "Epoch #27500 LR: 0.0250 Loss: 0.2846 accuracy: 0.9392 Test loss: 0.3015 Test accuracy: 0.9394\n",
      "Epoch #28000 LR: 0.0238 Loss: 0.2844 accuracy: 0.9395 Test loss: 0.3016 Test accuracy: 0.9394\n",
      "Epoch #28500 LR: 0.0238 Loss: 0.2825 accuracy: 0.9391 Test loss: 0.3016 Test accuracy: 0.9393\n",
      "Epoch #29000 LR: 0.0226 Loss: 0.2820 accuracy: 0.9392 Test loss: 0.3016 Test accuracy: 0.9393\n",
      "Epoch #29500 LR: 0.0226 Loss: 0.2796 accuracy: 0.9397 Test loss: 0.3016 Test accuracy: 0.9393\n",
      "Epoch #30000 LR: 0.0215 Loss: 0.2783 accuracy: 0.9398 Test loss: 0.3017 Test accuracy: 0.9394\n",
      "Epoch #30500 LR: 0.0215 Loss: 0.2801 accuracy: 0.9393 Test loss: 0.3017 Test accuracy: 0.9393\n",
      "Epoch #31000 LR: 0.0204 Loss: 0.2787 accuracy: 0.9395 Test loss: 0.3018 Test accuracy: 0.9392\n",
      "Epoch #31500 LR: 0.0204 Loss: 0.2800 accuracy: 0.9409 Test loss: 0.3018 Test accuracy: 0.9393\n",
      "Epoch #32000 LR: 0.0194 Loss: 0.2795 accuracy: 0.9390 Test loss: 0.3018 Test accuracy: 0.9391\n",
      "Epoch #32500 LR: 0.0194 Loss: 0.2801 accuracy: 0.9403 Test loss: 0.3019 Test accuracy: 0.9391\n",
      "Epoch #33000 LR: 0.0184 Loss: 0.2800 accuracy: 0.9400 Test loss: 0.3019 Test accuracy: 0.9392\n",
      "Epoch #33500 LR: 0.0184 Loss: 0.2790 accuracy: 0.9398 Test loss: 0.3020 Test accuracy: 0.9392\n",
      "Epoch #34000 LR: 0.0175 Loss: 0.2793 accuracy: 0.9393 Test loss: 0.3020 Test accuracy: 0.9392\n",
      "Epoch #34500 LR: 0.0175 Loss: 0.2794 accuracy: 0.9400 Test loss: 0.3021 Test accuracy: 0.9392\n",
      "Epoch #35000 LR: 0.0166 Loss: 0.2799 accuracy: 0.9400 Test loss: 0.3021 Test accuracy: 0.9392\n",
      "Epoch #35500 LR: 0.0166 Loss: 0.2788 accuracy: 0.9394 Test loss: 0.3022 Test accuracy: 0.9391\n",
      "Epoch #36000 LR: 0.0158 Loss: 0.2741 accuracy: 0.9403 Test loss: 0.3022 Test accuracy: 0.9390\n",
      "Epoch #36500 LR: 0.0158 Loss: 0.2769 accuracy: 0.9397 Test loss: 0.3023 Test accuracy: 0.9390\n",
      "Epoch #37000 LR: 0.0150 Loss: 0.2742 accuracy: 0.9403 Test loss: 0.3023 Test accuracy: 0.9389\n",
      "Epoch #37500 LR: 0.0150 Loss: 0.2758 accuracy: 0.9405 Test loss: 0.3023 Test accuracy: 0.9388\n",
      "Epoch #38000 LR: 0.0142 Loss: 0.2766 accuracy: 0.9401 Test loss: 0.3024 Test accuracy: 0.9390\n",
      "Epoch #38500 LR: 0.0142 Loss: 0.2762 accuracy: 0.9405 Test loss: 0.3024 Test accuracy: 0.9390\n",
      "Epoch #39000 LR: 0.0135 Loss: 0.2760 accuracy: 0.9398 Test loss: 0.3025 Test accuracy: 0.9391\n",
      "Epoch #39500 LR: 0.0135 Loss: 0.2752 accuracy: 0.9409 Test loss: 0.3025 Test accuracy: 0.9390\n",
      "Epoch #40000 LR: 0.0129 Loss: 0.2749 accuracy: 0.9410 Test loss: 0.3026 Test accuracy: 0.9390\n",
      "Epoch #40500 LR: 0.0129 Loss: 0.2745 accuracy: 0.9408 Test loss: 0.3026 Test accuracy: 0.9389\n",
      "Epoch #41000 LR: 0.0122 Loss: 0.2744 accuracy: 0.9408 Test loss: 0.3026 Test accuracy: 0.9389\n",
      "Epoch #41500 LR: 0.0122 Loss: 0.2723 accuracy: 0.9410 Test loss: 0.3027 Test accuracy: 0.9388\n",
      "Epoch #42000 LR: 0.0116 Loss: 0.2741 accuracy: 0.9413 Test loss: 0.3027 Test accuracy: 0.9389\n",
      "Epoch #42500 LR: 0.0116 Loss: 0.2735 accuracy: 0.9397 Test loss: 0.3028 Test accuracy: 0.9388\n",
      "Epoch #43000 LR: 0.0110 Loss: 0.2754 accuracy: 0.9404 Test loss: 0.3028 Test accuracy: 0.9388\n",
      "Epoch #43500 LR: 0.0110 Loss: 0.2744 accuracy: 0.9408 Test loss: 0.3028 Test accuracy: 0.9388\n",
      "Epoch #44000 LR: 0.0105 Loss: 0.2726 accuracy: 0.9404 Test loss: 0.3029 Test accuracy: 0.9388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #44500 LR: 0.0105 Loss: 0.2729 accuracy: 0.9411 Test loss: 0.3029 Test accuracy: 0.9388\n",
      "Epoch #45000 LR: 0.0099 Loss: 0.2735 accuracy: 0.9403 Test loss: 0.3029 Test accuracy: 0.9388\n",
      "Epoch #45500 LR: 0.0099 Loss: 0.2732 accuracy: 0.9408 Test loss: 0.3030 Test accuracy: 0.9387\n",
      "Epoch #46000 LR: 0.0094 Loss: 0.2716 accuracy: 0.9407 Test loss: 0.3030 Test accuracy: 0.9386\n",
      "Epoch #46500 LR: 0.0094 Loss: 0.2732 accuracy: 0.9400 Test loss: 0.3030 Test accuracy: 0.9387\n",
      "Epoch #47000 LR: 0.0090 Loss: 0.2733 accuracy: 0.9407 Test loss: 0.3031 Test accuracy: 0.9386\n",
      "Epoch #47500 LR: 0.0090 Loss: 0.2719 accuracy: 0.9412 Test loss: 0.3031 Test accuracy: 0.9386\n",
      "Epoch #48000 LR: 0.0085 Loss: 0.2716 accuracy: 0.9405 Test loss: 0.3031 Test accuracy: 0.9386\n",
      "Epoch #48500 LR: 0.0085 Loss: 0.2716 accuracy: 0.9406 Test loss: 0.3032 Test accuracy: 0.9386\n",
      "Epoch #49000 LR: 0.0081 Loss: 0.2733 accuracy: 0.9402 Test loss: 0.3032 Test accuracy: 0.9386\n",
      "Epoch #49500 LR: 0.0081 Loss: 0.2710 accuracy: 0.9401 Test loss: 0.3032 Test accuracy: 0.9386\n",
      "Epoch #50000 LR: 0.0077 Loss: 0.2716 accuracy: 0.9403 Test loss: 0.3033 Test accuracy: 0.9385\n",
      "======================================                       \n",
      "\n",
      "Final test accuracy: 0.9385\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "NUM_EPOCHS = 50000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_loss, epoch_accuracy,epoch_output, _ = sess.run([loss, accuracy,output, train_step],feed_dict={x_input: \n",
    "                                                                                         features,y: labels,\n",
    "                                                                                         current_keep_prob: 0.3,})\n",
    "        if (epoch+1)% 500 == 0:\n",
    "            val_losses, val_accuracies, val_output,current_learning_rate = sess.run([loss, accuracy,output,learning_rate],feed_dict={\n",
    "                                                                                          x_input: test_features,\n",
    "                                                                                          y:test_labels,\n",
    "                                                                                          current_keep_prob: 1.0})\n",
    "            print(\"Epoch #{}\".format(epoch+1), \"LR: {:.4f}\".format(current_learning_rate),\n",
    "                  \"Loss: {:.4f}\".format(epoch_loss), \n",
    "                  \"accuracy: {:.4f}\".format(epoch_accuracy),\n",
    "                  \"Test loss: {:.4f}\".format(val_losses), \n",
    "                  \"Test accuracy: {:.4f}\".format(val_accuracies))\n",
    "            if (epoch+1) == NUM_EPOCHS:\n",
    "                print(\"====================================== \\\n",
    "                      \\n\\nFinal test accuracy: {:.4f}\".format(val_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Adjusting threshold \n",
    "\"\"\"\n",
    "thresholds = np.arange(0, 1, 0.01)\n",
    "f1_array = np.zeros((output_shape, len(thresholds)))\n",
    "for idx in range(output_shape):\n",
    "    f1_array[idx, :] = [\n",
    "        f1_score(labels[:, idx], np.clip(np.round(epoch_output[:, idx] - threshold + 0.5), 0, 1))\n",
    "        for threshold in thresholds]\n",
    "threshold_arg = np.argmax(f1_array, axis=1)\n",
    "threshold_per_class = thresholds[threshold_arg]\n",
    "\n",
    "# Applying thresholds optimized per class\n",
    "model_output_rounded = np.zeros_like(epoch_output)\n",
    "for idx in range(output_shape):\n",
    "    model_output_rounded[:, idx] = np.clip(np.round(epoch_output[:, idx] - threshold_per_class[idx] + 0.5), 0, 1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set evaluation\n",
      "F1 is:0.188\n",
      "Recall is:0.224\n",
      "Precision is:0.274\n",
      "Hamming loss is:0.060\n",
      "Zero-one loss is:0.882\n",
      "Coverage error is:3.669\n",
      "===================\n",
      "Test set evaluation\n",
      "F1 is:0.132\n",
      "Recall is:0.165\n",
      "Precision is:0.125\n",
      "Hamming loss is:0.061\n",
      "Zero-one loss is:0.856\n",
      "Coverage error is:4.230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "# On training \n",
    "print(\"Training set evaluation\")\n",
    "#print(\"AUC is:{:.3f}\".format(roc_auc_score(labels, epoch_output)))\n",
    "print(\"F1 is:{:.3f}\".format(f1_score(labels, np.round(epoch_output),average=\"micro\")))\n",
    "print(\"Recall is:{:.3f}\".format(recall_score(labels, np.round(epoch_output),average=\"micro\")))\n",
    "print(\"Precision is:{:.3f}\".format(precision_score(labels, np.round(epoch_output),average=\"micro\")))\n",
    "print(\"Average Precision is:{:.3f}\".format(average_precision_score(labels, epoch_output,average=\"micro\")))\n",
    "print(\"Hamming loss is:{:.3f}\".format(hamming_loss(labels, np.round(epoch_output))))\n",
    "print(\"Zero-one loss is:{:.3f}\".format(zero_one_loss(labels, np.round(epoch_output))))\n",
    "print(\"Coverage error is:{:.3f}\".format(coverage_error(labels, epoch_output)))\n",
    "\n",
    "\"\"\"\n",
    "print(\"After treshold optimization\")\n",
    "print(\"F1 is:{:.3f}\".format(f1_score(labels, model_output_rounded,average=\"macro\")))\n",
    "print(\"Recall is:{:.3f}\".format(recall_score(labels, model_output_rounded,average=\"macro\")))\n",
    "print(\"Precision is:{:.3f}\".format(precision_score(labels, model_output_rounded,average=\"macro\")))\n",
    "print(\"Hamming loss is:{:.3f}\".format(hamming_loss(labels, model_output_rounded)))\n",
    "print(\"Zero-one loss is:{:.3f}\".format(zero_one_loss(labels, model_output_rounded))) \n",
    "    \n",
    "# Applying thresholds optimized per class for testset\n",
    "test_output_rounded = np.zeros_like(val_output)\n",
    "for idx in range(output_shape):\n",
    "    test_output_rounded[:, idx] = np.clip(np.round(val_output[:, idx] - threshold_per_class[idx] + 0.5), 0, 1)\n",
    "\"\"\"\n",
    "\n",
    "# On test\n",
    "print(\"===================\")\n",
    "print(\"Test set evaluation\")\n",
    "#print(\"AUC is:{:.3f}\".format(roc_auc_score(test_labels, val_output)))\n",
    "print(\"F1 is:{:.3f}\".format(f1_score(test_labels, np.round(val_output),average=\"micro\")))\n",
    "print(\"Recall is:{:.3f}\".format(recall_score(test_labels, np.round(val_output),average=\"micro\")))\n",
    "print(\"Precision is:{:.3f}\".format(precision_score(test_labels, np.round(val_output),average=\"micro\")))\n",
    "print(\"Average Precision is:{:.3f}\".format(average_precision_score(test_labels, val_output,average=\"micro\")))\n",
    "print(\"Hamming loss is:{:.3f}\".format(hamming_loss(test_labels, np.round(val_output))))\n",
    "print(\"Zero-one loss is:{:.3f}\".format(zero_one_loss(test_labels, np.round(val_output))))\n",
    "print(\"Coverage error is:{:.3f}\".format(coverage_error(test_labels, val_output)))\n",
    "\n",
    "\"\"\"\n",
    "print(\"After treshold optimization\")\n",
    "print(\"F1 is:{:.3f}\".format(f1_score(test_labels, test_output_rounded,average=\"macro\")))\n",
    "print(\"Recall is:{:.3f}\".format(recall_score(test_labels,test_output_rounded,average=\"macro\")))\n",
    "print(\"Precision is:{:.3f}\".format(precision_score(test_labels, test_output_rounded,average=\"macro\")))\n",
    "print(\"Hamming loss is:{:.3f}\".format(hamming_loss(test_labels, test_output_rounded)))\n",
    "print(\"Zero-one loss is:{:.3f}\".format(zero_one_loss(test_labels, test_output_rounded)))\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With missing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #500 LR: 0.1000 Loss: 0.3082 accuracy: 0.9480 Test loss: 0.3857 Test accuracy: 0.9458\n",
      "Epoch #1000 LR: 0.0950 Loss: 0.2949 accuracy: 0.9547 Test loss: 0.3808 Test accuracy: 0.9459\n",
      "Epoch #1500 LR: 0.0950 Loss: 0.2896 accuracy: 0.9573 Test loss: 0.3780 Test accuracy: 0.9459\n",
      "Epoch #2000 LR: 0.0903 Loss: 0.2855 accuracy: 0.9577 Test loss: 0.3757 Test accuracy: 0.9461\n",
      "Epoch #2500 LR: 0.0903 Loss: 0.2826 accuracy: 0.9578 Test loss: 0.3732 Test accuracy: 0.9464\n",
      "Epoch #3000 LR: 0.0857 Loss: 0.2805 accuracy: 0.9583 Test loss: 0.3706 Test accuracy: 0.9468\n",
      "Epoch #3500 LR: 0.0857 Loss: 0.2781 accuracy: 0.9588 Test loss: 0.3679 Test accuracy: 0.9478\n",
      "Epoch #4000 LR: 0.0815 Loss: 0.2760 accuracy: 0.9590 Test loss: 0.3648 Test accuracy: 0.9489\n",
      "Epoch #4500 LR: 0.0815 Loss: 0.2733 accuracy: 0.9592 Test loss: 0.3618 Test accuracy: 0.9504\n",
      "Epoch #5000 LR: 0.0774 Loss: 0.2722 accuracy: 0.9590 Test loss: 0.3587 Test accuracy: 0.9517\n"
     ]
    }
   ],
   "source": [
    "ones_indices = np.nonzero(labels)\n",
    "ratio_of_hidden_samples = 0.4\n",
    "number_of_hidden_samples = int(len(ones_indices[0]) * ratio_of_hidden_samples)\n",
    "random_indices = random.sample(list(np.arange(len(ones_indices[0]))),number_of_hidden_samples)\n",
    "indices_to_hide = (ones_indices[0][random_indices] , ones_indices[1][random_indices])\n",
    "labels_with_missing_positives = np.copy(labels)\n",
    "for counter in range (number_of_hidden_samples):\n",
    "    labels_with_missing_positives[indices_to_hide[0][counter]][indices_to_hide[1][counter]] = 0\n",
    "    \n",
    "    \n",
    "# Training with missing labels with 40%\n",
    "NUM_EPOCHS = 5000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_loss, epoch_accuracy,epoch_output, _ = sess.run([loss, accuracy,output, train_step],feed_dict={x_input: \n",
    "                                                                                         features,y: labels_with_missing_positives,\n",
    "                                                                                         current_keep_prob: 0.3})\n",
    "        if (epoch+1)% 500 == 0:\n",
    "            val_losses, val_accuracies, val_output,current_learning_rate = sess.run([loss, accuracy,output,learning_rate],feed_dict={\n",
    "                                                                                          x_input: test_features,\n",
    "                                                                                          y:test_labels,\n",
    "                                                                                          current_keep_prob: 1.0})\n",
    "            print(\"Epoch #{}\".format(epoch+1),  \"LR: {:.4f}\".format(current_learning_rate),\n",
    "                  \"Loss: {:.4f}\".format(epoch_loss), \n",
    "                  \"accuracy: {:.4f}\".format(epoch_accuracy), \n",
    "                  \"Test loss: {:.4f}\".format(val_losses), \n",
    "                  \"Test accuracy: {:.4f}\".format(val_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set evaluation\n",
      "F1 is:0.051\n",
      "Recall is:0.048\n",
      "Precision is:0.136\n",
      "Hamming loss is:0.050\n",
      "Zero-one loss is:0.806\n",
      "Coverage error is:5.144\n",
      "===================\n",
      "Test set evaluation\n",
      "F1 is:0.050\n",
      "Recall is:0.049\n",
      "Precision is:0.108\n",
      "Hamming loss is:0.048\n",
      "Zero-one loss is:0.767\n",
      "Coverage error is:4.796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "# On training \n",
    "print(\"Training set evaluation\")\n",
    "#print(\"AUC is:{:.3f}\".format(roc_auc_score(labels, epoch_output)))\n",
    "print(\"F1 is:{:.3f}\".format(f1_score(labels, np.round(epoch_output),average=\"micro\")))\n",
    "print(\"Recall is:{:.3f}\".format(recall_score(labels, np.round(epoch_output),average=\"micro\")))\n",
    "print(\"Precision is:{:.3f}\".format(precision_score(labels, np.round(epoch_output),average=\"micro\")))\n",
    "print(\"Average Precision is:{:.3f}\".format(average_precision_score(labels, epoch_output,average=\"micro\")))\n",
    "print(\"Hamming loss is:{:.3f}\".format(hamming_loss(labels, np.round(epoch_output))))\n",
    "print(\"Zero-one loss is:{:.3f}\".format(zero_one_loss(labels, np.round(epoch_output))))\n",
    "print(\"Coverage error is:{:.3f}\".format(coverage_error(labels, epoch_output)))\n",
    "\n",
    "\"\"\"\n",
    "print(\"After treshold optimization\")\n",
    "print(\"F1 is:{:.3f}\".format(f1_score(labels, model_output_rounded,average=\"macro\")))\n",
    "print(\"Recall is:{:.3f}\".format(recall_score(labels, model_output_rounded,average=\"macro\")))\n",
    "print(\"Precision is:{:.3f}\".format(precision_score(labels, model_output_rounded,average=\"macro\")))\n",
    "print(\"Hamming loss is:{:.3f}\".format(hamming_loss(labels, model_output_rounded)))\n",
    "print(\"Zero-one loss is:{:.3f}\".format(zero_one_loss(labels, model_output_rounded))) \n",
    "    \n",
    "# Applying thresholds optimized per class for testset\n",
    "test_output_rounded = np.zeros_like(val_output)\n",
    "for idx in range(output_shape):\n",
    "    test_output_rounded[:, idx] = np.clip(np.round(val_output[:, idx] - threshold_per_class[idx] + 0.5), 0, 1)\n",
    "\"\"\"\n",
    "\n",
    "# On test\n",
    "print(\"===================\")\n",
    "print(\"Test set evaluation\")\n",
    "#print(\"AUC is:{:.3f}\".format(roc_auc_score(test_labels, val_output)))\n",
    "print(\"F1 is:{:.3f}\".format(f1_score(test_labels, np.round(val_output),average=\"micro\")))\n",
    "print(\"Recall is:{:.3f}\".format(recall_score(test_labels, np.round(val_output),average=\"micro\")))\n",
    "print(\"Precision is:{:.3f}\".format(precision_score(test_labels, np.round(val_output),average=\"micro\")))\n",
    "print(\"Average Precision is:{:.3f}\".format(average_precision_score(test_labels, val_output,average=\"micro\")))\n",
    "print(\"Hamming loss is:{:.3f}\".format(hamming_loss(test_labels, np.round(val_output))))\n",
    "print(\"Zero-one loss is:{:.3f}\".format(zero_one_loss(test_labels, np.round(val_output))))\n",
    "print(\"Coverage error is:{:.3f}\".format(coverage_error(test_labels, val_output)))\n",
    "\n",
    "\"\"\"\n",
    "print(\"After treshold optimization\")\n",
    "print(\"F1 is:{:.3f}\".format(f1_score(test_labels, test_output_rounded,average=\"macro\")))\n",
    "print(\"Recall is:{:.3f}\".format(recall_score(test_labels,test_output_rounded,average=\"macro\")))\n",
    "print(\"Precision is:{:.3f}\".format(precision_score(test_labels, test_output_rounded,average=\"macro\")))\n",
    "print(\"Hamming loss is:{:.3f}\".format(hamming_loss(test_labels, test_output_rounded)))\n",
    "print(\"Zero-one loss is:{:.3f}\".format(zero_one_loss(test_labels, test_output_rounded)))\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With fixed negative weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_negative_weights = np.zeros_like(labels) + 1 \n",
    "train_positive_weights = np.zeros_like(labels) + 5 # We make positive weight 5 becuase of data imbalance\n",
    "for counter in range (number_of_hidden_samples):\n",
    "    train_negative_weights[indices_to_hide[0][counter]][indices_to_hide[1][counter]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1118 18:56:50.992047 139823429289728 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "positive_weights = tf.placeholder(tf.float32, [None,output_shape], name = \"Positive_weights\")\n",
    "negative_weights = tf.placeholder(tf.float32, [None, output_shape], name=\"negative_weights\")\n",
    "my_weights_loss = weighted_loss(y_true= y, y_pred= output,\n",
    "                              positive_weights= positive_weights, negative_weights= negative_weights)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(my_weights_loss,global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #500 LR: 0.1000 Loss: 0.3068 Weighted Loss: 0.2983 accuracy: 0.9415 Test loss: 0.3818 Test accuracy: 0.9454\n",
      "Epoch #1000 LR: 0.0950 Loss: 0.2971 Weighted Loss: 0.2887 accuracy: 0.9479 Test loss: 0.3774 Test accuracy: 0.9454\n",
      "Epoch #1500 LR: 0.0950 Loss: 0.2916 Weighted Loss: 0.2832 accuracy: 0.9515 Test loss: 0.3749 Test accuracy: 0.9454\n",
      "Epoch #2000 LR: 0.0903 Loss: 0.2865 Weighted Loss: 0.2780 accuracy: 0.9532 Test loss: 0.3728 Test accuracy: 0.9455\n",
      "Epoch #2500 LR: 0.0903 Loss: 0.2840 Weighted Loss: 0.2756 accuracy: 0.9537 Test loss: 0.3708 Test accuracy: 0.9456\n",
      "Epoch #3000 LR: 0.0857 Loss: 0.2846 Weighted Loss: 0.2761 accuracy: 0.9538 Test loss: 0.3685 Test accuracy: 0.9458\n",
      "Epoch #3500 LR: 0.0857 Loss: 0.2804 Weighted Loss: 0.2717 accuracy: 0.9544 Test loss: 0.3660 Test accuracy: 0.9459\n",
      "Epoch #4000 LR: 0.0815 Loss: 0.2781 Weighted Loss: 0.2693 accuracy: 0.9542 Test loss: 0.3631 Test accuracy: 0.9464\n",
      "Epoch #4500 LR: 0.0815 Loss: 0.2760 Weighted Loss: 0.2671 accuracy: 0.9542 Test loss: 0.3601 Test accuracy: 0.9471\n",
      "Epoch #5000 LR: 0.0774 Loss: 0.2732 Weighted Loss: 0.2641 accuracy: 0.9540 Test loss: 0.3569 Test accuracy: 0.9480\n"
     ]
    }
   ],
   "source": [
    "# Training with negative weights!\n",
    "NUM_EPOCHS = 5000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_my_weights_loss, epoch_loss, epoch_accuracy,epoch_output, _ = sess.run([my_weights_loss, loss, accuracy,output, train_step],feed_dict={x_input: \n",
    "                                                                                         features,y: labels_with_missing_positives,positive_weights: train_positive_weights,\n",
    "                                                                                                  negative_weights: train_negative_weights,\n",
    "                                                                                                  current_keep_prob: 0.3})\n",
    "        if (epoch+1)% 500 == 0:\n",
    "            val_losses, val_accuracies, val_output,current_learning_rate = sess.run([loss, accuracy,output,learning_rate],feed_dict={\n",
    "                                                                                          x_input: test_features,\n",
    "                                                                                          y:test_labels,\n",
    "                                                                                          current_keep_prob: 1.0})\n",
    "            print(\"Epoch #{}\".format(epoch+1),  \"LR: {:.4f}\".format(current_learning_rate),\n",
    "                  \"Loss: {:.4f}\".format(epoch_loss), \n",
    "                  \"Weighted Loss: {:.4f}\".format(epoch_my_weights_loss),\"accuracy: {:.4f}\".format(epoch_accuracy), \n",
    "                  \"Test loss: {:.4f}\".format(val_losses), \"Test accuracy: {:.4f}\".format(val_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set evaluation\n",
      "F1 is:0.323\n",
      "Recall is:0.289\n",
      "Precision is:0.367\n",
      "Average Precision is:0.308\n",
      "Hamming loss is:0.054\n",
      "Zero-one loss is:0.811\n",
      "Coverage error is:5.250\n",
      "===================\n",
      "Test set evaluation\n",
      "F1 is:0.337\n",
      "Recall is:0.302\n",
      "Precision is:0.382\n",
      "Average Precision is:0.331\n",
      "Hamming loss is:0.052\n",
      "Zero-one loss is:0.786\n",
      "Coverage error is:4.858\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "# On training \n",
    "print(\"Training set evaluation\")\n",
    "#print(\"AUC is:{:.3f}\".format(roc_auc_score(labels, epoch_output)))\n",
    "print(\"F1 is:{:.3f}\".format(f1_score(labels, np.round(epoch_output),average=\"micro\")))\n",
    "print(\"Recall is:{:.3f}\".format(recall_score(labels, np.round(epoch_output),average=\"micro\")))\n",
    "print(\"Precision is:{:.3f}\".format(precision_score(labels, np.round(epoch_output),average=\"micro\")))\n",
    "print(\"Average Precision is:{:.3f}\".format(average_precision_score(labels, epoch_output,average=\"micro\")))\n",
    "print(\"Hamming loss is:{:.3f}\".format(hamming_loss(labels, np.round(epoch_output))))\n",
    "print(\"Zero-one loss is:{:.3f}\".format(zero_one_loss(labels, np.round(epoch_output))))\n",
    "print(\"Coverage error is:{:.3f}\".format(coverage_error(labels, epoch_output)))\n",
    "\n",
    "\"\"\"\n",
    "print(\"After treshold optimization\")\n",
    "print(\"F1 is:{:.3f}\".format(f1_score(labels, model_output_rounded,average=\"macro\")))\n",
    "print(\"Recall is:{:.3f}\".format(recall_score(labels, model_output_rounded,average=\"macro\")))\n",
    "print(\"Precision is:{:.3f}\".format(precision_score(labels, model_output_rounded,average=\"macro\")))\n",
    "print(\"Hamming loss is:{:.3f}\".format(hamming_loss(labels, model_output_rounded)))\n",
    "print(\"Zero-one loss is:{:.3f}\".format(zero_one_loss(labels, model_output_rounded))) \n",
    "    \n",
    "# Applying thresholds optimized per class for testset\n",
    "test_output_rounded = np.zeros_like(val_output)\n",
    "for idx in range(output_shape):\n",
    "    test_output_rounded[:, idx] = np.clip(np.round(val_output[:, idx] - threshold_per_class[idx] + 0.5), 0, 1)\n",
    "\"\"\"\n",
    "\n",
    "# On test\n",
    "print(\"===================\")\n",
    "print(\"Test set evaluation\")\n",
    "#print(\"AUC is:{:.3f}\".format(roc_auc_score(test_labels, val_output)))\n",
    "print(\"F1 is:{:.3f}\".format(f1_score(test_labels, np.round(val_output),average=\"micro\")))\n",
    "print(\"Recall is:{:.3f}\".format(recall_score(test_labels, np.round(val_output),average=\"micro\")))\n",
    "print(\"Precision is:{:.3f}\".format(precision_score(test_labels, np.round(val_output),average=\"micro\")))\n",
    "print(\"Average Precision is:{:.3f}\".format(average_precision_score(test_labels, val_output,average=\"micro\")))\n",
    "print(\"Hamming loss is:{:.3f}\".format(hamming_loss(test_labels, np.round(val_output))))\n",
    "print(\"Zero-one loss is:{:.3f}\".format(zero_one_loss(test_labels, np.round(val_output))))\n",
    "print(\"Coverage error is:{:.3f}\".format(coverage_error(test_labels, val_output)))\n",
    "\n",
    "\"\"\"\n",
    "print(\"After treshold optimization\")\n",
    "print(\"F1 is:{:.3f}\".format(f1_score(test_labels, test_output_rounded,average=\"macro\")))\n",
    "print(\"Recall is:{:.3f}\".format(recall_score(test_labels,test_output_rounded,average=\"macro\")))\n",
    "print(\"Precision is:{:.3f}\".format(precision_score(test_labels, test_output_rounded,average=\"macro\")))\n",
    "print(\"Hamming loss is:{:.3f}\".format(hamming_loss(test_labels, test_output_rounded)))\n",
    "print(\"Zero-one loss is:{:.3f}\".format(zero_one_loss(test_labels, test_output_rounded)))\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg.fit(X_train,y_train)\n",
    "\n",
    "preds = xg_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 413,  423, 1043,   55,  442,    0,    1,   39,   10,   41,   10,\n",
       "          2,   80,  251,   14,    2,    3,   14,    3,  540,  291,    6,\n",
       "        651,   18,   15,  369,    9,   18,   10,    7,   12,  108,   16],\n",
       "      dtype=uint64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(labels,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
