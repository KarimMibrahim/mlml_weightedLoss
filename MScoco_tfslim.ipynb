{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import strftime, localtime\n",
    "import matplotlib.pyplot as plt\n",
    "#from utilities import create_analysis_report, load_validation_set_raw\n",
    "from skimage.transform import resize\n",
    "\n",
    "SLIM_PATH = '/srv/workspace/research/mlml/models/research/slim'\n",
    "os.chdir(SLIM_PATH)\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from nets import inception\n",
    "from datasets import dataset_utils\n",
    "from preprocessing import inception_preprocessing\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, roc_auc_score, \\\n",
    "    hamming_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "\n",
    "SOURCE_PATH = \"/srv/workspace/research/mlml/mlml_weightedLoss/\"\n",
    "IMAGES_PATH = \"/srv/workspace/research/mlml/datasets/mscoco/train_formatted_npz/\"\n",
    "TEST_IMAGES_PATH = \"/srv/workspace/research/mlml/datasets/mscoco/val_formatted_npz/\"\n",
    "OUTPUT_PATH =\"/srv/workspace/research/mlml/experiments_results/\"\n",
    "PRETRAINED_MODEL_DIR = '/srv/workspace/research/mlml/pretrained_models/'\n",
    "\n",
    "EXPERIMENTNAME = \"pretrained_inceptionnet_first_run\"\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "\n",
    "global_labels = pd.read_csv('/srv/workspace/research/mlml/datasets/mscoco//ms binarized_labels.csv')\n",
    "test_global_labels = pd.read_csv('/srv/workspace/research/mlml/datasets/mscoco/ms_val_binarized_labels.csv')\n",
    "\n",
    "LABELS_LIST = global_labels.columns[2:]\n",
    "NUM_CLASSES = len(LABELS_LIST)\n",
    "\n",
    "# Training paramaeters\n",
    "BATCH_SIZE = 32\n",
    "TRAINING_STEPS = int(len(global_labels)/BATCH_SIZE)\n",
    "#VALIDATION_STEPS = 156\n",
    "NUM_EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading pretrained model \n",
    "def download_pretrained_model():\n",
    "    url = \"http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\"\n",
    "    if not tf.gfile.Exists(PRETRAINED_MODEL_DIR):\n",
    "        tf.gfile.MakeDirs(PRETRAINED_MODEL_DIR)\n",
    "\n",
    "    dataset_utils.download_and_uncompress_tarball(url, PRETRAINED_MODEL_DIR)\n",
    "\n",
    "def dataset_from_csv(csv_path, **kwargs):\n",
    "    \"\"\"\n",
    "        Load dataset from a csv file.\n",
    "        kwargs are forwarded to the pandas.read_csv function.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, **kwargs)\n",
    "\n",
    "    dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices(\n",
    "            {\n",
    "                key:df[key].values\n",
    "                for key in df\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def set_tensor_shape(tensor, tensor_shape):\n",
    "        \"\"\"\n",
    "            set shape for a tensor (not in place, as opposed to tf.set_shape)\n",
    "        \"\"\"\n",
    "        tensor.set_shape(tensor_shape)\n",
    "        return tensor\n",
    "\n",
    "def mscoco_labels_idx_to_names(labels,labelnames = LABELS_LIST):\n",
    "    return labelnames[np.where(labels == 1)]\n",
    "\n",
    "\n",
    "def load_image_npz(*args):\n",
    "    \"\"\"\n",
    "        loads spectrogram with error tracking.\n",
    "        args : song ID, path to dataset\n",
    "        return:\n",
    "            Features: numpy ndarray, computed features (if no error occured, otherwise: 0)\n",
    "            Error: boolean, False if no error, True if an error was raised during features computation.\n",
    "    \"\"\"\n",
    "    # TODO: edit path\n",
    "    path = IMAGES_PATH\n",
    "    image_id, dummy_path = args\n",
    "    try:\n",
    "        # tf.logging.info(f\"Load spectrogram for {song_id}\")\n",
    "        image = np.load(os.path.join(path, str(image_id) + '.npz'))['image']\n",
    "        image = image.astype(np.float32)\n",
    "        image = resize(image, INPUT_SHAPE)\n",
    "        # image = tf.keras.preprocessing.image.array_to_img(image,data_format='channels_last')\n",
    "        return image, False\n",
    "    except Exception as err:\n",
    "        print(\"\\n Error while computing features for \" + str(image_id) + '\\n')\n",
    "        return np.float32(0.0), True\n",
    "\n",
    "\n",
    "def load_imge_tf(sample, identifier_key=\"index\",\n",
    "                 path=\"/srv/workspace/research/mlml/datasets/mscoco/train_formatted_npz/\", device=\"/cpu:0\",\n",
    "                 features_key=\"features\"):\n",
    "    \"\"\"\n",
    "        wrap load_images into a tensorflow function.\n",
    "    \"\"\"\n",
    "    with tf.device(device):\n",
    "        input_args = [sample[identifier_key], tf.constant(path)]\n",
    "        res = tf.py_func(load_image_npz,\n",
    "                         input_args,\n",
    "                         (tf.float32, tf.bool),\n",
    "                         stateful=False),\n",
    "        image, error = res[0]\n",
    "        image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "        res = dict(list(sample.items()) + [(features_key, image), (\"error\", error)])\n",
    "        return res\n",
    "\n",
    "# Dataset pipelines\n",
    "def get_labels_py(image_id):\n",
    "    labels = global_labels[global_labels.index == image_id]\n",
    "    labels = labels.iloc[:, 2:].values.flatten()\n",
    "    labels = labels.astype(np.float32)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def tf_get_labels_py(sample, device=\"/cpu:0\"):\n",
    "    with tf.device(device):\n",
    "        input_args = [sample[\"index\"]]\n",
    "        labels = tf.py_func(get_labels_py,\n",
    "                            input_args,\n",
    "                            [tf.float32],\n",
    "                            stateful=False)\n",
    "        res = dict(list(sample.items()) + [(\"binary_label\", labels)])\n",
    "        return res\n",
    "\n",
    "\n",
    "def get_dataset(input_csv, input_shape=INPUT_SHAPE, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                infinite_generator=True, random_crop=False,\n",
    "                num_parallel_calls=32):\n",
    "    # build dataset from csv file\n",
    "    dataset = dataset_from_csv(input_csv)\n",
    "    # Shuffle data\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=100, seed=1, reshuffle_each_iteration=True)\n",
    "\n",
    "    # load image\n",
    "    dataset = dataset.map(lambda sample: load_imge_tf(sample), num_parallel_calls=1)\n",
    "\n",
    "    # filter out errors\n",
    "    dataset = dataset.filter(lambda sample: tf.logical_not(sample[\"error\"]))\n",
    "\n",
    "    # resize image\n",
    "    # dataset = dataset.map(lambda sample: dict(sample, features=tf.image.resize_images(sample['features'], (224, 224))),\n",
    "    #                      num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # Apply permute dimensions\n",
    "    # dataset = dataset.map(lambda sample: dict(sample, features=tf.transpose(sample[\"features\"], perm=[1, 2, 0])),\n",
    "    #                      num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # set features shape\n",
    "    dataset = dataset.map(lambda sample: dict(sample,\n",
    "                                              features=set_tensor_shape(sample[\"features\"], input_shape)))\n",
    "\n",
    "    # if cache_dir:\n",
    "    #    os.makedirs(cache_dir, exist_ok=True)\n",
    "    #    dataset = dataset.cache(cache_dir)\n",
    "\n",
    "    dataset = dataset.map(lambda sample: tf_get_labels_py(sample), num_parallel_calls=1)\n",
    "\n",
    "    # set output shape\n",
    "    dataset = dataset.map(lambda sample: dict(sample, binary_label=set_tensor_shape(\n",
    "        sample[\"binary_label\"], (NUM_CLASSES))))\n",
    "\n",
    "    if infinite_generator:\n",
    "        # Repeat indefinitly\n",
    "        dataset = dataset.repeat(count=-1)\n",
    "\n",
    "    # Make batch\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Select only features and annotation\n",
    "    dataset = dataset.map(lambda sample: (sample[\"features\"], sample[\"binary_label\"]))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def plot_loss_acuracy(epoch_losses_history, epoch_accurcies_history, val_losses_history, val_accuracies_history, path):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(epoch_accurcies_history)\n",
    "    plt.plot(val_accuracies_history)\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(path, \"model_accuracy.png\"))\n",
    "    plt.savefig(os.path.join(path, \"model_accuracy.pdf\"), format='pdf')\n",
    "    # plt.savefig(os.path.join(path,label + \"_model_accuracy.eps\"), format='eps', dpi=900)\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(epoch_losses_history)\n",
    "    plt.plot(val_losses_history)\n",
    "    plt.title('Model loss (Cross Entropy without weighting)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(path, \"model_loss.png\"))\n",
    "    plt.savefig(os.path.join(path, \"model_loss.pdf\"), format='pdf')\n",
    "    # plt.savefig(os.path.join(path,label + \"_model_loss.eps\"), format='eps', dpi=900)\n",
    "    \n",
    "def load_test_set_raw(image_path=TEST_IMAGES_PATH):\n",
    "    # Loading testset groundtruth\n",
    "    test_ground_truth = pd.read_csv('/srv/workspace/research/mlml/datasets/mscoco/ms_val_binarized_labels.csv')\n",
    "    test_classes = test_ground_truth.values[:,2:]\n",
    "    test_classes = test_classes.astype(int)\n",
    "    images_ids = test_ground_truth.values[1]\n",
    "\n",
    "    test_images = np.zeros([len(test_ground_truth), 224, 224,3])\n",
    "    songs_ID = np.zeros([len(test_ground_truth), 1])\n",
    "    for idx, filename in enumerate(list(images_ids)):\n",
    "        image = np.load(os.path.join(IMAGES_PATH, str(filename) + '.npz'))['image']\n",
    "        image = image.astype(np.float32)\n",
    "        image = resize(image, INPUT_SHAPE)\n",
    "        test_images[idx] = image\n",
    "    return test_images, test_classes\n",
    "\n",
    "def create_analysis_report(model_output, groundtruth, output_path, LABELS_LIST, validation_output=None,\n",
    "                           validation_groundtruth=None):\n",
    "    \"\"\"\n",
    "    Create a report of all the different evaluation metrics, including optimizing the threshold with the validation set\n",
    "    if it is passed in the parameters\n",
    "    \"\"\"\n",
    "    # Round the probabilities at 0.5\n",
    "    model_output_rounded = np.round(model_output)\n",
    "    model_output_rounded = np.clip(model_output_rounded, 0, 1)\n",
    "    # Create a dataframe where we keep all the evaluations, starting by prediction accuracy\n",
    "    accuracies_perclass = sum(model_output_rounded == groundtruth) / len(groundtruth)\n",
    "    results_df = pd.DataFrame(columns=LABELS_LIST)\n",
    "    results_df.index.astype(str, copy=False)\n",
    "    percentage_of_positives_perclass = sum(groundtruth) / len(groundtruth)\n",
    "    results_df.loc[0] = percentage_of_positives_perclass\n",
    "    results_df.loc[1] = accuracies_perclass\n",
    "    results_df.index = ['Ratio of positive samples', 'Model accuracy']\n",
    "\n",
    "    # plot the accuracies per class\n",
    "    results_df.T.plot.bar(figsize=(22, 12), fontsize=18)\n",
    "    plt.title('Model accuracy vs the ratio of positive samples per class')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(os.path.join(output_path, \"accuracies_vs_positiveRate.pdf\"), format=\"pdf\")\n",
    "    plt.savefig(os.path.join(output_path, \"accuracies_vs_positiveRate.png\"))\n",
    "\n",
    "    # Getting the true positive rate perclass\n",
    "    true_positives_ratio_perclass = sum((model_output_rounded == groundtruth) * (groundtruth == 1)) / sum(groundtruth)\n",
    "    results_df.loc[2] = true_positives_ratio_perclass\n",
    "    # Get true negative ratio\n",
    "    true_negative_ratio_perclass = sum((model_output_rounded == groundtruth)\n",
    "                                       * (groundtruth == 0)) / (len(groundtruth) - sum(groundtruth))\n",
    "    results_df.loc[3] = true_negative_ratio_perclass\n",
    "    # compute additional metrics (AUC,f1,recall,precision)\n",
    "    auc_roc_per_label = roc_auc_score(groundtruth, model_output, average=None)\n",
    "    precision_perlabel = precision_score(groundtruth, model_output_rounded, average=None)\n",
    "    recall_perlabel = recall_score(groundtruth, model_output_rounded, average=None)\n",
    "    f1_perlabel = f1_score(groundtruth, model_output_rounded, average=None)\n",
    "    kappa_perlabel = [cohen_kappa_score(groundtruth[:, x], model_output_rounded[:, x]) for x in range(len(LABELS_LIST))]\n",
    "    results_df = results_df.append(\n",
    "        pd.DataFrame([auc_roc_per_label, precision_perlabel, recall_perlabel, f1_perlabel, kappa_perlabel], columns=LABELS_LIST))\n",
    "    results_df.index = ['Ratio of positive samples', 'Model accuracy', 'True positives ratio',\n",
    "                        'True negatives ratio', \"AUC\", \"Recall\", \"Precision\", \"f1-score\", \"Kappa score\"]\n",
    "\n",
    "    # Creating evaluation plots\n",
    "    plot_true_poisitve_vs_all_positives(model_output_rounded, groundtruth,\n",
    "                                        os.path.join(output_path, 'TruePositive_vs_allPositives'), LABELS_LIST)\n",
    "    plot_output_coocurances(model_output_rounded, os.path.join(output_path, 'output_coocurances'), LABELS_LIST)\n",
    "    plot_false_netgatives_confusion_matrix(model_output_rounded, groundtruth,\n",
    "                                           os.path.join(output_path, 'false_negative_coocurances'), LABELS_LIST)\n",
    "\n",
    "    # Adjusting threshold based on validation set\n",
    "    if (validation_groundtruth is not None and validation_output is not None):\n",
    "        np.savetxt(os.path.join(output_path, 'validation_predictions.out'), validation_output, delimiter=',')\n",
    "        np.savetxt(os.path.join(output_path, 'valid_ground_truth_classes.txt'), validation_groundtruth, delimiter=',')\n",
    "        thresholds = np.arange(0, 1, 0.01)\n",
    "        f1_array = np.zeros((len(LABELS_LIST), len(thresholds)))\n",
    "        for idx, label in enumerate(LABELS_LIST):\n",
    "            f1_array[idx, :] = [\n",
    "                f1_score(validation_groundtruth[:, idx], np.clip(np.round(validation_output[:, idx] - threshold + 0.5), 0, 1))\n",
    "                for threshold in thresholds]\n",
    "        threshold_arg = np.argmax(f1_array, axis=1)\n",
    "        threshold_per_class = thresholds[threshold_arg]\n",
    "\n",
    "        # plot the f1 score across thresholds\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        for idx, x in enumerate(LABELS_LIST):\n",
    "            plt.plot(thresholds, f1_array[idx, :], linewidth=5)\n",
    "        plt.legend(LABELS_LIST, loc='best')\n",
    "        plt.title(\"F1 Score vs different prediction threshold values for each class\")\n",
    "        plt.savefig(os.path.join(output_path, \"f1_score_vs_thresholds.pdf\"), format=\"pdf\")\n",
    "        plt.savefig(os.path.join(output_path, \"f1_score_vs_thresholds.png\"))\n",
    "\n",
    "        # Applying thresholds optimized per class\n",
    "        model_output_rounded = np.zeros_like(model_output)\n",
    "        for idx, label in enumerate(LABELS_LIST):\n",
    "            model_output_rounded[:, idx] = np.clip(np.round(model_output[:, idx] - threshold_per_class[idx] + 0.5), 0, 1)\n",
    "\n",
    "        accuracies_perclass = sum(model_output_rounded == groundtruth) / len(groundtruth)\n",
    "        # Getting the true positive rate perclass\n",
    "        true_positives_ratio_perclass = sum((model_output_rounded == groundtruth) * (groundtruth == 1)) / sum(\n",
    "            groundtruth)\n",
    "        # Get true negative ratio\n",
    "        true_negative_ratio_perclass = sum((model_output_rounded == groundtruth)\n",
    "                                           * (groundtruth == 0)) / (len(groundtruth) - sum(groundtruth))\n",
    "        results_df = results_df.append(\n",
    "            pd.DataFrame([accuracies_perclass, true_positives_ratio_perclass,\n",
    "                          true_negative_ratio_perclass], columns=LABELS_LIST))\n",
    "        # compute additional metrics (AUC,f1,recall,precision)\n",
    "        auc_roc_per_label = roc_auc_score(groundtruth, model_output, average=None)\n",
    "        precision_perlabel = precision_score(groundtruth, model_output_rounded, average=None)\n",
    "        recall_perlabel = recall_score(groundtruth, model_output_rounded, average=None)\n",
    "        f1_perlabel = f1_score(groundtruth, model_output_rounded, average=None)\n",
    "        kappa_perlabel = [cohen_kappa_score(groundtruth[:, x], model_output_rounded[:, x]) for x in\n",
    "                          range(len(LABELS_LIST))]\n",
    "        results_df = results_df.append(\n",
    "            pd.DataFrame([auc_roc_per_label, precision_perlabel, recall_perlabel, f1_perlabel,kappa_perlabel],\n",
    "                         columns=LABELS_LIST))\n",
    "        results_df.index = ['Ratio of positive samples', 'Model accuracy', 'True positives ratio',\n",
    "                            'True negatives ratio', \"AUC\", \"Precision\", \"Recall\", \"f1-score\",  \"Kappa score\",\n",
    "                            'Optimized model accuracy', 'Optimized true positives ratio',\n",
    "                            'Optimized true negatives ratio', \"Optimized AUC\",\n",
    "                            \"Optimized precision\", \"Optimized recall\", \"Optimized f1-score\",  \"Optimized Kappa score\"]\n",
    "\n",
    "        # Creating evaluation plots\n",
    "        plot_true_poisitve_vs_all_positives(model_output_rounded, groundtruth,\n",
    "                                            os.path.join(output_path, 'TruePositive_vs_allPositives[optimized]'),\n",
    "                                            LABELS_LIST)\n",
    "        plot_output_coocurances(model_output_rounded, os.path.join(output_path, 'output_coocurances[optimized]'),\n",
    "                                LABELS_LIST)\n",
    "        plot_false_netgatives_confusion_matrix(model_output_rounded, groundtruth,\n",
    "                                               os.path.join(output_path, 'false_negative_coocurances[optimized]'),\n",
    "                                               LABELS_LIST)\n",
    "    results_df['average'] = results_df.mean(numeric_only=True, axis=1)\n",
    "    results_df.T.to_csv(os.path.join(output_path, \"results_report.csv\"), float_format=\"%.2f\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tune pretrained model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Experiment: pretrained_inceptionnet_first_run\n",
      "\n",
      "\n",
      "\n",
      "INFO:tensorflow:Summary name Original cross_entropy is illegal; using Original_cross_entropy instead.\n",
      "Execute the following in a terminal:\n",
      "tensorboard --logdir=/srv/workspace/research/mlml/experiments_results/pretrained_inceptionnet_first_run/2019-12-04_13-29-45\n",
      "INFO:tensorflow:Restoring parameters from /srv/workspace/research/mlml/pretrained_models/inception_v1.ckpt\n",
      "Epoch #1 Loss: 0.1166 accuracy: 0.9646\n",
      "Epoch #2 Loss: 0.0793 accuracy: 0.9734\n",
      "Epoch #3 Loss: 0.0731 accuracy: 0.9751\n",
      "Epoch #4 Loss: 0.0694 accuracy: 0.9762\n",
      "Epoch #5 Loss: 0.0666 accuracy: 0.9770\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Experiment: \" + EXPERIMENTNAME + \"\\n\\n\\n\")\n",
    "# Loading datasets\n",
    "training_dataset = get_dataset('/srv/workspace/research/mlml/datasets/mscoco//ms binarized_labels.csv')\n",
    "#val_dataset = get_validation_dataset(os.path.join(SOURCE_PATH, \"GroundTruth/validation_ground_truth.csv\"))\n",
    "with tf.Graph().as_default():\n",
    "    # Setting up model\n",
    "    training_iterator = training_dataset.make_one_shot_iterator()\n",
    "    training_next_element = training_iterator.get_next()\n",
    "    \n",
    "    # Setting up model\n",
    "    labels = tf.placeholder(tf.float32, [None, NUM_CLASSES], name=\"true_labels\")\n",
    "    input_images = tf.placeholder(tf.float32, [None, 224, 224, 3], name=\"input\")\n",
    "    train_phase = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "\n",
    "    #positive_weights = tf.placeholder(tf.float32, [None,15], name = \"Positive_weights\")\n",
    "    #negative_weights = tf.placeholder(tf.float32, [None, 15], name=\"negative_weights\")\n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "        logits, _ = inception.inception_v1(input_images, num_classes=NUM_CLASSES, is_training=train_phase)\n",
    "\n",
    "    # Defining loss and metrics\n",
    "    # Define loss and training optimizer\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    probabilities = tf.nn.sigmoid(logits)\n",
    "    tf.summary.histogram('outputs', probabilities)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate=0.1, global_step=global_step, decay_steps=1000,\n",
    "                                              decay_rate=0.95,staircase=True)\n",
    "    train_step = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)\n",
    "    #my_weights_loss = weighted_loss(y_true= y, y_pred= model_output,\n",
    "    #                             positive_weights= positive_weights, negative_weights= negative_weights)\n",
    "\n",
    "    # define accuracy\n",
    "    correct_prediction = tf.equal(tf.round(probabilities), labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Adding tensorboard summaries\n",
    "    tf.summary.scalar('Original_cross_entropy', loss)\n",
    "    #tf.summary.scalar('Weighted cross entropy',  my_weights_loss)\n",
    "    tf.summary.scalar('Accuracy', accuracy)\n",
    "    # Merge all the summaries\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # restoring pretrained model weights\n",
    "    checkpoint_exclude_scopes = [\"InceptionV1/Logits\", \"InceptionV1/AuxLogits\"]\n",
    "    exclusions = [scope.strip() for scope in checkpoint_exclude_scopes]\n",
    "    variables_to_restore = []\n",
    "    for var in slim.get_model_variables():\n",
    "        for exclusion in exclusions:\n",
    "            if var.op.name.startswith(exclusion):\n",
    "                break\n",
    "        else:\n",
    "            variables_to_restore.append(var)\n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "        os.path.join(PRETRAINED_MODEL_DIR, 'inception_v1.ckpt'),\n",
    "        variables_to_restore)\n",
    "\n",
    "    # Setting up saving directory\n",
    "    experiment_name = strftime(\"%Y-%m-%d_%H-%M-%S\", localtime())\n",
    "    exp_dir = os.path.join(OUTPUT_PATH, EXPERIMENTNAME, experiment_name)\n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    epoch_losses_history, epoch_accurcies_history, val_losses_history, val_accuracies_history = [], [], [], []\n",
    "    #my_loss_history, my_loss_val_history = [], []\n",
    "    with tf.Session() as sess:\n",
    "        # Write summaries to LOG_DIR -- used by TensorBoard\n",
    "        train_writer = tf.summary.FileWriter(exp_dir + '/tensorboard/train', graph=tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter(exp_dir + '/tensorboard/test', graph=tf.get_default_graph())\n",
    "        print(\"Execute the following in a terminal:\\n\" + \"tensorboard --logdir=\" + exp_dir)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        init_fn(sess)\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            batch_loss, batch_accuracy = np.zeros([TRAINING_STEPS, 1]), np.zeros([TRAINING_STEPS, 1])\n",
    "            #batch_my_loss, val_my_loss = np.zeros([TRAINING_STEPS, 1]), np.zeros([VALIDATION_STEPS, 1])\n",
    "            #val_accuracies, val_losses = np.zeros([VALIDATION_STEPS, 1]), np.zeros([VALIDATION_STEPS, 1])\n",
    "            for batch_counter in range(TRAINING_STEPS):\n",
    "                batch = sess.run(training_next_element)\n",
    "                batch_images = batch[0]\n",
    "                batch_labels = np.squeeze(batch[1])\n",
    "                summary, batch_loss[batch_counter], batch_accuracy[batch_counter], _ = sess.run([merged, loss, accuracy, train_step],\n",
    "                                                                                                feed_dict={input_images:batch_images,\n",
    "                                                                                                          labels:batch_labels,\n",
    "                                                                                                          train_phase = True})\n",
    "            print(\"Epoch #{}\".format(epoch+1), \"Loss: {:.4f}\".format(np.mean(batch_loss)),\n",
    "                  \"accuracy: {:.4f}\".format(np.mean(batch_accuracy)))\n",
    "            epoch_losses_history.append(np.mean(batch_loss)); epoch_accurcies_history.append(np.mean(batch_accuracy))\n",
    "            #my_loss_history.append(np.mean(batch_my_loss))\n",
    "            # Add to summaries\n",
    "            train_writer.add_summary(summary, epoch)\n",
    "        \n",
    "         # Testing the model [I split the testset into smaller splits because of memory error]\n",
    "        test_images, test_classes = load_test_set_raw()\n",
    "        test_pred_prob = sess.run(model_output,feed_dict={input_images: test_images\n",
    "                                                           train_phase: False})\n",
    "        accuracy, auc_roc, hamming_error = evaluate_model(test_pred_prob, test_classes,\n",
    "                                                          saving_path=exp_dir,\n",
    "                                                          evaluation_file_path=os.path.join(exp_dir,\n",
    "                                                                                            \"evaluation_results.txt\"))\n",
    "\n",
    "        results = create_analysis_report(test_pred_prob, test_classes, exp_dir, LABELS_LIST)\n",
    "\n",
    "\n",
    "    # Plot and save losses\n",
    "    plot_loss_acuracy(epoch_losses_history, epoch_accurcies_history,exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
